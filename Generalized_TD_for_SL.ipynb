{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from utils.TD import TD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.99366542   5.00578355   0.63581661   2.44872684 -10.31186138\n",
      "  17.51763825   1.03114679 -17.77574728   7.34087816  -6.42763345\n",
      "  -8.79966234   3.03563353 -20.51935608]\n",
      "Intercept: 29.04431567479631\n",
      "Model R^2 score: 0.7730570034661727\n"
     ]
    }
   ],
   "source": [
    "housedata = np.loadtxt('data\\\\readyhousedata.txt', delimiter=',')\n",
    "\n",
    "X = housedata[:, :-1]\n",
    "y = housedata[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "reg = LinearRegression() \n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "weights = reg.coef_\n",
    "intercept = reg.intercept_\n",
    "\n",
    "print(weights)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "score = reg.score(X_test, y_test)\n",
    "print(\"Model R^2 score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-11.18211572   5.11801614   0.48564318   1.96249423  -9.94041809\n",
      "  17.75437566   0.74035574 -17.16481025   7.18176397  -6.59126913\n",
      "  -9.10714929   3.24368026 -20.77519175]\n"
     ]
    }
   ],
   "source": [
    "num_samples = X_train.shape[0]\n",
    "P = np.ones((num_samples, num_samples)) / num_samples # Equal probability to move to any state\n",
    "\n",
    "alpha = 0.01  # Learning rate\n",
    "gamma = 0   # Discount factor\n",
    "num_iterations = 1e6  # Number of iterations\n",
    "epsilon = 1e-9\n",
    "\n",
    "td = TD(\n",
    "    n_iter=num_iterations,\n",
    "    P=P,\n",
    "    link=lambda x : x,\n",
    "    inv_link=lambda x : x,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "\n",
    "td.fit(X_train, y_train)\n",
    "\n",
    "w_hat_house = td.weights\n",
    "bias_house = td.bias\n",
    "\n",
    "print(w_hat_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using SGD:\n",
      " [-10.99550082   5.00148479   0.62866441   2.44291755 -10.32353625\n",
      "  17.49539486   1.02345406 -17.77975053   7.34330163  -6.42853038\n",
      "  -8.8058957    3.0204479  -20.5263245 ]\n"
     ]
    }
   ],
   "source": [
    "def mini_batch_sgd(X: np.ndarray, y: np.ndarray, learning_rate: float, n_iter: int, epsilon: float, batch_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Performs Mini-Batch Stochastic Gradient Descent (SGD) for linear regression with shuffling.\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.ndarray): Feature matrix (n_samples, n_features).\n",
    "    y (np.ndarray): Target vector (n_samples,).\n",
    "    learning_rate (float): Step size for updating weights.\n",
    "    n_iter (int): Number of iterations (epochs).\n",
    "    epsilon (float): Convergence threshold.\n",
    "    batch_size (int): Size of the mini-batches.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The learned weights.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    # Add bias term to the feature matrix\n",
    "    X_bias = np.c_[np.ones(n_samples), X]  # Adds a column of ones for the bias term\n",
    "\n",
    "    # Initialize weights to zeros\n",
    "    weights = np.zeros(n_features + 1)\n",
    "\n",
    "    for epoch in range(int(n_iter)):\n",
    "        # Step 1: Shuffle the data at the beginning of each epoch\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_bias_shuffled = X_bias[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            # Step 2: Select the mini-batch\n",
    "            X_batch = X_bias_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "            \n",
    "            # Initialize gradients to zero\n",
    "            gradient = np.zeros_like(weights)\n",
    "            \n",
    "            # Step 3: Compute the gradient over the mini-batch\n",
    "            for j in range(X_batch.shape[0]):\n",
    "                # Prediction\n",
    "                prediction = np.dot(X_batch[j], weights)\n",
    "                error = y_batch[j] - prediction\n",
    "                \n",
    "                # Update gradient\n",
    "                gradient += -2 * X_batch[j] * error\n",
    "\n",
    "            # Step 4: Update the weights\n",
    "            weights -= learning_rate * gradient / batch_size\n",
    "\n",
    "        # Optional: Check for convergence (if gradient is small enough)\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            print(f\"Converged after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_iter = 1e4  # Number of epochs\n",
    "epsilon = 1e-6  # Convergence threshold\n",
    "batch_size = 16  # Mini-batch size\n",
    "\n",
    "# Run SGD manually\n",
    "weights_sgd = mini_batch_sgd(X_train, y_train, learning_rate, n_iter, epsilon, batch_size)\n",
    "\n",
    "# Print the learned weights\n",
    "print(\"Weights learned using SGD:\\n\", weights_sgd[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test set using TD: 4.098375087372361\n",
      "RMSE on the test set using L2 Regression: 4.154652986817322\n",
      "RMSE on the test set using SGD: 4.151977290028692\n",
      "---------------\n",
      "Norm of weights from TD: 38.530798371766224\n",
      "Norm of weights from L2 Regression: 38.54163631701667\n",
      "Norm of weights from SGD: 38.540331685087516\n",
      "Norm of difference in weights for L2: 1.1019435590742406\n",
      "Norm of difference in weights for sgd: 1.1076609311472736\n"
     ]
    }
   ],
   "source": [
    "pred_TD = td.predict(X_test)\n",
    "pred_L2 = reg.predict(X_test)\n",
    "pred_sgd = np.dot(X_test, weights_sgd[1:]) + weights_sgd[0]\n",
    "\n",
    "rmse_TD = td.rmse(X_test, y_test)\n",
    "rmse_L2 = np.sqrt(mean_squared_error(y_test, pred_L2))\n",
    "rmse_sgd = np.sqrt(mean_squared_error(y_test, pred_sgd))\n",
    "\n",
    "print(\"RMSE on the test set using TD:\", rmse_TD)\n",
    "print(\"RMSE on the test set using L2 Regression:\", rmse_L2)\n",
    "print(\"RMSE on the test set using SGD:\", rmse_sgd)\n",
    "print(\"---------------\")\n",
    "print(\"Norm of weights from TD:\", np.linalg.norm(w_hat_house, 2))\n",
    "print(\"Norm of weights from L2 Regression:\", np.linalg.norm(weights, 2))\n",
    "print(\"Norm of weights from SGD:\", np.linalg.norm(weights_sgd[1:], 2))\n",
    "print(\"Norm of difference in weights for L2:\", np.linalg.norm(weights - w_hat_house, 2))\n",
    "print(\"Norm of difference in weights for sgd:\", np.linalg.norm(w_hat_house - weights_sgd[1:], 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
