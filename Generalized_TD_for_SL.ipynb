{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from utils.TD import TD_SGD, TD_Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using LR:\n",
      " [-10.99366542   5.00578355   0.63581661   2.44872684 -10.31186138\n",
      "  17.51763825   1.03114679 -17.77574728   7.34087816  -6.42763345\n",
      "  -8.79966234   3.03563353 -20.51935608]\n",
      "Intercept: 29.04431567479631\n",
      "Model R^2 score: 0.7730570034661727\n"
     ]
    }
   ],
   "source": [
    "housedata = np.loadtxt('data\\\\readyhousedata.txt', delimiter=',')\n",
    "\n",
    "X = housedata[:, :-1]\n",
    "y = housedata[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "reg = LinearRegression() \n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "weights = reg.coef_\n",
    "intercept = reg.intercept_\n",
    "\n",
    "print(\"Weights learned using LR:\\n\", weights)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "score = reg.score(X_test, y_test)\n",
    "print(\"Model R^2 score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using SGD:\n",
      " [-11.00262763   5.00418417   0.63335422   2.4533718  -10.31607188\n",
      "  17.4982821    1.02933367 -17.78123565   7.33294589  -6.42777006\n",
      "  -8.80854031   3.0300865  -20.5191458 ]\n"
     ]
    }
   ],
   "source": [
    "def mini_batch_sgd(\n",
    "        X: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        learning_rate: float, \n",
    "        n_iter: int, \n",
    "        batch_size: int,\n",
    "        epsilon: float = 0, \n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Performs Mini-Batch Stochastic Gradient Descent (SGD) for linear regression with shuffling.\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.ndarray): Feature matrix (n_samples, n_features).\n",
    "    y (np.ndarray): Target vector (n_samples,).\n",
    "    learning_rate (float): Step size for updating weights.\n",
    "    n_iter (int): Number of iterations (epochs).\n",
    "    epsilon (float): Convergence threshold.\n",
    "    batch_size (int): Size of the mini-batches.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The learned weights.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    # Add bias term to the feature matrix\n",
    "    X_bias = np.c_[np.ones(n_samples), X]  # Adds a column of ones for the bias term\n",
    "\n",
    "    # Initialize weights to zeros\n",
    "    weights = np.zeros(n_features + 1)\n",
    "\n",
    "    for epoch in range(int(n_iter)):\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_bias_shuffled = X_bias[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_bias_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "            \n",
    "            # Initialize gradients to zero\n",
    "            gradient = np.zeros_like(weights)\n",
    "            \n",
    "            # Compute the gradient over the mini-batch\n",
    "            for j in range(X_batch.shape[0]):\n",
    "                # Prediction\n",
    "                prediction = np.dot(X_batch[j], weights)\n",
    "                error = y_batch[j] - prediction\n",
    "                \n",
    "                # Update gradient\n",
    "                gradient += -2 * X_batch[j] * error\n",
    "\n",
    "            # Update the weights\n",
    "            weights -= learning_rate * gradient / batch_size\n",
    "\n",
    "        # Check for convergence (if gradient is small enough)\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            print(f\"Converged after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_iter = 1e4  # Number of epochs\n",
    "epsilon = 1e-6  # Convergence threshold\n",
    "batch_size = 16  # Mini-batch size\n",
    "\n",
    "# Run SGD manually\n",
    "weights_sgd = mini_batch_sgd(X_train, y_train, learning_rate, n_iter, epsilon=epsilon, batch_size=batch_size)\n",
    "\n",
    "# Print the learned weights\n",
    "print(\"Weights learned using SGD:\\n\", weights_sgd[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using TD SGD:\n",
      " [-10.69778107   4.95533416   0.47354431   2.06862183 -10.09323432\n",
      "  18.0246395    1.23837445 -17.16714062   7.01245493  -6.71658602\n",
      "  -8.93571416   2.9863624  -20.88360889]\n"
     ]
    }
   ],
   "source": [
    "num_samples = X_train.shape[0]\n",
    "P = np.ones((num_samples, num_samples)) / num_samples # Equal probability to move to any state\n",
    "\n",
    "alpha = 0.01  # Learning rate\n",
    "gamma = 0   # Discount factor\n",
    "num_iterations = 1e5  # Number of iterations\n",
    "epsilon = 1e-9\n",
    "\n",
    "td_sgd = TD_SGD(\n",
    "    n_iter=num_iterations,\n",
    "    P=P,\n",
    "    link=lambda x : x,\n",
    "    inv_link=lambda x : x,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    epsilon=epsilon,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "td_sgd.fit(X_train, y_train)\n",
    "\n",
    "w_hat_house = td_sgd.weights\n",
    "bias_house = td_sgd.bias\n",
    "\n",
    "print(\"Weights learned using TD SGD:\\n\", w_hat_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using TD Adam:\n",
      " [-10.737863     4.6121483    0.31280735   1.3354009   -9.944276\n",
      "  17.693153     1.3588558  -16.592127     6.8893633   -6.770194\n",
      "  -9.154123     2.9209814  -20.840343  ]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05  # Learning rate\n",
    "gamma = 0   # Discount factor\n",
    "num_iterations = 1e5  # Number of iterations\n",
    "epsilon = 1e-9\n",
    "\n",
    "td_adam = TD_Adam(\n",
    "    n_iter=num_iterations,\n",
    "    P=P,\n",
    "    link=lambda x : x,\n",
    "    inv_link=lambda x : x,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    epsilon=epsilon,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "td_adam.fit(X_train, y_train)\n",
    "\n",
    "print(\"Weights learned using TD Adam:\\n\", td_adam.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test set using TD SGD: 4.1246346540847725\n",
      "RMSE on the test set using TD Adam: 4.245506682158816\n",
      "RMSE on the test set using L2 Regression: 4.154652986817322\n",
      "RMSE on the test set using SGD: 4.153448927184146\n",
      "---------------\n",
      "Norm of difference in weights for L2 and TD SGD: 1.1506830507130055\n",
      "Norm of difference in weights for sgd and TD SGD: 1.1633814460756713\n",
      "Norm of difference in weights for L2 and TD Adam: 2.382252727248095\n",
      "Norm of difference in weights for sgd and TD Adam: 2.3737354229007175\n"
     ]
    }
   ],
   "source": [
    "pred_TD_sgd = td_sgd.predict(X_test)\n",
    "pred_TD_adam = td_adam.predict(X_test)\n",
    "pred_L2 = reg.predict(X_test)\n",
    "pred_sgd = np.dot(X_test, weights_sgd[1:]) + weights_sgd[0]\n",
    "\n",
    "rmse_TD_sgd = td_sgd.rmse(X_test, y_test)\n",
    "rmse_TD_adam = td_adam.rmse(X_test, y_test)\n",
    "rmse_L2 = np.sqrt(mean_squared_error(y_test, pred_L2))\n",
    "rmse_sgd = np.sqrt(mean_squared_error(y_test, pred_sgd))\n",
    "\n",
    "print(\"RMSE on the test set using TD SGD:\", rmse_TD_sgd)\n",
    "print(\"RMSE on the test set using TD Adam:\", rmse_TD_adam)\n",
    "print(\"RMSE on the test set using L2 Regression:\", rmse_L2)\n",
    "print(\"RMSE on the test set using SGD:\", rmse_sgd)\n",
    "print(\"---------------\")\n",
    "print(\"Norm of difference in weights for L2 and TD SGD:\", np.linalg.norm(weights - td_sgd.weights, 2))\n",
    "print(\"Norm of difference in weights for sgd and TD SGD:\", np.linalg.norm(weights_sgd[1:] - td_sgd.weights, 2))\n",
    "print(\"Norm of difference in weights for L2 and TD Adam:\", np.linalg.norm(weights - td_adam.weights, 2))\n",
    "print(\"Norm of difference in weights for sgd and TD Adam:\", np.linalg.norm(weights_sgd[1:] - td_adam.weights, 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
