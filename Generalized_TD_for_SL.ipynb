{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x179bb459af0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from utils.TD import TD_SGD, TD_Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using LR:\n",
      " tensor([-10.9937,   5.0058,   0.6358,   2.4487, -10.3119,  17.5176,   1.0312,\n",
      "        -17.7757,   7.3409,  -6.4276,  -8.7997,   3.0356, -20.5194])\n",
      "Intercept: tensor(29.0443)\n",
      "Model R^2 score: 0.7730569243431091\n"
     ]
    }
   ],
   "source": [
    "housedata = torch.tensor(np.loadtxt('data\\\\readyhousedata.txt', delimiter=','), dtype=torch.float32)\n",
    "\n",
    "X = housedata[:, :-1]\n",
    "y = housedata[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "reg = LinearRegression() \n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "weights = torch.tensor(reg.coef_)\n",
    "intercept = torch.tensor(reg.intercept_)\n",
    "\n",
    "print(\"Weights learned using LR:\\n\", weights)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "score = reg.score(X_test, y_test)\n",
    "print(\"Model R^2 score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using SGD:\n",
      " tensor([-10.9912,   5.0124,   0.6570,   2.4577, -10.2984,  17.5350,   1.0759,\n",
      "        -17.7649,   7.3631,  -6.4073,  -8.7644,   3.0875, -20.5076])\n"
     ]
    }
   ],
   "source": [
    "def mini_batch_sgd(\n",
    "        X: torch.Tensor, \n",
    "        y: torch.Tensor, \n",
    "        learning_rate: float, \n",
    "        n_iter: int, \n",
    "        batch_size: int,\n",
    "        epsilon: float = 0, \n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs Mini-Batch Stochastic Gradient Descent (SGD) for linear regression with shuffling.\n",
    "    \n",
    "    Parameters:\n",
    "    X (torch.Tensor): Feature matrix (n_samples, n_features).\n",
    "    y (torch.Tensor): Target vector (n_samples,).\n",
    "    learning_rate (float): Step size for updating weights.\n",
    "    n_iter (int): Number of iterations (epochs).\n",
    "    epsilon (float): Convergence threshold.\n",
    "    batch_size (int): Size of the mini-batches.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The learned weights.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    # Add bias term to the feature matrix\n",
    "    X_bias = torch.cat([torch.ones(n_samples, 1), X], dim=1)  # Adds a column of ones for the bias term\n",
    "\n",
    "    # Initialize weights to zeros\n",
    "    weights = torch.zeros(n_features + 1)\n",
    "\n",
    "    for epoch in range(int(n_iter)):\n",
    "        indices = torch.randperm(n_samples)\n",
    "        X_bias_shuffled = X_bias[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_bias_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "            \n",
    "            # Initialize gradients to zero\n",
    "            gradient = torch.zeros_like(weights)\n",
    "            \n",
    "            # Compute the gradient over the mini-batch\n",
    "            for j in range(X_batch.shape[0]):\n",
    "                # Prediction\n",
    "                prediction = torch.dot(X_batch[j], weights)\n",
    "                error = y_batch[j] - prediction\n",
    "                \n",
    "                # Update gradient\n",
    "                gradient += -2 * X_batch[j] * error\n",
    "\n",
    "            # Update the weights\n",
    "            weights -= learning_rate * gradient / batch_size\n",
    "\n",
    "        # Check for convergence (if gradient is small enough)\n",
    "        if torch.norm(gradient) < epsilon:\n",
    "            print(f\"Converged after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_iter = 1e4  # Number of epochs\n",
    "epsilon = 1e-6  # Convergence threshold\n",
    "batch_size = 16  # Mini-batch size\n",
    "\n",
    "# Run SGD manually\n",
    "weights_sgd = mini_batch_sgd(X_train, y_train, learning_rate, n_iter, epsilon=epsilon, batch_size=batch_size)\n",
    "\n",
    "# Print the learned weights\n",
    "print(\"Weights learned using SGD:\\n\", weights_sgd[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using TD SGD:\n",
      " tensor([-1.0654e+01,  4.9181e+00,  9.4121e-03,  1.4318e+00, -1.0087e+01,\n",
      "         1.7143e+01,  8.6331e-01, -1.7416e+01,  7.2050e+00, -6.5871e+00,\n",
      "        -8.0881e+00,  2.8847e+00, -2.0169e+01])\n"
     ]
    }
   ],
   "source": [
    "num_samples = X_train.shape[0]\n",
    "P = torch.ones((num_samples, num_samples)) / num_samples # Equal probability to move to any state\n",
    "\n",
    "alpha = 0.01  # Learning rate\n",
    "gamma = 0   # Discount factor\n",
    "num_iterations = 1e5  # Number of iterations\n",
    "epsilon = 1e-9\n",
    "\n",
    "td_sgd = TD_SGD(\n",
    "    n_iter=num_iterations,\n",
    "    P=P,\n",
    "    link=lambda x : x,\n",
    "    inv_link=lambda x : x,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    epsilon=epsilon,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "td_sgd.fit(X_train, y_train)\n",
    "\n",
    "w_hat_house = td_sgd.weights\n",
    "bias_house = td_sgd.bias\n",
    "\n",
    "print(\"Weights learned using TD SGD:\\n\", w_hat_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending optimization early at iteration 85489\n",
      "Weights learned using TD Adam:\n",
      " tensor([-12.1528,   4.4889,   0.4160,   1.6978,  -9.6249,  16.9506,   1.4479,\n",
      "        -18.0856,   7.0949,  -6.5179,  -8.8375,   2.6827, -21.3809])\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05  # Learning rate\n",
    "gamma = 0   # Discount factor\n",
    "num_iterations = 1e5  # Number of iterations\n",
    "epsilon = 1e-9\n",
    "\n",
    "td_adam = TD_Adam(\n",
    "    n_iter=num_iterations,\n",
    "    P=P,\n",
    "    link=lambda x : x,\n",
    "    inv_link=lambda x : x,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    epsilon=epsilon,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "td_adam.fit(X_train, y_train)\n",
    "\n",
    "print(\"Weights learned using TD Adam:\\n\", td_adam.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test set using TD SGD: 4.25275182723999\n",
      "RMSE on the test set using TD Adam: 4.218838214874268\n",
      "RMSE on the test set using L2 Regression: 4.154654026031494\n",
      "RMSE on the test set using SGD: 4.178890705108643\n",
      "---------------\n",
      "Norm of difference in weights for L2 and TD SGD: 1.6105726957321167\n",
      "Norm of difference in weights for sgd and TD SGD: 1.6218323707580566\n",
      "Norm of difference in weights for L2 and TD Adam: 2.0549962520599365\n",
      "Norm of difference in weights for sgd and TD Adam: 2.076366424560547\n"
     ]
    }
   ],
   "source": [
    "pred_TD_sgd = td_sgd.predict(X_test)\n",
    "pred_TD_adam = td_adam.predict(X_test)\n",
    "pred_L2 = reg.predict(X_test)\n",
    "pred_sgd = torch.matmul(X_test, weights_sgd[1:]) + weights_sgd[0]\n",
    "\n",
    "rmse_TD_sgd = td_sgd.rmse(X_test, y_test)\n",
    "rmse_TD_adam = td_adam.rmse(X_test, y_test)\n",
    "rmse_L2 = torch.sqrt(torch.tensor(mean_squared_error(y_test, pred_L2)))\n",
    "rmse_sgd = torch.sqrt(torch.tensor(mean_squared_error(y_test, pred_sgd)))\n",
    "\n",
    "print(f\"RMSE on the test set using TD SGD: {rmse_TD_sgd}\")\n",
    "print(f\"RMSE on the test set using TD Adam: {rmse_TD_adam}\")\n",
    "print(f\"RMSE on the test set using L2 Regression: {rmse_L2}\")\n",
    "print(f\"RMSE on the test set using SGD: {rmse_sgd}\")\n",
    "print(\"---------------\")\n",
    "print(f\"Norm of difference in weights for L2 and TD SGD: {torch.norm(weights - td_sgd.weights, 2)}\")\n",
    "print(f\"Norm of difference in weights for sgd and TD SGD: {torch.norm(weights_sgd[1:] - td_sgd.weights, 2)}\")\n",
    "print(f\"Norm of difference in weights for L2 and TD Adam: {torch.norm(weights - td_adam.weights, 2)}\")\n",
    "print(f\"Norm of difference in weights for sgd and TD Adam: {torch.norm(weights_sgd[1:] - td_adam.weights, 2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
