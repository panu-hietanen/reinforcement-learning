{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_state(index: int, P: np.ndarray) -> int:\n",
    "    probs = P[index]\n",
    "\n",
    "    next = np.random.choice(range(P.shape[1]), p=probs)\n",
    "\n",
    "    return int(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_loop(\n",
    "        n_iter: int, \n",
    "        X: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        P: np.ndarray,\n",
    "        link: Callable[[np.ndarray], np.ndarray], \n",
    "        inv_link: Callable[[np.ndarray], np.ndarray], \n",
    "        gamma: float, \n",
    "        alpha: float,\n",
    "        epsilon: float,\n",
    "    ) -> np.ndarray:\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    w = np.zeros(n_features)\n",
    "\n",
    "    curr_index = int(np.random.randint(n_samples))\n",
    "    curr_x = X[curr_index]\n",
    "    curr_y = y[curr_index]\n",
    "\n",
    "    i = 0\n",
    "    grad = np.ones_like(w) * np.inf\n",
    "\n",
    "    while i < n_iter and np.linalg.norm(alpha * grad, 2) > epsilon:\n",
    "        # Next state samples\n",
    "        next_index = sample_next_state(index=curr_index, P=P)\n",
    "        next_x = X[next_index]\n",
    "        next_y = y[next_index]\n",
    "\n",
    "        # Find predictions\n",
    "        curr_z = np.dot(curr_x, w)\n",
    "        next_z = np.dot(next_x, w)\n",
    "\n",
    "        # Find rewards\n",
    "        r = inv_link(curr_y) - gamma * inv_link(next_y)\n",
    "\n",
    "        # TD target\n",
    "        z_t = r + gamma * next_z\n",
    "\n",
    "        # Find gradient\n",
    "        grad = (link(curr_z) - link(z_t)) * curr_x\n",
    "\n",
    "        # Update weights\n",
    "        w -= alpha * grad\n",
    "        \n",
    "        # Update state and index\n",
    "        curr_index, curr_x, curr_y = next_index, next_x, next_y\n",
    "        i += 1\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.99366542   5.00578355   0.63581661   2.44872684 -10.31186138\n",
      "  17.51763825   1.03114679 -17.77574728   7.34087816  -6.42763345\n",
      "  -8.79966234   3.03563353 -20.51935608]\n",
      "Intercept: 29.04431567479631\n",
      "Model R^2 score: 0.7730570034661727\n"
     ]
    }
   ],
   "source": [
    "housedata = np.loadtxt('data\\\\readyhousedata.txt', delimiter=',')\n",
    "\n",
    "X = housedata[:, :-1]\n",
    "y = housedata[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "reg = LinearRegression() \n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "weights = reg.coef_\n",
    "intercept = reg.intercept_\n",
    "\n",
    "print(weights)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "score = reg.score(X_test, y_test)\n",
    "print(\"Model R^2 score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8.53979376   5.54501021   4.91634298   2.22193504  -4.23974712\n",
      "  36.36392932   3.46280445  -5.98104928   6.59621793  -5.85474065\n",
      "  -3.89650886  10.11729015 -11.9291673 ]\n"
     ]
    }
   ],
   "source": [
    "num_samples = X_train.shape[0]\n",
    "P = np.ones((num_samples, num_samples)) / num_samples # Equal probability to move to any state\n",
    "\n",
    "alpha = 0.01  # Learning rate\n",
    "gamma = 0   # Discount factor\n",
    "num_iterations = 1e6  # Number of iterations\n",
    "epsilon = 1e-9\n",
    "\n",
    "w_hat_house = td_loop(\n",
    "    n_iter=num_iterations,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    P=P,\n",
    "    link=lambda x : x,\n",
    "    inv_link=lambda x : x,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "\n",
    "print(w_hat_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights learned using SGD:\n",
      " [-10.99123084   5.0108542    0.63526613   2.44703682 -10.31842922\n",
      "  17.498233     1.03114665 -17.7785983    7.33932725  -6.43290669\n",
      "  -8.8054347    3.03159724 -20.52845324]\n"
     ]
    }
   ],
   "source": [
    "def mini_batch_sgd(X: np.ndarray, y: np.ndarray, learning_rate: float, n_iter: int, epsilon: float, batch_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Performs Mini-Batch Stochastic Gradient Descent (SGD) for linear regression with shuffling.\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.ndarray): Feature matrix (n_samples, n_features).\n",
    "    y (np.ndarray): Target vector (n_samples,).\n",
    "    learning_rate (float): Step size for updating weights.\n",
    "    n_iter (int): Number of iterations (epochs).\n",
    "    epsilon (float): Convergence threshold.\n",
    "    batch_size (int): Size of the mini-batches.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The learned weights.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    # Add bias term to the feature matrix\n",
    "    X_bias = np.c_[np.ones(n_samples), X]  # Adds a column of ones for the bias term\n",
    "\n",
    "    # Initialize weights to zeros\n",
    "    weights = np.zeros(n_features + 1)\n",
    "\n",
    "    for epoch in range(int(n_iter)):\n",
    "        # Step 1: Shuffle the data at the beginning of each epoch\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_bias_shuffled = X_bias[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            # Step 2: Select the mini-batch\n",
    "            X_batch = X_bias_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "            \n",
    "            # Initialize gradients to zero\n",
    "            gradient = np.zeros_like(weights)\n",
    "            \n",
    "            # Step 3: Compute the gradient over the mini-batch\n",
    "            for j in range(X_batch.shape[0]):\n",
    "                # Prediction\n",
    "                prediction = np.dot(X_batch[j], weights)\n",
    "                error = y_batch[j] - prediction\n",
    "                \n",
    "                # Update gradient\n",
    "                gradient += -2 * X_batch[j] * error\n",
    "\n",
    "            # Step 4: Update the weights\n",
    "            weights -= learning_rate * gradient / batch_size\n",
    "\n",
    "        # Optional: Check for convergence (if gradient is small enough)\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            print(f\"Converged after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_iter = 1e4  # Number of epochs\n",
    "epsilon = 1e-6  # Convergence threshold\n",
    "batch_size = 16  # Mini-batch size\n",
    "\n",
    "# Run SGD manually\n",
    "weights_sgd = mini_batch_sgd(X_train, y_train, learning_rate, n_iter, epsilon, batch_size)\n",
    "\n",
    "# Print the learned weights\n",
    "print(\"Weights learned using SGD:\\n\", weights_sgd[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test set using TD: 4.65389069063977\n",
      "RMSE on the test set using L2 Regression: 4.154652986817322\n",
      "RMSE on the test set using SGD: 4.153674732376023\n",
      "---------------\n",
      "Norm of weights from TD: 43.11024087417123\n",
      "Norm of weights from L2 Regression: 38.54163631701667\n",
      "Norm of weights from SGD: 38.54217165140593\n",
      "Norm of difference in weights for L2: 26.65769219399023\n",
      "Norm of difference in weights for sgd: 26.679054613373875\n"
     ]
    }
   ],
   "source": [
    "pred_TD = np.dot(X_test, w_hat_house)\n",
    "pred_L2 = reg.predict(X_test)\n",
    "pred_sgd = np.dot(X_test, weights_sgd[1:]) + weights_sgd[0]\n",
    "\n",
    "rmse_TD = np.sqrt(mean_squared_error(y_test, pred_TD))\n",
    "rmse_L2 = np.sqrt(mean_squared_error(y_test, pred_L2))\n",
    "rmse_sgd = np.sqrt(mean_squared_error(y_test, pred_sgd))\n",
    "\n",
    "print(\"RMSE on the test set using TD:\", rmse_TD)\n",
    "print(\"RMSE on the test set using L2 Regression:\", rmse_L2)\n",
    "print(\"RMSE on the test set using SGD:\", rmse_sgd)\n",
    "print(\"---------------\")\n",
    "print(\"Norm of weights from TD:\", np.linalg.norm(w_hat_house, 2))\n",
    "print(\"Norm of weights from L2 Regression:\", np.linalg.norm(weights, 2))\n",
    "print(\"Norm of weights from SGD:\", np.linalg.norm(weights_sgd[1:], 2))\n",
    "print(\"Norm of difference in weights for L2:\", np.linalg.norm(weights - w_hat_house, 2))\n",
    "print(\"Norm of difference in weights for sgd:\", np.linalg.norm(w_hat_house - weights_sgd[1:], 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
