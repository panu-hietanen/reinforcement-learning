{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.NeuralNet import ThreeLayerFCNN_Adam, ThreeLayerFCNN_SGD\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the housing data\n",
    "housedata = np.loadtxt('data\\\\readyhousedata.txt', delimiter=',')\n",
    "\n",
    "# Separate features and target\n",
    "X = housedata[:, :-1]\n",
    "y = housedata[:, -1]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Split dara\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the number of samples and features from the training set\n",
    "n_samples, n_features = X_train.shape\n",
    "hidden_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Data loader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create test data loader for evaluation\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "num_epochs = 800\n",
    "\n",
    "model = ThreeLayerFCNN_Adam(\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    n_epochs=num_epochs,\n",
    "    input_size=n_features, \n",
    "    hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/800], Loss: 422.7016\n",
      "Epoch [2/800], Loss: 141.1096\n",
      "Epoch [3/800], Loss: 74.0719\n",
      "Epoch [4/800], Loss: 58.9036\n",
      "Epoch [5/800], Loss: 52.2917\n",
      "Epoch [6/800], Loss: 46.5174\n",
      "Epoch [7/800], Loss: 41.5657\n",
      "Epoch [8/800], Loss: 34.9947\n",
      "Epoch [9/800], Loss: 31.1324\n",
      "Epoch [10/800], Loss: 27.0948\n",
      "Epoch [11/800], Loss: 24.1451\n",
      "Epoch [12/800], Loss: 23.4091\n",
      "Epoch [13/800], Loss: 22.8011\n",
      "Epoch [14/800], Loss: 21.8818\n",
      "Epoch [15/800], Loss: 20.2160\n",
      "Epoch [16/800], Loss: 20.2283\n",
      "Epoch [17/800], Loss: 20.2516\n",
      "Epoch [18/800], Loss: 20.5126\n",
      "Epoch [19/800], Loss: 21.2260\n",
      "Epoch [20/800], Loss: 18.9054\n",
      "Epoch [21/800], Loss: 17.5200\n",
      "Epoch [22/800], Loss: 17.5324\n",
      "Epoch [23/800], Loss: 17.4050\n",
      "Epoch [24/800], Loss: 17.3084\n",
      "Epoch [25/800], Loss: 17.7051\n",
      "Epoch [26/800], Loss: 16.9653\n",
      "Epoch [27/800], Loss: 16.4373\n",
      "Epoch [28/800], Loss: 16.6725\n",
      "Epoch [29/800], Loss: 17.7126\n",
      "Epoch [30/800], Loss: 16.2915\n",
      "Epoch [31/800], Loss: 16.5217\n",
      "Epoch [32/800], Loss: 15.2639\n",
      "Epoch [33/800], Loss: 13.8762\n",
      "Epoch [34/800], Loss: 15.3709\n",
      "Epoch [35/800], Loss: 15.0001\n",
      "Epoch [36/800], Loss: 14.4874\n",
      "Epoch [37/800], Loss: 14.5217\n",
      "Epoch [38/800], Loss: 15.4311\n",
      "Epoch [39/800], Loss: 14.1002\n",
      "Epoch [40/800], Loss: 13.1029\n",
      "Epoch [41/800], Loss: 11.9914\n",
      "Epoch [42/800], Loss: 13.3726\n",
      "Epoch [43/800], Loss: 14.1675\n",
      "Epoch [44/800], Loss: 13.3806\n",
      "Epoch [45/800], Loss: 11.6504\n",
      "Epoch [46/800], Loss: 11.4704\n",
      "Epoch [47/800], Loss: 11.5453\n",
      "Epoch [48/800], Loss: 10.8232\n",
      "Epoch [49/800], Loss: 11.4941\n",
      "Epoch [50/800], Loss: 11.3426\n",
      "Epoch [51/800], Loss: 11.0793\n",
      "Epoch [52/800], Loss: 9.8679\n",
      "Epoch [53/800], Loss: 10.3173\n",
      "Epoch [54/800], Loss: 10.8370\n",
      "Epoch [55/800], Loss: 10.1868\n",
      "Epoch [56/800], Loss: 10.6307\n",
      "Epoch [57/800], Loss: 9.6328\n",
      "Epoch [58/800], Loss: 9.1891\n",
      "Epoch [59/800], Loss: 9.2255\n",
      "Epoch [60/800], Loss: 9.4149\n",
      "Epoch [61/800], Loss: 8.9268\n",
      "Epoch [62/800], Loss: 8.2305\n",
      "Epoch [63/800], Loss: 8.1726\n",
      "Epoch [64/800], Loss: 8.6492\n",
      "Epoch [65/800], Loss: 9.6932\n",
      "Epoch [66/800], Loss: 8.5352\n",
      "Epoch [67/800], Loss: 8.2951\n",
      "Epoch [68/800], Loss: 10.1454\n",
      "Epoch [69/800], Loss: 8.0525\n",
      "Epoch [70/800], Loss: 7.6276\n",
      "Epoch [71/800], Loss: 7.7968\n",
      "Epoch [72/800], Loss: 8.2191\n",
      "Epoch [73/800], Loss: 8.0339\n",
      "Epoch [74/800], Loss: 7.5657\n",
      "Epoch [75/800], Loss: 7.6232\n",
      "Epoch [76/800], Loss: 7.9069\n",
      "Epoch [77/800], Loss: 8.3520\n",
      "Epoch [78/800], Loss: 8.4009\n",
      "Epoch [79/800], Loss: 7.5413\n",
      "Epoch [80/800], Loss: 7.8080\n",
      "Epoch [81/800], Loss: 7.1682\n",
      "Epoch [82/800], Loss: 8.2042\n",
      "Epoch [83/800], Loss: 8.1929\n",
      "Epoch [84/800], Loss: 8.6172\n",
      "Epoch [85/800], Loss: 7.9389\n",
      "Epoch [86/800], Loss: 8.0109\n",
      "Epoch [87/800], Loss: 10.3570\n",
      "Epoch [88/800], Loss: 7.4408\n",
      "Epoch [89/800], Loss: 6.6770\n",
      "Epoch [90/800], Loss: 7.9418\n",
      "Epoch [91/800], Loss: 8.7032\n",
      "Epoch [92/800], Loss: 6.9448\n",
      "Epoch [93/800], Loss: 6.6577\n",
      "Epoch [94/800], Loss: 6.9632\n",
      "Epoch [95/800], Loss: 6.8096\n",
      "Epoch [96/800], Loss: 6.8838\n",
      "Epoch [97/800], Loss: 6.7357\n",
      "Epoch [98/800], Loss: 6.3934\n",
      "Epoch [99/800], Loss: 7.2957\n",
      "Epoch [100/800], Loss: 6.8791\n",
      "Epoch [101/800], Loss: 7.8791\n",
      "Epoch [102/800], Loss: 7.6136\n",
      "Epoch [103/800], Loss: 8.6411\n",
      "Epoch [104/800], Loss: 6.3812\n",
      "Epoch [105/800], Loss: 6.2725\n",
      "Epoch [106/800], Loss: 7.8568\n",
      "Epoch [107/800], Loss: 6.8689\n",
      "Epoch [108/800], Loss: 7.2035\n",
      "Epoch [109/800], Loss: 8.5363\n",
      "Epoch [110/800], Loss: 8.5126\n",
      "Epoch [111/800], Loss: 6.7013\n",
      "Epoch [112/800], Loss: 6.0058\n",
      "Epoch [113/800], Loss: 6.5361\n",
      "Epoch [114/800], Loss: 6.8833\n",
      "Epoch [115/800], Loss: 7.2202\n",
      "Epoch [116/800], Loss: 6.2857\n",
      "Epoch [117/800], Loss: 5.8323\n",
      "Epoch [118/800], Loss: 6.2034\n",
      "Epoch [119/800], Loss: 6.4237\n",
      "Epoch [120/800], Loss: 5.7130\n",
      "Epoch [121/800], Loss: 6.9981\n",
      "Epoch [122/800], Loss: 7.6000\n",
      "Epoch [123/800], Loss: 6.7051\n",
      "Epoch [124/800], Loss: 6.0365\n",
      "Epoch [125/800], Loss: 5.9118\n",
      "Epoch [126/800], Loss: 6.0490\n",
      "Epoch [127/800], Loss: 6.0644\n",
      "Epoch [128/800], Loss: 5.7532\n",
      "Epoch [129/800], Loss: 5.9272\n",
      "Epoch [130/800], Loss: 6.3356\n",
      "Epoch [131/800], Loss: 6.4414\n",
      "Epoch [132/800], Loss: 6.0389\n",
      "Epoch [133/800], Loss: 5.8094\n",
      "Epoch [134/800], Loss: 5.9599\n",
      "Epoch [135/800], Loss: 7.6608\n",
      "Epoch [136/800], Loss: 5.8259\n",
      "Epoch [137/800], Loss: 6.5209\n",
      "Epoch [138/800], Loss: 5.6969\n",
      "Epoch [139/800], Loss: 5.5044\n",
      "Epoch [140/800], Loss: 5.4736\n",
      "Epoch [141/800], Loss: 5.6071\n",
      "Epoch [142/800], Loss: 5.4699\n",
      "Epoch [143/800], Loss: 5.2739\n",
      "Epoch [144/800], Loss: 5.2077\n",
      "Epoch [145/800], Loss: 6.5060\n",
      "Epoch [146/800], Loss: 6.9433\n",
      "Epoch [147/800], Loss: 5.8152\n",
      "Epoch [148/800], Loss: 6.1532\n",
      "Epoch [149/800], Loss: 5.6852\n",
      "Epoch [150/800], Loss: 6.1312\n",
      "Epoch [151/800], Loss: 5.4964\n",
      "Epoch [152/800], Loss: 5.9331\n",
      "Epoch [153/800], Loss: 6.0118\n",
      "Epoch [154/800], Loss: 5.8146\n",
      "Epoch [155/800], Loss: 5.5190\n",
      "Epoch [156/800], Loss: 5.2063\n",
      "Epoch [157/800], Loss: 5.1575\n",
      "Epoch [158/800], Loss: 5.4672\n",
      "Epoch [159/800], Loss: 5.4519\n",
      "Epoch [160/800], Loss: 5.2499\n",
      "Epoch [161/800], Loss: 5.4614\n",
      "Epoch [162/800], Loss: 6.3686\n",
      "Epoch [163/800], Loss: 5.5853\n",
      "Epoch [164/800], Loss: 6.1118\n",
      "Epoch [165/800], Loss: 6.4882\n",
      "Epoch [166/800], Loss: 5.9044\n",
      "Epoch [167/800], Loss: 5.9781\n",
      "Epoch [168/800], Loss: 5.9635\n",
      "Epoch [169/800], Loss: 5.4837\n",
      "Epoch [170/800], Loss: 4.8587\n",
      "Epoch [171/800], Loss: 4.8841\n",
      "Epoch [172/800], Loss: 5.1589\n",
      "Epoch [173/800], Loss: 5.5538\n",
      "Epoch [174/800], Loss: 5.9384\n",
      "Epoch [175/800], Loss: 6.1819\n",
      "Epoch [176/800], Loss: 5.5453\n",
      "Epoch [177/800], Loss: 5.5274\n",
      "Epoch [178/800], Loss: 6.0044\n",
      "Epoch [179/800], Loss: 5.5277\n",
      "Epoch [180/800], Loss: 6.7880\n",
      "Epoch [181/800], Loss: 8.0332\n",
      "Epoch [182/800], Loss: 5.9304\n",
      "Epoch [183/800], Loss: 5.5712\n",
      "Epoch [184/800], Loss: 6.2689\n",
      "Epoch [185/800], Loss: 5.7174\n",
      "Epoch [186/800], Loss: 5.1715\n",
      "Epoch [187/800], Loss: 5.0725\n",
      "Epoch [188/800], Loss: 4.8968\n",
      "Epoch [189/800], Loss: 4.9245\n",
      "Epoch [190/800], Loss: 4.8924\n",
      "Epoch [191/800], Loss: 5.2218\n",
      "Epoch [192/800], Loss: 5.6350\n",
      "Epoch [193/800], Loss: 5.2177\n",
      "Epoch [194/800], Loss: 5.1309\n",
      "Epoch [195/800], Loss: 5.0788\n",
      "Epoch [196/800], Loss: 5.3846\n",
      "Epoch [197/800], Loss: 4.9106\n",
      "Epoch [198/800], Loss: 4.9570\n",
      "Epoch [199/800], Loss: 5.1676\n",
      "Epoch [200/800], Loss: 5.7160\n",
      "Epoch [201/800], Loss: 7.0462\n",
      "Epoch [202/800], Loss: 5.8871\n",
      "Epoch [203/800], Loss: 5.8109\n",
      "Epoch [204/800], Loss: 6.1353\n",
      "Epoch [205/800], Loss: 5.9346\n",
      "Epoch [206/800], Loss: 5.7018\n",
      "Epoch [207/800], Loss: 5.6623\n",
      "Epoch [208/800], Loss: 5.4490\n",
      "Epoch [209/800], Loss: 5.0399\n",
      "Epoch [210/800], Loss: 4.8840\n",
      "Epoch [211/800], Loss: 4.9689\n",
      "Epoch [212/800], Loss: 5.1605\n",
      "Epoch [213/800], Loss: 7.5721\n",
      "Epoch [214/800], Loss: 7.2300\n",
      "Epoch [215/800], Loss: 6.0823\n",
      "Epoch [216/800], Loss: 5.0297\n",
      "Epoch [217/800], Loss: 5.2255\n",
      "Epoch [218/800], Loss: 5.3791\n",
      "Epoch [219/800], Loss: 4.9264\n",
      "Epoch [220/800], Loss: 4.4310\n",
      "Epoch [221/800], Loss: 4.3221\n",
      "Epoch [222/800], Loss: 6.2940\n",
      "Epoch [223/800], Loss: 5.5239\n",
      "Epoch [224/800], Loss: 4.6254\n",
      "Epoch [225/800], Loss: 4.4296\n",
      "Epoch [226/800], Loss: 4.5944\n",
      "Epoch [227/800], Loss: 4.6000\n",
      "Epoch [228/800], Loss: 4.2125\n",
      "Epoch [229/800], Loss: 5.3797\n",
      "Epoch [230/800], Loss: 5.5841\n",
      "Epoch [231/800], Loss: 4.4315\n",
      "Epoch [232/800], Loss: 4.6714\n",
      "Epoch [233/800], Loss: 4.7002\n",
      "Epoch [234/800], Loss: 4.7183\n",
      "Epoch [235/800], Loss: 4.6996\n",
      "Epoch [236/800], Loss: 4.9366\n",
      "Epoch [237/800], Loss: 4.3085\n",
      "Epoch [238/800], Loss: 4.4735\n",
      "Epoch [239/800], Loss: 3.9893\n",
      "Epoch [240/800], Loss: 4.1839\n",
      "Epoch [241/800], Loss: 4.2911\n",
      "Epoch [242/800], Loss: 4.2850\n",
      "Epoch [243/800], Loss: 5.3807\n",
      "Epoch [244/800], Loss: 4.6852\n",
      "Epoch [245/800], Loss: 4.5445\n",
      "Epoch [246/800], Loss: 5.0109\n",
      "Epoch [247/800], Loss: 4.2354\n",
      "Epoch [248/800], Loss: 3.9942\n",
      "Epoch [249/800], Loss: 4.1853\n",
      "Epoch [250/800], Loss: 3.9156\n",
      "Epoch [251/800], Loss: 4.0573\n",
      "Epoch [252/800], Loss: 5.2222\n",
      "Epoch [253/800], Loss: 5.2636\n",
      "Epoch [254/800], Loss: 5.0672\n",
      "Epoch [255/800], Loss: 4.6283\n",
      "Epoch [256/800], Loss: 4.2368\n",
      "Epoch [257/800], Loss: 3.8594\n",
      "Epoch [258/800], Loss: 3.7328\n",
      "Epoch [259/800], Loss: 3.6187\n",
      "Epoch [260/800], Loss: 3.8405\n",
      "Epoch [261/800], Loss: 4.0374\n",
      "Epoch [262/800], Loss: 4.9005\n",
      "Epoch [263/800], Loss: 3.7666\n",
      "Epoch [264/800], Loss: 3.5200\n",
      "Epoch [265/800], Loss: 3.5653\n",
      "Epoch [266/800], Loss: 5.0622\n",
      "Epoch [267/800], Loss: 5.0987\n",
      "Epoch [268/800], Loss: 4.8599\n",
      "Epoch [269/800], Loss: 4.4392\n",
      "Epoch [270/800], Loss: 4.6420\n",
      "Epoch [271/800], Loss: 3.6067\n",
      "Epoch [272/800], Loss: 3.6296\n",
      "Epoch [273/800], Loss: 3.8136\n",
      "Epoch [274/800], Loss: 3.9175\n",
      "Epoch [275/800], Loss: 3.5654\n",
      "Epoch [276/800], Loss: 4.1719\n",
      "Epoch [277/800], Loss: 3.5461\n",
      "Epoch [278/800], Loss: 4.0242\n",
      "Epoch [279/800], Loss: 4.0169\n",
      "Epoch [280/800], Loss: 4.5272\n",
      "Epoch [281/800], Loss: 7.5381\n",
      "Epoch [282/800], Loss: 5.4787\n",
      "Epoch [283/800], Loss: 6.0303\n",
      "Epoch [284/800], Loss: 4.6723\n",
      "Epoch [285/800], Loss: 5.0935\n",
      "Epoch [286/800], Loss: 4.4912\n",
      "Epoch [287/800], Loss: 3.6422\n",
      "Epoch [288/800], Loss: 4.1132\n",
      "Epoch [289/800], Loss: 3.8813\n",
      "Epoch [290/800], Loss: 3.5714\n",
      "Epoch [291/800], Loss: 4.3858\n",
      "Epoch [292/800], Loss: 3.8860\n",
      "Epoch [293/800], Loss: 3.6154\n",
      "Epoch [294/800], Loss: 3.7409\n",
      "Epoch [295/800], Loss: 3.6759\n",
      "Epoch [296/800], Loss: 3.9875\n",
      "Epoch [297/800], Loss: 3.5270\n",
      "Epoch [298/800], Loss: 3.6235\n",
      "Epoch [299/800], Loss: 3.5284\n",
      "Epoch [300/800], Loss: 3.3533\n",
      "Epoch [301/800], Loss: 3.3447\n",
      "Epoch [302/800], Loss: 4.0195\n",
      "Epoch [303/800], Loss: 3.4425\n",
      "Epoch [304/800], Loss: 3.4048\n",
      "Epoch [305/800], Loss: 4.2308\n",
      "Epoch [306/800], Loss: 3.7445\n",
      "Epoch [307/800], Loss: 3.5470\n",
      "Epoch [308/800], Loss: 3.4148\n",
      "Epoch [309/800], Loss: 3.3411\n",
      "Epoch [310/800], Loss: 3.2923\n",
      "Epoch [311/800], Loss: 3.5104\n",
      "Epoch [312/800], Loss: 3.3038\n",
      "Epoch [313/800], Loss: 2.9546\n",
      "Epoch [314/800], Loss: 3.1487\n",
      "Epoch [315/800], Loss: 3.1979\n",
      "Epoch [316/800], Loss: 3.2648\n",
      "Epoch [317/800], Loss: 4.6204\n",
      "Epoch [318/800], Loss: 4.3254\n",
      "Epoch [319/800], Loss: 3.9997\n",
      "Epoch [320/800], Loss: 3.3235\n",
      "Epoch [321/800], Loss: 3.1224\n",
      "Epoch [322/800], Loss: 3.1714\n",
      "Epoch [323/800], Loss: 3.0830\n",
      "Epoch [324/800], Loss: 3.3033\n",
      "Epoch [325/800], Loss: 4.1214\n",
      "Epoch [326/800], Loss: 3.2056\n",
      "Epoch [327/800], Loss: 3.3498\n",
      "Epoch [328/800], Loss: 3.3285\n",
      "Epoch [329/800], Loss: 2.7539\n",
      "Epoch [330/800], Loss: 2.9593\n",
      "Epoch [331/800], Loss: 2.9180\n",
      "Epoch [332/800], Loss: 3.4784\n",
      "Epoch [333/800], Loss: 3.2174\n",
      "Epoch [334/800], Loss: 3.8030\n",
      "Epoch [335/800], Loss: 3.7085\n",
      "Epoch [336/800], Loss: 2.8528\n",
      "Epoch [337/800], Loss: 3.0826\n",
      "Epoch [338/800], Loss: 3.1729\n",
      "Epoch [339/800], Loss: 3.0828\n",
      "Epoch [340/800], Loss: 2.8584\n",
      "Epoch [341/800], Loss: 2.7493\n",
      "Epoch [342/800], Loss: 3.1589\n",
      "Epoch [343/800], Loss: 2.6509\n",
      "Epoch [344/800], Loss: 3.0405\n",
      "Epoch [345/800], Loss: 4.1335\n",
      "Epoch [346/800], Loss: 3.1419\n",
      "Epoch [347/800], Loss: 3.0143\n",
      "Epoch [348/800], Loss: 2.8474\n",
      "Epoch [349/800], Loss: 3.4857\n",
      "Epoch [350/800], Loss: 3.3691\n",
      "Epoch [351/800], Loss: 3.7629\n",
      "Epoch [352/800], Loss: 2.9591\n",
      "Epoch [353/800], Loss: 2.9137\n",
      "Epoch [354/800], Loss: 2.7450\n",
      "Epoch [355/800], Loss: 3.3257\n",
      "Epoch [356/800], Loss: 3.0482\n",
      "Epoch [357/800], Loss: 3.0105\n",
      "Epoch [358/800], Loss: 3.0082\n",
      "Epoch [359/800], Loss: 2.9245\n",
      "Epoch [360/800], Loss: 3.1416\n",
      "Epoch [361/800], Loss: 2.7887\n",
      "Epoch [362/800], Loss: 2.8246\n",
      "Epoch [363/800], Loss: 2.7285\n",
      "Epoch [364/800], Loss: 2.9760\n",
      "Epoch [365/800], Loss: 3.2113\n",
      "Epoch [366/800], Loss: 2.8609\n",
      "Epoch [367/800], Loss: 2.8219\n",
      "Epoch [368/800], Loss: 2.6941\n",
      "Epoch [369/800], Loss: 2.8127\n",
      "Epoch [370/800], Loss: 3.3082\n",
      "Epoch [371/800], Loss: 2.9865\n",
      "Epoch [372/800], Loss: 3.5834\n",
      "Epoch [373/800], Loss: 3.0881\n",
      "Epoch [374/800], Loss: 3.5166\n",
      "Epoch [375/800], Loss: 2.7181\n",
      "Epoch [376/800], Loss: 2.8284\n",
      "Epoch [377/800], Loss: 2.8038\n",
      "Epoch [378/800], Loss: 2.5819\n",
      "Epoch [379/800], Loss: 2.4248\n",
      "Epoch [380/800], Loss: 2.4879\n",
      "Epoch [381/800], Loss: 3.0216\n",
      "Epoch [382/800], Loss: 2.7151\n",
      "Epoch [383/800], Loss: 2.7986\n",
      "Epoch [384/800], Loss: 2.5243\n",
      "Epoch [385/800], Loss: 2.5466\n",
      "Epoch [386/800], Loss: 2.5119\n",
      "Epoch [387/800], Loss: 2.6018\n",
      "Epoch [388/800], Loss: 2.2460\n",
      "Epoch [389/800], Loss: 2.4145\n",
      "Epoch [390/800], Loss: 2.7489\n",
      "Epoch [391/800], Loss: 2.7499\n",
      "Epoch [392/800], Loss: 2.6581\n",
      "Epoch [393/800], Loss: 2.4740\n",
      "Epoch [394/800], Loss: 2.5420\n",
      "Epoch [395/800], Loss: 2.6345\n",
      "Epoch [396/800], Loss: 2.5990\n",
      "Epoch [397/800], Loss: 2.7595\n",
      "Epoch [398/800], Loss: 2.5973\n",
      "Epoch [399/800], Loss: 2.5279\n",
      "Epoch [400/800], Loss: 2.1821\n",
      "Epoch [401/800], Loss: 2.4959\n",
      "Epoch [402/800], Loss: 2.8400\n",
      "Epoch [403/800], Loss: 2.7892\n",
      "Epoch [404/800], Loss: 2.8855\n",
      "Epoch [405/800], Loss: 2.8187\n",
      "Epoch [406/800], Loss: 2.9359\n",
      "Epoch [407/800], Loss: 3.2175\n",
      "Epoch [408/800], Loss: 2.9524\n",
      "Epoch [409/800], Loss: 2.8896\n",
      "Epoch [410/800], Loss: 2.5458\n",
      "Epoch [411/800], Loss: 2.3679\n",
      "Epoch [412/800], Loss: 2.4431\n",
      "Epoch [413/800], Loss: 2.5570\n",
      "Epoch [414/800], Loss: 2.1700\n",
      "Epoch [415/800], Loss: 2.5001\n",
      "Epoch [416/800], Loss: 2.4562\n",
      "Epoch [417/800], Loss: 2.8308\n",
      "Epoch [418/800], Loss: 2.4780\n",
      "Epoch [419/800], Loss: 2.1431\n",
      "Epoch [420/800], Loss: 2.1824\n",
      "Epoch [421/800], Loss: 2.0066\n",
      "Epoch [422/800], Loss: 2.0436\n",
      "Epoch [423/800], Loss: 2.3772\n",
      "Epoch [424/800], Loss: 2.2598\n",
      "Epoch [425/800], Loss: 2.0377\n",
      "Epoch [426/800], Loss: 2.3234\n",
      "Epoch [427/800], Loss: 3.0481\n",
      "Epoch [428/800], Loss: 2.5064\n",
      "Epoch [429/800], Loss: 2.5166\n",
      "Epoch [430/800], Loss: 2.2136\n",
      "Epoch [431/800], Loss: 2.3492\n",
      "Epoch [432/800], Loss: 2.4092\n",
      "Epoch [433/800], Loss: 2.1852\n",
      "Epoch [434/800], Loss: 2.1622\n",
      "Epoch [435/800], Loss: 2.0510\n",
      "Epoch [436/800], Loss: 2.6185\n",
      "Epoch [437/800], Loss: 2.3830\n",
      "Epoch [438/800], Loss: 2.3279\n",
      "Epoch [439/800], Loss: 2.4421\n",
      "Epoch [440/800], Loss: 2.2624\n",
      "Epoch [441/800], Loss: 2.1266\n",
      "Epoch [442/800], Loss: 2.0967\n",
      "Epoch [443/800], Loss: 2.5175\n",
      "Epoch [444/800], Loss: 2.5331\n",
      "Epoch [445/800], Loss: 2.7889\n",
      "Epoch [446/800], Loss: 2.2116\n",
      "Epoch [447/800], Loss: 2.0681\n",
      "Epoch [448/800], Loss: 2.1996\n",
      "Epoch [449/800], Loss: 2.2183\n",
      "Epoch [450/800], Loss: 1.9889\n",
      "Epoch [451/800], Loss: 2.0082\n",
      "Epoch [452/800], Loss: 2.0508\n",
      "Epoch [453/800], Loss: 2.1114\n",
      "Epoch [454/800], Loss: 2.1138\n",
      "Epoch [455/800], Loss: 2.9063\n",
      "Epoch [456/800], Loss: 3.3940\n",
      "Epoch [457/800], Loss: 2.7485\n",
      "Epoch [458/800], Loss: 3.1464\n",
      "Epoch [459/800], Loss: 3.5089\n",
      "Epoch [460/800], Loss: 2.4959\n",
      "Epoch [461/800], Loss: 2.5858\n",
      "Epoch [462/800], Loss: 2.2607\n",
      "Epoch [463/800], Loss: 2.2118\n",
      "Epoch [464/800], Loss: 2.1198\n",
      "Epoch [465/800], Loss: 1.9948\n",
      "Epoch [466/800], Loss: 2.0253\n",
      "Epoch [467/800], Loss: 2.1262\n",
      "Epoch [468/800], Loss: 2.1524\n",
      "Epoch [469/800], Loss: 2.1424\n",
      "Epoch [470/800], Loss: 2.3194\n",
      "Epoch [471/800], Loss: 2.4884\n",
      "Epoch [472/800], Loss: 2.5063\n",
      "Epoch [473/800], Loss: 2.6519\n",
      "Epoch [474/800], Loss: 3.8053\n",
      "Epoch [475/800], Loss: 3.0604\n",
      "Epoch [476/800], Loss: 2.7566\n",
      "Epoch [477/800], Loss: 3.7374\n",
      "Epoch [478/800], Loss: 3.2165\n",
      "Epoch [479/800], Loss: 2.4861\n",
      "Epoch [480/800], Loss: 2.2692\n",
      "Epoch [481/800], Loss: 2.6237\n",
      "Epoch [482/800], Loss: 2.0802\n",
      "Epoch [483/800], Loss: 1.9195\n",
      "Epoch [484/800], Loss: 2.2276\n",
      "Epoch [485/800], Loss: 2.8424\n",
      "Epoch [486/800], Loss: 4.3886\n",
      "Epoch [487/800], Loss: 3.4186\n",
      "Epoch [488/800], Loss: 2.8649\n",
      "Epoch [489/800], Loss: 2.9005\n",
      "Epoch [490/800], Loss: 3.3617\n",
      "Epoch [491/800], Loss: 2.2537\n",
      "Epoch [492/800], Loss: 2.3569\n",
      "Epoch [493/800], Loss: 2.1410\n",
      "Epoch [494/800], Loss: 2.3575\n",
      "Epoch [495/800], Loss: 1.9888\n",
      "Epoch [496/800], Loss: 2.1746\n",
      "Epoch [497/800], Loss: 2.2962\n",
      "Epoch [498/800], Loss: 2.5302\n",
      "Epoch [499/800], Loss: 2.2349\n",
      "Epoch [500/800], Loss: 2.1990\n",
      "Epoch [501/800], Loss: 2.2008\n",
      "Epoch [502/800], Loss: 1.9513\n",
      "Epoch [503/800], Loss: 2.1860\n",
      "Epoch [504/800], Loss: 2.0978\n",
      "Epoch [505/800], Loss: 2.2113\n",
      "Epoch [506/800], Loss: 2.3273\n",
      "Epoch [507/800], Loss: 2.0889\n",
      "Epoch [508/800], Loss: 2.2268\n",
      "Epoch [509/800], Loss: 1.8928\n",
      "Epoch [510/800], Loss: 1.8219\n",
      "Epoch [511/800], Loss: 1.8279\n",
      "Epoch [512/800], Loss: 2.3406\n",
      "Epoch [513/800], Loss: 2.4590\n",
      "Epoch [514/800], Loss: 2.8191\n",
      "Epoch [515/800], Loss: 2.3581\n",
      "Epoch [516/800], Loss: 2.3598\n",
      "Epoch [517/800], Loss: 2.3628\n",
      "Epoch [518/800], Loss: 2.2501\n",
      "Epoch [519/800], Loss: 2.5721\n",
      "Epoch [520/800], Loss: 2.8918\n",
      "Epoch [521/800], Loss: 2.1236\n",
      "Epoch [522/800], Loss: 2.0852\n",
      "Epoch [523/800], Loss: 2.5711\n",
      "Epoch [524/800], Loss: 2.3546\n",
      "Epoch [525/800], Loss: 2.2856\n",
      "Epoch [526/800], Loss: 2.5699\n",
      "Epoch [527/800], Loss: 2.0419\n",
      "Epoch [528/800], Loss: 2.1620\n",
      "Epoch [529/800], Loss: 3.1035\n",
      "Epoch [530/800], Loss: 3.2965\n",
      "Epoch [531/800], Loss: 2.2743\n",
      "Epoch [532/800], Loss: 2.3762\n",
      "Epoch [533/800], Loss: 2.8686\n",
      "Epoch [534/800], Loss: 2.0952\n",
      "Epoch [535/800], Loss: 2.0864\n",
      "Epoch [536/800], Loss: 1.6731\n",
      "Epoch [537/800], Loss: 1.8008\n",
      "Epoch [538/800], Loss: 1.8233\n",
      "Epoch [539/800], Loss: 2.0847\n",
      "Epoch [540/800], Loss: 2.0062\n",
      "Epoch [541/800], Loss: 2.2218\n",
      "Epoch [542/800], Loss: 2.0070\n",
      "Epoch [543/800], Loss: 2.0765\n",
      "Epoch [544/800], Loss: 1.8820\n",
      "Epoch [545/800], Loss: 2.1363\n",
      "Epoch [546/800], Loss: 1.9611\n",
      "Epoch [547/800], Loss: 2.7180\n",
      "Epoch [548/800], Loss: 2.1058\n",
      "Epoch [549/800], Loss: 2.0374\n",
      "Epoch [550/800], Loss: 2.2290\n",
      "Epoch [551/800], Loss: 2.1081\n",
      "Epoch [552/800], Loss: 2.4144\n",
      "Epoch [553/800], Loss: 1.9680\n",
      "Epoch [554/800], Loss: 2.1097\n",
      "Epoch [555/800], Loss: 3.3127\n",
      "Epoch [556/800], Loss: 3.6576\n",
      "Epoch [557/800], Loss: 3.6432\n",
      "Epoch [558/800], Loss: 2.6031\n",
      "Epoch [559/800], Loss: 2.2365\n",
      "Epoch [560/800], Loss: 2.0618\n",
      "Epoch [561/800], Loss: 2.2805\n",
      "Epoch [562/800], Loss: 2.3877\n",
      "Epoch [563/800], Loss: 2.5600\n",
      "Epoch [564/800], Loss: 2.1060\n",
      "Epoch [565/800], Loss: 2.4159\n",
      "Epoch [566/800], Loss: 1.7657\n",
      "Epoch [567/800], Loss: 1.6926\n",
      "Epoch [568/800], Loss: 1.8308\n",
      "Epoch [569/800], Loss: 1.8874\n",
      "Epoch [570/800], Loss: 1.8103\n",
      "Epoch [571/800], Loss: 2.3036\n",
      "Epoch [572/800], Loss: 2.6048\n",
      "Epoch [573/800], Loss: 2.1165\n",
      "Epoch [574/800], Loss: 1.9786\n",
      "Epoch [575/800], Loss: 1.8507\n",
      "Epoch [576/800], Loss: 1.8999\n",
      "Epoch [577/800], Loss: 1.7402\n",
      "Epoch [578/800], Loss: 1.8993\n",
      "Epoch [579/800], Loss: 1.8688\n",
      "Epoch [580/800], Loss: 1.6960\n",
      "Epoch [581/800], Loss: 1.6600\n",
      "Epoch [582/800], Loss: 1.6409\n",
      "Epoch [583/800], Loss: 1.7563\n",
      "Epoch [584/800], Loss: 2.0834\n",
      "Epoch [585/800], Loss: 1.9055\n",
      "Epoch [586/800], Loss: 1.9652\n",
      "Epoch [587/800], Loss: 1.7492\n",
      "Epoch [588/800], Loss: 1.7919\n",
      "Epoch [589/800], Loss: 1.6259\n",
      "Epoch [590/800], Loss: 1.7635\n",
      "Epoch [591/800], Loss: 2.0898\n",
      "Epoch [592/800], Loss: 2.7264\n",
      "Epoch [593/800], Loss: 1.8679\n",
      "Epoch [594/800], Loss: 1.7146\n",
      "Epoch [595/800], Loss: 1.7905\n",
      "Epoch [596/800], Loss: 1.9775\n",
      "Epoch [597/800], Loss: 1.8879\n",
      "Epoch [598/800], Loss: 2.0056\n",
      "Epoch [599/800], Loss: 1.8651\n",
      "Epoch [600/800], Loss: 1.6470\n",
      "Epoch [601/800], Loss: 2.0429\n",
      "Epoch [602/800], Loss: 1.7399\n",
      "Epoch [603/800], Loss: 1.8160\n",
      "Epoch [604/800], Loss: 1.8015\n",
      "Epoch [605/800], Loss: 1.6807\n",
      "Epoch [606/800], Loss: 1.8831\n",
      "Epoch [607/800], Loss: 2.0372\n",
      "Epoch [608/800], Loss: 2.2367\n",
      "Epoch [609/800], Loss: 2.0025\n",
      "Epoch [610/800], Loss: 1.8042\n",
      "Epoch [611/800], Loss: 1.7484\n",
      "Epoch [612/800], Loss: 1.7350\n",
      "Epoch [613/800], Loss: 2.2479\n",
      "Epoch [614/800], Loss: 2.3519\n",
      "Epoch [615/800], Loss: 2.5821\n",
      "Epoch [616/800], Loss: 2.0329\n",
      "Epoch [617/800], Loss: 2.0485\n",
      "Epoch [618/800], Loss: 2.2783\n",
      "Epoch [619/800], Loss: 2.7044\n",
      "Epoch [620/800], Loss: 2.0900\n",
      "Epoch [621/800], Loss: 2.2232\n",
      "Epoch [622/800], Loss: 2.2120\n",
      "Epoch [623/800], Loss: 2.1592\n",
      "Epoch [624/800], Loss: 2.2551\n",
      "Epoch [625/800], Loss: 2.5949\n",
      "Epoch [626/800], Loss: 2.5502\n",
      "Epoch [627/800], Loss: 2.5613\n",
      "Epoch [628/800], Loss: 2.8329\n",
      "Epoch [629/800], Loss: 2.7765\n",
      "Epoch [630/800], Loss: 2.3745\n",
      "Epoch [631/800], Loss: 2.3400\n",
      "Epoch [632/800], Loss: 2.3119\n",
      "Epoch [633/800], Loss: 2.3548\n",
      "Epoch [634/800], Loss: 1.8759\n",
      "Epoch [635/800], Loss: 1.9547\n",
      "Epoch [636/800], Loss: 2.2326\n",
      "Epoch [637/800], Loss: 2.9962\n",
      "Epoch [638/800], Loss: 3.7320\n",
      "Epoch [639/800], Loss: 2.1966\n",
      "Epoch [640/800], Loss: 2.2297\n",
      "Epoch [641/800], Loss: 1.7119\n",
      "Epoch [642/800], Loss: 1.5815\n",
      "Epoch [643/800], Loss: 1.7519\n",
      "Epoch [644/800], Loss: 2.0025\n",
      "Epoch [645/800], Loss: 2.3851\n",
      "Epoch [646/800], Loss: 2.0614\n",
      "Epoch [647/800], Loss: 1.7354\n",
      "Epoch [648/800], Loss: 1.6550\n",
      "Epoch [649/800], Loss: 1.9038\n",
      "Epoch [650/800], Loss: 2.0006\n",
      "Epoch [651/800], Loss: 2.4952\n",
      "Epoch [652/800], Loss: 1.8639\n",
      "Epoch [653/800], Loss: 1.5921\n",
      "Epoch [654/800], Loss: 1.5298\n",
      "Epoch [655/800], Loss: 1.7404\n",
      "Epoch [656/800], Loss: 2.1119\n",
      "Epoch [657/800], Loss: 1.7554\n",
      "Epoch [658/800], Loss: 1.5419\n",
      "Epoch [659/800], Loss: 1.6903\n",
      "Epoch [660/800], Loss: 1.7626\n",
      "Epoch [661/800], Loss: 2.4449\n",
      "Epoch [662/800], Loss: 3.2897\n",
      "Epoch [663/800], Loss: 2.5622\n",
      "Epoch [664/800], Loss: 1.8162\n",
      "Epoch [665/800], Loss: 2.1343\n",
      "Epoch [666/800], Loss: 1.9346\n",
      "Epoch [667/800], Loss: 1.8268\n",
      "Epoch [668/800], Loss: 1.9909\n",
      "Epoch [669/800], Loss: 1.8027\n",
      "Epoch [670/800], Loss: 1.6688\n",
      "Epoch [671/800], Loss: 1.7991\n",
      "Epoch [672/800], Loss: 1.8637\n",
      "Epoch [673/800], Loss: 1.7995\n",
      "Epoch [674/800], Loss: 1.9797\n",
      "Epoch [675/800], Loss: 1.5727\n",
      "Epoch [676/800], Loss: 1.7343\n",
      "Epoch [677/800], Loss: 1.5212\n",
      "Epoch [678/800], Loss: 1.5686\n",
      "Epoch [679/800], Loss: 1.6176\n",
      "Epoch [680/800], Loss: 1.6784\n",
      "Epoch [681/800], Loss: 1.7184\n",
      "Epoch [682/800], Loss: 1.7018\n",
      "Epoch [683/800], Loss: 1.8294\n",
      "Epoch [684/800], Loss: 1.5348\n",
      "Epoch [685/800], Loss: 1.5856\n",
      "Epoch [686/800], Loss: 1.7242\n",
      "Epoch [687/800], Loss: 1.5509\n",
      "Epoch [688/800], Loss: 1.7430\n",
      "Epoch [689/800], Loss: 1.9987\n",
      "Epoch [690/800], Loss: 2.2288\n",
      "Epoch [691/800], Loss: 1.7284\n",
      "Epoch [692/800], Loss: 1.8023\n",
      "Epoch [693/800], Loss: 1.6600\n",
      "Epoch [694/800], Loss: 1.8303\n",
      "Epoch [695/800], Loss: 1.9243\n",
      "Epoch [696/800], Loss: 1.6783\n",
      "Epoch [697/800], Loss: 1.7016\n",
      "Epoch [698/800], Loss: 1.7812\n",
      "Epoch [699/800], Loss: 1.9617\n",
      "Epoch [700/800], Loss: 1.8778\n",
      "Epoch [701/800], Loss: 1.7608\n",
      "Epoch [702/800], Loss: 1.6926\n",
      "Epoch [703/800], Loss: 1.7150\n",
      "Epoch [704/800], Loss: 1.7736\n",
      "Epoch [705/800], Loss: 1.8777\n",
      "Epoch [706/800], Loss: 1.7951\n",
      "Epoch [707/800], Loss: 1.7846\n",
      "Epoch [708/800], Loss: 2.5072\n",
      "Epoch [709/800], Loss: 2.2658\n",
      "Epoch [710/800], Loss: 2.0473\n",
      "Epoch [711/800], Loss: 2.2077\n",
      "Epoch [712/800], Loss: 2.1643\n",
      "Epoch [713/800], Loss: 1.7701\n",
      "Epoch [714/800], Loss: 1.8914\n",
      "Epoch [715/800], Loss: 2.0381\n",
      "Epoch [716/800], Loss: 2.2052\n",
      "Epoch [717/800], Loss: 1.7400\n",
      "Epoch [718/800], Loss: 1.5706\n",
      "Epoch [719/800], Loss: 1.6981\n",
      "Epoch [720/800], Loss: 1.5498\n",
      "Epoch [721/800], Loss: 1.4165\n",
      "Epoch [722/800], Loss: 1.4203\n",
      "Epoch [723/800], Loss: 1.5906\n",
      "Epoch [724/800], Loss: 1.3903\n",
      "Epoch [725/800], Loss: 1.7008\n",
      "Epoch [726/800], Loss: 1.5799\n",
      "Epoch [727/800], Loss: 1.6654\n",
      "Epoch [728/800], Loss: 1.6665\n",
      "Epoch [729/800], Loss: 1.3776\n",
      "Epoch [730/800], Loss: 1.6066\n",
      "Epoch [731/800], Loss: 1.5400\n",
      "Epoch [732/800], Loss: 1.5266\n",
      "Epoch [733/800], Loss: 1.4107\n",
      "Epoch [734/800], Loss: 1.9365\n",
      "Epoch [735/800], Loss: 1.7213\n",
      "Epoch [736/800], Loss: 1.6541\n",
      "Epoch [737/800], Loss: 1.5446\n",
      "Epoch [738/800], Loss: 1.4927\n",
      "Epoch [739/800], Loss: 1.6472\n",
      "Epoch [740/800], Loss: 1.6110\n",
      "Epoch [741/800], Loss: 1.6843\n",
      "Epoch [742/800], Loss: 1.8445\n",
      "Epoch [743/800], Loss: 1.7948\n",
      "Epoch [744/800], Loss: 1.6001\n",
      "Epoch [745/800], Loss: 1.7212\n",
      "Epoch [746/800], Loss: 1.6021\n",
      "Epoch [747/800], Loss: 1.7166\n",
      "Epoch [748/800], Loss: 1.6007\n",
      "Epoch [749/800], Loss: 1.6732\n",
      "Epoch [750/800], Loss: 1.6757\n",
      "Epoch [751/800], Loss: 1.7534\n",
      "Epoch [752/800], Loss: 1.9742\n",
      "Epoch [753/800], Loss: 1.7686\n",
      "Epoch [754/800], Loss: 1.6218\n",
      "Epoch [755/800], Loss: 1.7458\n",
      "Epoch [756/800], Loss: 1.6090\n",
      "Epoch [757/800], Loss: 1.5385\n",
      "Epoch [758/800], Loss: 1.5804\n",
      "Epoch [759/800], Loss: 1.5223\n",
      "Epoch [760/800], Loss: 1.6539\n",
      "Epoch [761/800], Loss: 1.8609\n",
      "Epoch [762/800], Loss: 1.5106\n",
      "Epoch [763/800], Loss: 1.6710\n",
      "Epoch [764/800], Loss: 1.7763\n",
      "Epoch [765/800], Loss: 1.8855\n",
      "Epoch [766/800], Loss: 1.8875\n",
      "Epoch [767/800], Loss: 1.7418\n",
      "Epoch [768/800], Loss: 1.8842\n",
      "Epoch [769/800], Loss: 1.7744\n",
      "Epoch [770/800], Loss: 1.5846\n",
      "Epoch [771/800], Loss: 1.4154\n",
      "Epoch [772/800], Loss: 1.3112\n",
      "Epoch [773/800], Loss: 1.5704\n",
      "Epoch [774/800], Loss: 1.9869\n",
      "Epoch [775/800], Loss: 1.8176\n",
      "Epoch [776/800], Loss: 1.7488\n",
      "Epoch [777/800], Loss: 1.9554\n",
      "Epoch [778/800], Loss: 1.5684\n",
      "Epoch [779/800], Loss: 1.3886\n",
      "Epoch [780/800], Loss: 1.5336\n",
      "Epoch [781/800], Loss: 1.3451\n",
      "Epoch [782/800], Loss: 1.4651\n",
      "Epoch [783/800], Loss: 1.7006\n",
      "Epoch [784/800], Loss: 2.1020\n",
      "Epoch [785/800], Loss: 1.9160\n",
      "Epoch [786/800], Loss: 1.5667\n",
      "Epoch [787/800], Loss: 1.7169\n",
      "Epoch [788/800], Loss: 1.8347\n",
      "Epoch [789/800], Loss: 1.7574\n",
      "Epoch [790/800], Loss: 1.5662\n",
      "Epoch [791/800], Loss: 1.4734\n",
      "Epoch [792/800], Loss: 1.6634\n",
      "Epoch [793/800], Loss: 1.4615\n",
      "Epoch [794/800], Loss: 1.6054\n",
      "Epoch [795/800], Loss: 1.7851\n",
      "Epoch [796/800], Loss: 2.0815\n",
      "Epoch [797/800], Loss: 1.8510\n",
      "Epoch [798/800], Loss: 1.6133\n",
      "Epoch [799/800], Loss: 1.6192\n",
      "Epoch [800/800], Loss: 1.6763\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.512531280517578\n",
      "6.397020995616913\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model.rmse(X_test, y_test)}\")\n",
    "print(f\"{model.evaluate(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.TD import TD_SGD, TD_Adam\n",
    "\n",
    "num_samples = X_train.shape[0]\n",
    "P = torch.ones((num_samples, num_samples)) / num_samples # Equal probability to move to any state\n",
    "\n",
    "alpha = 0.01  # Learning rate\n",
    "gamma = 0   # Discount factor\n",
    "num_iterations = 1e5  # Number of iterations\n",
    "epsilon = 1e-9\n",
    "\n",
    "td_sgd = TD_SGD(\n",
    "    n_iter=num_iterations,\n",
    "    P=P,\n",
    "    link=lambda x : x,\n",
    "    inv_link=lambda x : x,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "\n",
    "td_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE from TD SGD: 4.139604091644287\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE from TD SGD: {td_sgd.rmse(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESET_FLAG = False\n",
    "\n",
    "if RESET_FLAG:\n",
    "    model.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
