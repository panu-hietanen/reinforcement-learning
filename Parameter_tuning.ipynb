{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2211a6adaf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import Tuning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.TD import TD_Adam, TD_SGD\n",
    "from utils.NeuralNet import TwoLayerFCNN_Adam, TwoLayerFCNN_SGD\n",
    "\n",
    "from utils.Tuning import random_search\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housedata = torch.tensor(np.loadtxt('data\\\\readyhousedata.txt', delimiter=','), dtype=torch.float32)\n",
    "\n",
    "X = housedata[:, :-1]\n",
    "y = housedata[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_param_grid = {\n",
    "    'hidden_size': [32, 64, 128],  \n",
    "    'lr': [0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'n_epochs': [50, 100, 150],\n",
    "}\n",
    "\n",
    "td_param_grid = {\n",
    "    'n_iter': [1e4, 1e5, 1e6],\n",
    "    'gamma': [0], \n",
    "    'alpha': [0.01, 0.001],\n",
    "    'epsilon': [1e-7, 1e-8, 1e-9],\n",
    "    'betas': [\n",
    "        (0.9, 0.999), \n",
    "        (0.85, 0.999),  \n",
    "        (0.95, 0.999),  \n",
    "        (0.9, 0.99),  \n",
    "        (0.85, 0.99),  \n",
    "        (0.95, 0.99), \n",
    "        (0.8, 0.999),  \n",
    "        (0.9, 0.9999),  \n",
    "        (0.95, 0.9999),\n",
    "        (0.85, 0.9999)\n",
    "    ]\n",
    "}\n",
    "\n",
    "grids = {\n",
    "    'td': td_param_grid,\n",
    "    'nn': nn_param_grid,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Training NN with optimizer=sgd, hidden_size=32, lr=0.0001, batch_size=16, n_epochs=100\n",
      "Epoch [1/100], Loss: 578.8221\n",
      "Epoch [2/100], Loss: 565.0906\n",
      "Epoch [3/100], Loss: 551.2965\n",
      "Epoch [4/100], Loss: 535.5896\n",
      "Epoch [5/100], Loss: 528.2543\n",
      "Epoch [6/100], Loss: 503.4082\n",
      "Epoch [7/100], Loss: 462.2839\n",
      "Epoch [8/100], Loss: 414.1014\n",
      "Epoch [9/100], Loss: 377.1880\n",
      "Epoch [10/100], Loss: 322.5501\n",
      "Epoch [11/100], Loss: 253.7141\n",
      "Epoch [12/100], Loss: 208.0972\n",
      "Epoch [13/100], Loss: 166.9559\n",
      "Epoch [14/100], Loss: 150.6411\n",
      "Epoch [15/100], Loss: 129.0274\n",
      "Epoch [16/100], Loss: 111.1772\n",
      "Epoch [17/100], Loss: 104.2727\n",
      "Epoch [18/100], Loss: 100.5431\n",
      "Epoch [19/100], Loss: 98.1132\n",
      "Epoch [20/100], Loss: 97.1334\n",
      "Epoch [21/100], Loss: 94.6515\n",
      "Epoch [22/100], Loss: 91.9232\n",
      "Epoch [23/100], Loss: 90.7042\n",
      "Epoch [24/100], Loss: 94.1270\n",
      "Epoch [25/100], Loss: 89.8064\n",
      "Epoch [26/100], Loss: 87.0830\n",
      "Epoch [27/100], Loss: 85.8247\n",
      "Epoch [28/100], Loss: 84.1371\n",
      "Epoch [29/100], Loss: 83.1777\n",
      "Epoch [30/100], Loss: 81.7762\n",
      "Epoch [31/100], Loss: 79.4381\n",
      "Epoch [32/100], Loss: 78.8022\n",
      "Epoch [33/100], Loss: 79.8264\n",
      "Epoch [34/100], Loss: 77.0285\n",
      "Epoch [35/100], Loss: 75.7420\n",
      "Epoch [36/100], Loss: 74.5426\n",
      "Epoch [37/100], Loss: 72.9148\n",
      "Epoch [38/100], Loss: 72.1229\n",
      "Epoch [39/100], Loss: 72.0982\n",
      "Epoch [40/100], Loss: 70.7066\n",
      "Epoch [41/100], Loss: 69.7267\n",
      "Epoch [42/100], Loss: 68.4721\n",
      "Epoch [43/100], Loss: 69.0422\n",
      "Epoch [44/100], Loss: 66.9787\n",
      "Epoch [45/100], Loss: 68.2958\n",
      "Epoch [46/100], Loss: 70.2764\n",
      "Epoch [47/100], Loss: 65.4512\n",
      "Epoch [48/100], Loss: 66.2439\n",
      "Epoch [49/100], Loss: 64.0863\n",
      "Epoch [50/100], Loss: 62.1202\n",
      "Epoch [51/100], Loss: 62.0665\n",
      "Epoch [52/100], Loss: 61.9253\n",
      "Epoch [53/100], Loss: 61.0194\n",
      "Epoch [54/100], Loss: 62.0716\n",
      "Epoch [55/100], Loss: 59.4721\n",
      "Epoch [56/100], Loss: 61.1851\n",
      "Epoch [57/100], Loss: 64.2786\n",
      "Epoch [58/100], Loss: 58.2592\n",
      "Epoch [59/100], Loss: 62.9810\n",
      "Epoch [60/100], Loss: 57.2305\n",
      "Epoch [61/100], Loss: 57.4258\n",
      "Epoch [62/100], Loss: 57.8462\n",
      "Epoch [63/100], Loss: 55.8691\n",
      "Epoch [64/100], Loss: 55.6240\n",
      "Epoch [65/100], Loss: 55.4537\n",
      "Epoch [66/100], Loss: 55.0704\n",
      "Epoch [67/100], Loss: 54.7615\n",
      "Epoch [68/100], Loss: 54.4825\n",
      "Epoch [69/100], Loss: 54.1965\n",
      "Epoch [70/100], Loss: 54.0666\n",
      "Epoch [71/100], Loss: 53.7029\n",
      "Epoch [72/100], Loss: 52.9824\n",
      "Epoch [73/100], Loss: 53.1598\n",
      "Epoch [74/100], Loss: 52.0715\n",
      "Epoch [75/100], Loss: 55.7081\n",
      "Epoch [76/100], Loss: 52.3070\n",
      "Epoch [77/100], Loss: 51.8323\n",
      "Epoch [78/100], Loss: 57.3819\n",
      "Epoch [79/100], Loss: 50.8875\n",
      "Epoch [80/100], Loss: 56.0481\n",
      "Epoch [81/100], Loss: 50.9865\n",
      "Epoch [82/100], Loss: 51.3607\n",
      "Epoch [83/100], Loss: 50.1845\n",
      "Epoch [84/100], Loss: 50.1575\n",
      "Epoch [85/100], Loss: 50.2278\n",
      "Epoch [86/100], Loss: 50.0785\n",
      "Epoch [87/100], Loss: 54.3804\n",
      "Epoch [88/100], Loss: 49.1305\n",
      "Epoch [89/100], Loss: 49.4843\n",
      "Epoch [90/100], Loss: 48.6452\n",
      "Epoch [91/100], Loss: 48.4409\n",
      "Epoch [92/100], Loss: 48.0890\n",
      "Epoch [93/100], Loss: 55.0170\n",
      "Epoch [94/100], Loss: 51.3519\n",
      "Epoch [95/100], Loss: 54.5872\n",
      "Epoch [96/100], Loss: 57.3265\n",
      "Epoch [97/100], Loss: 48.0638\n",
      "Epoch [98/100], Loss: 49.9053\n",
      "Epoch [99/100], Loss: 47.1146\n",
      "Epoch [100/100], Loss: 47.1306\n",
      "Validation RMSE: 33.75434562138149\n",
      "Iteration 2: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=16, n_epochs=100\n",
      "Epoch [1/100], Loss: 478.2574\n",
      "Epoch [2/100], Loss: 141.1443\n",
      "Epoch [3/100], Loss: 89.5852\n",
      "Epoch [4/100], Loss: 78.0116\n",
      "Epoch [5/100], Loss: 64.8770\n",
      "Epoch [6/100], Loss: 57.7489\n",
      "Epoch [7/100], Loss: 53.9134\n",
      "Epoch [8/100], Loss: 56.9200\n",
      "Epoch [9/100], Loss: 47.3844\n",
      "Epoch [10/100], Loss: 50.3668\n",
      "Epoch [11/100], Loss: 44.0750\n",
      "Epoch [12/100], Loss: 44.2094\n",
      "Epoch [13/100], Loss: 41.7134\n",
      "Epoch [14/100], Loss: 39.8391\n",
      "Epoch [15/100], Loss: 40.3039\n",
      "Epoch [16/100], Loss: 38.1199\n",
      "Epoch [17/100], Loss: 36.7248\n",
      "Epoch [18/100], Loss: 35.6777\n",
      "Epoch [19/100], Loss: 35.5107\n",
      "Epoch [20/100], Loss: 33.4401\n",
      "Epoch [21/100], Loss: 33.0612\n",
      "Epoch [22/100], Loss: 31.9686\n",
      "Epoch [23/100], Loss: 37.2467\n",
      "Epoch [24/100], Loss: 30.8863\n",
      "Epoch [25/100], Loss: 29.5162\n",
      "Epoch [26/100], Loss: 37.2945\n",
      "Epoch [27/100], Loss: 30.2755\n",
      "Epoch [28/100], Loss: 35.3924\n",
      "Epoch [29/100], Loss: 29.0402\n",
      "Epoch [30/100], Loss: 27.8629\n",
      "Epoch [31/100], Loss: 27.1774\n",
      "Epoch [32/100], Loss: 26.9872\n",
      "Epoch [33/100], Loss: 26.8671\n",
      "Epoch [34/100], Loss: 26.6766\n",
      "Epoch [35/100], Loss: 26.2717\n",
      "Epoch [36/100], Loss: 27.0986\n",
      "Epoch [37/100], Loss: 26.3419\n",
      "Epoch [38/100], Loss: 27.5636\n",
      "Epoch [39/100], Loss: 25.3625\n",
      "Epoch [40/100], Loss: 24.5451\n",
      "Epoch [41/100], Loss: 24.4934\n",
      "Epoch [42/100], Loss: 25.1092\n",
      "Epoch [43/100], Loss: 24.3536\n",
      "Epoch [44/100], Loss: 24.3612\n",
      "Epoch [45/100], Loss: 24.2001\n",
      "Epoch [46/100], Loss: 29.2485\n",
      "Epoch [47/100], Loss: 24.2141\n",
      "Epoch [48/100], Loss: 23.7661\n",
      "Epoch [49/100], Loss: 23.7999\n",
      "Epoch [50/100], Loss: 24.6168\n",
      "Epoch [51/100], Loss: 23.1186\n",
      "Epoch [52/100], Loss: 23.8410\n",
      "Epoch [53/100], Loss: 23.0847\n",
      "Epoch [54/100], Loss: 23.2713\n",
      "Epoch [55/100], Loss: 22.8996\n",
      "Epoch [56/100], Loss: 25.5826\n",
      "Epoch [57/100], Loss: 24.3584\n",
      "Epoch [58/100], Loss: 22.6834\n",
      "Epoch [59/100], Loss: 22.6534\n",
      "Epoch [60/100], Loss: 22.8768\n",
      "Epoch [61/100], Loss: 22.8516\n",
      "Epoch [62/100], Loss: 22.8469\n",
      "Epoch [63/100], Loss: 22.5942\n",
      "Epoch [64/100], Loss: 23.5781\n",
      "Epoch [65/100], Loss: 22.1727\n",
      "Epoch [66/100], Loss: 24.0482\n",
      "Epoch [67/100], Loss: 22.2564\n",
      "Epoch [68/100], Loss: 22.2607\n",
      "Epoch [69/100], Loss: 21.9523\n",
      "Epoch [70/100], Loss: 21.7516\n",
      "Epoch [71/100], Loss: 21.6485\n",
      "Epoch [72/100], Loss: 22.1196\n",
      "Epoch [73/100], Loss: 22.2118\n",
      "Epoch [74/100], Loss: 21.8093\n",
      "Epoch [75/100], Loss: 22.7290\n",
      "Epoch [76/100], Loss: 21.5199\n",
      "Epoch [77/100], Loss: 21.6030\n",
      "Epoch [78/100], Loss: 21.1035\n",
      "Epoch [79/100], Loss: 21.2198\n",
      "Epoch [80/100], Loss: 21.4284\n",
      "Epoch [81/100], Loss: 21.7309\n",
      "Epoch [82/100], Loss: 21.2949\n",
      "Epoch [83/100], Loss: 21.0649\n",
      "Epoch [84/100], Loss: 20.9988\n",
      "Epoch [85/100], Loss: 21.0664\n",
      "Epoch [86/100], Loss: 21.3208\n",
      "Epoch [87/100], Loss: 20.9823\n",
      "Epoch [88/100], Loss: 20.9830\n",
      "Epoch [89/100], Loss: 22.1973\n",
      "Epoch [90/100], Loss: 20.8302\n",
      "Epoch [91/100], Loss: 21.1420\n",
      "Epoch [92/100], Loss: 20.6779\n",
      "Epoch [93/100], Loss: 20.8629\n",
      "Epoch [94/100], Loss: 20.5148\n",
      "Epoch [95/100], Loss: 20.4252\n",
      "Epoch [96/100], Loss: 20.2364\n",
      "Epoch [97/100], Loss: 20.2758\n",
      "Epoch [98/100], Loss: 20.1488\n",
      "Epoch [99/100], Loss: 20.3264\n",
      "Epoch [100/100], Loss: 20.3322\n",
      "Validation RMSE: 11.666303089686803\n",
      "Iteration 3: Training NN with optimizer=sgd, hidden_size=128, lr=0.0001, batch_size=64, n_epochs=150\n",
      "Epoch [1/150], Loss: 591.0465\n",
      "Epoch [2/150], Loss: 596.3883\n",
      "Epoch [3/150], Loss: 554.4155\n",
      "Epoch [4/150], Loss: 550.2431\n",
      "Epoch [5/150], Loss: 606.5065\n",
      "Epoch [6/150], Loss: 551.9070\n",
      "Epoch [7/150], Loss: 568.7096\n",
      "Epoch [8/150], Loss: 525.0825\n",
      "Epoch [9/150], Loss: 520.3327\n",
      "Epoch [10/150], Loss: 513.3390\n",
      "Epoch [11/150], Loss: 482.3656\n",
      "Epoch [12/150], Loss: 473.5291\n",
      "Epoch [13/150], Loss: 465.0829\n",
      "Epoch [14/150], Loss: 461.9241\n",
      "Epoch [15/150], Loss: 450.2701\n",
      "Epoch [16/150], Loss: 438.8188\n",
      "Epoch [17/150], Loss: 419.0283\n",
      "Epoch [18/150], Loss: 410.9682\n",
      "Epoch [19/150], Loss: 386.2433\n",
      "Epoch [20/150], Loss: 376.2420\n",
      "Epoch [21/150], Loss: 368.3555\n",
      "Epoch [22/150], Loss: 334.9749\n",
      "Epoch [23/150], Loss: 333.4402\n",
      "Epoch [24/150], Loss: 308.0128\n",
      "Epoch [25/150], Loss: 301.0769\n",
      "Epoch [26/150], Loss: 291.8591\n",
      "Epoch [27/150], Loss: 260.1022\n",
      "Epoch [28/150], Loss: 248.9639\n",
      "Epoch [29/150], Loss: 231.0395\n",
      "Epoch [30/150], Loss: 233.0828\n",
      "Epoch [31/150], Loss: 200.1241\n",
      "Epoch [32/150], Loss: 197.5128\n",
      "Epoch [33/150], Loss: 197.1227\n",
      "Epoch [34/150], Loss: 167.1319\n",
      "Epoch [35/150], Loss: 163.8089\n",
      "Epoch [36/150], Loss: 156.7010\n",
      "Epoch [37/150], Loss: 167.4153\n",
      "Epoch [38/150], Loss: 143.5367\n",
      "Epoch [39/150], Loss: 137.2550\n",
      "Epoch [40/150], Loss: 134.9064\n",
      "Epoch [41/150], Loss: 123.5365\n",
      "Epoch [42/150], Loss: 124.2820\n",
      "Epoch [43/150], Loss: 121.2308\n",
      "Epoch [44/150], Loss: 112.2520\n",
      "Epoch [45/150], Loss: 114.1532\n",
      "Epoch [46/150], Loss: 110.0203\n",
      "Epoch [47/150], Loss: 108.1761\n",
      "Epoch [48/150], Loss: 111.7701\n",
      "Epoch [49/150], Loss: 114.3314\n",
      "Epoch [50/150], Loss: 103.8623\n",
      "Epoch [51/150], Loss: 103.2546\n",
      "Epoch [52/150], Loss: 104.0643\n",
      "Epoch [53/150], Loss: 103.3390\n",
      "Epoch [54/150], Loss: 98.2696\n",
      "Epoch [55/150], Loss: 93.9251\n",
      "Epoch [56/150], Loss: 99.4574\n",
      "Epoch [57/150], Loss: 98.5921\n",
      "Epoch [58/150], Loss: 96.9790\n",
      "Epoch [59/150], Loss: 95.4656\n",
      "Epoch [60/150], Loss: 94.3689\n",
      "Epoch [61/150], Loss: 93.5414\n",
      "Epoch [62/150], Loss: 92.6127\n",
      "Epoch [63/150], Loss: 90.5247\n",
      "Epoch [64/150], Loss: 91.9770\n",
      "Epoch [65/150], Loss: 94.7406\n",
      "Epoch [66/150], Loss: 98.0918\n",
      "Epoch [67/150], Loss: 86.3458\n",
      "Epoch [68/150], Loss: 84.6421\n",
      "Epoch [69/150], Loss: 91.0212\n",
      "Epoch [70/150], Loss: 86.9814\n",
      "Epoch [71/150], Loss: 85.5665\n",
      "Epoch [72/150], Loss: 87.3061\n",
      "Epoch [73/150], Loss: 87.6883\n",
      "Epoch [74/150], Loss: 85.5258\n",
      "Epoch [75/150], Loss: 89.2908\n",
      "Epoch [76/150], Loss: 85.9531\n",
      "Epoch [77/150], Loss: 90.2674\n",
      "Epoch [78/150], Loss: 89.4129\n",
      "Epoch [79/150], Loss: 86.8121\n",
      "Epoch [80/150], Loss: 90.2829\n",
      "Epoch [81/150], Loss: 83.8210\n",
      "Epoch [82/150], Loss: 84.1450\n",
      "Epoch [83/150], Loss: 94.6368\n",
      "Epoch [84/150], Loss: 81.9893\n",
      "Epoch [85/150], Loss: 82.9865\n",
      "Epoch [86/150], Loss: 83.1249\n",
      "Epoch [87/150], Loss: 82.6709\n",
      "Epoch [88/150], Loss: 87.4298\n",
      "Epoch [89/150], Loss: 82.0418\n",
      "Epoch [90/150], Loss: 82.9493\n",
      "Epoch [91/150], Loss: 83.2286\n",
      "Epoch [92/150], Loss: 83.2540\n",
      "Epoch [93/150], Loss: 82.2159\n",
      "Epoch [94/150], Loss: 82.0157\n",
      "Epoch [95/150], Loss: 81.5885\n",
      "Epoch [96/150], Loss: 79.2320\n",
      "Epoch [97/150], Loss: 78.9008\n",
      "Epoch [98/150], Loss: 76.2672\n",
      "Epoch [99/150], Loss: 79.1785\n",
      "Epoch [100/150], Loss: 75.8996\n",
      "Epoch [101/150], Loss: 73.0436\n",
      "Epoch [102/150], Loss: 77.1134\n",
      "Epoch [103/150], Loss: 72.3627\n",
      "Epoch [104/150], Loss: 76.4966\n",
      "Epoch [105/150], Loss: 83.1122\n",
      "Epoch [106/150], Loss: 74.9222\n",
      "Epoch [107/150], Loss: 75.6932\n",
      "Epoch [108/150], Loss: 73.6135\n",
      "Epoch [109/150], Loss: 78.1524\n",
      "Epoch [110/150], Loss: 74.5960\n",
      "Epoch [111/150], Loss: 75.2978\n",
      "Epoch [112/150], Loss: 81.6547\n",
      "Epoch [113/150], Loss: 76.5161\n",
      "Epoch [114/150], Loss: 78.5256\n",
      "Epoch [115/150], Loss: 80.1749\n",
      "Epoch [116/150], Loss: 75.4618\n",
      "Epoch [117/150], Loss: 71.3277\n",
      "Epoch [118/150], Loss: 69.1034\n",
      "Epoch [119/150], Loss: 71.5885\n",
      "Epoch [120/150], Loss: 72.9961\n",
      "Epoch [121/150], Loss: 74.5767\n",
      "Epoch [122/150], Loss: 75.9214\n",
      "Epoch [123/150], Loss: 70.9743\n",
      "Epoch [124/150], Loss: 73.0843\n",
      "Epoch [125/150], Loss: 72.2317\n",
      "Epoch [126/150], Loss: 74.0934\n",
      "Epoch [127/150], Loss: 67.7127\n",
      "Epoch [128/150], Loss: 66.4905\n",
      "Epoch [129/150], Loss: 66.1446\n",
      "Epoch [130/150], Loss: 73.4635\n",
      "Epoch [131/150], Loss: 66.1355\n",
      "Epoch [132/150], Loss: 73.8526\n",
      "Epoch [133/150], Loss: 66.6876\n",
      "Epoch [134/150], Loss: 70.4079\n",
      "Epoch [135/150], Loss: 67.7083\n",
      "Epoch [136/150], Loss: 66.5413\n",
      "Epoch [137/150], Loss: 70.5118\n",
      "Epoch [138/150], Loss: 73.1521\n",
      "Epoch [139/150], Loss: 64.6965\n",
      "Epoch [140/150], Loss: 67.8035\n",
      "Epoch [141/150], Loss: 65.9813\n",
      "Epoch [142/150], Loss: 67.4664\n",
      "Epoch [143/150], Loss: 62.2169\n",
      "Epoch [144/150], Loss: 61.6067\n",
      "Epoch [145/150], Loss: 62.5406\n",
      "Epoch [146/150], Loss: 67.0439\n",
      "Epoch [147/150], Loss: 66.4624\n",
      "Epoch [148/150], Loss: 69.8934\n",
      "Epoch [149/150], Loss: 66.4438\n",
      "Epoch [150/150], Loss: 64.7327\n",
      "Validation RMSE: 51.79014205932617\n",
      "Iteration 4: Training NN with optimizer=sgd, hidden_size=64, lr=0.0001, batch_size=64, n_epochs=100\n",
      "Epoch [1/100], Loss: 584.0809\n",
      "Epoch [2/100], Loss: 565.6737\n",
      "Epoch [3/100], Loss: 562.1826\n",
      "Epoch [4/100], Loss: 542.2123\n",
      "Epoch [5/100], Loss: 557.6325\n",
      "Epoch [6/100], Loss: 539.4547\n",
      "Epoch [7/100], Loss: 545.7527\n",
      "Epoch [8/100], Loss: 535.8748\n",
      "Epoch [9/100], Loss: 502.5271\n",
      "Epoch [10/100], Loss: 484.3322\n",
      "Epoch [11/100], Loss: 479.2005\n",
      "Epoch [12/100], Loss: 492.7024\n",
      "Epoch [13/100], Loss: 461.8540\n",
      "Epoch [14/100], Loss: 433.0598\n",
      "Epoch [15/100], Loss: 443.8406\n",
      "Epoch [16/100], Loss: 418.2910\n",
      "Epoch [17/100], Loss: 406.3916\n",
      "Epoch [18/100], Loss: 379.9329\n",
      "Epoch [19/100], Loss: 363.7961\n",
      "Epoch [20/100], Loss: 369.9386\n",
      "Epoch [21/100], Loss: 358.2264\n",
      "Epoch [22/100], Loss: 325.9250\n",
      "Epoch [23/100], Loss: 313.2643\n",
      "Epoch [24/100], Loss: 308.5619\n",
      "Epoch [25/100], Loss: 266.5653\n",
      "Epoch [26/100], Loss: 256.7916\n",
      "Epoch [27/100], Loss: 243.1068\n",
      "Epoch [28/100], Loss: 240.2108\n",
      "Epoch [29/100], Loss: 213.1201\n",
      "Epoch [30/100], Loss: 206.3104\n",
      "Epoch [31/100], Loss: 199.4060\n",
      "Epoch [32/100], Loss: 183.6249\n",
      "Epoch [33/100], Loss: 166.4804\n",
      "Epoch [34/100], Loss: 180.2483\n",
      "Epoch [35/100], Loss: 156.6292\n",
      "Epoch [36/100], Loss: 149.2975\n",
      "Epoch [37/100], Loss: 140.4551\n",
      "Epoch [38/100], Loss: 136.9039\n",
      "Epoch [39/100], Loss: 143.6604\n",
      "Epoch [40/100], Loss: 133.2245\n",
      "Epoch [41/100], Loss: 141.1171\n",
      "Epoch [42/100], Loss: 129.4302\n",
      "Epoch [43/100], Loss: 123.8217\n",
      "Epoch [44/100], Loss: 117.2555\n",
      "Epoch [45/100], Loss: 129.8167\n",
      "Epoch [46/100], Loss: 113.6955\n",
      "Epoch [47/100], Loss: 108.4449\n",
      "Epoch [48/100], Loss: 114.9136\n",
      "Epoch [49/100], Loss: 105.8716\n",
      "Epoch [50/100], Loss: 103.0283\n",
      "Epoch [51/100], Loss: 102.1882\n",
      "Epoch [52/100], Loss: 111.4977\n",
      "Epoch [53/100], Loss: 106.3626\n",
      "Epoch [54/100], Loss: 106.7853\n",
      "Epoch [55/100], Loss: 104.9991\n",
      "Epoch [56/100], Loss: 105.0680\n",
      "Epoch [57/100], Loss: 104.8307\n",
      "Epoch [58/100], Loss: 104.6376\n",
      "Epoch [59/100], Loss: 99.4699\n",
      "Epoch [60/100], Loss: 99.7210\n",
      "Epoch [61/100], Loss: 102.3718\n",
      "Epoch [62/100], Loss: 96.0874\n",
      "Epoch [63/100], Loss: 97.5456\n",
      "Epoch [64/100], Loss: 93.1207\n",
      "Epoch [65/100], Loss: 95.2831\n",
      "Epoch [66/100], Loss: 96.2925\n",
      "Epoch [67/100], Loss: 101.0487\n",
      "Epoch [68/100], Loss: 94.3127\n",
      "Epoch [69/100], Loss: 91.2679\n",
      "Epoch [70/100], Loss: 92.5157\n",
      "Epoch [71/100], Loss: 95.2579\n",
      "Epoch [72/100], Loss: 93.5060\n",
      "Epoch [73/100], Loss: 93.7439\n",
      "Epoch [74/100], Loss: 96.6521\n",
      "Epoch [75/100], Loss: 92.8392\n",
      "Epoch [76/100], Loss: 90.0543\n",
      "Epoch [77/100], Loss: 94.5633\n",
      "Epoch [78/100], Loss: 88.8793\n",
      "Epoch [79/100], Loss: 89.8457\n",
      "Epoch [80/100], Loss: 90.8404\n",
      "Epoch [81/100], Loss: 87.9589\n",
      "Epoch [82/100], Loss: 91.8380\n",
      "Epoch [83/100], Loss: 89.3343\n",
      "Epoch [84/100], Loss: 92.1064\n",
      "Epoch [85/100], Loss: 91.1099\n",
      "Epoch [86/100], Loss: 96.5876\n",
      "Epoch [87/100], Loss: 84.4373\n",
      "Epoch [88/100], Loss: 90.0963\n",
      "Epoch [89/100], Loss: 85.9100\n",
      "Epoch [90/100], Loss: 85.3839\n",
      "Epoch [91/100], Loss: 86.9890\n",
      "Epoch [92/100], Loss: 82.5971\n",
      "Epoch [93/100], Loss: 82.8627\n",
      "Epoch [94/100], Loss: 87.2053\n",
      "Epoch [95/100], Loss: 91.4696\n",
      "Epoch [96/100], Loss: 82.7653\n",
      "Epoch [97/100], Loss: 85.1829\n",
      "Epoch [98/100], Loss: 84.3564\n",
      "Epoch [99/100], Loss: 80.4995\n",
      "Epoch [100/100], Loss: 80.0192\n",
      "Validation RMSE: 70.0640697479248\n",
      "Iteration 5: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=32, n_epochs=150\n",
      "Epoch [1/150], Loss: 539.6590\n",
      "Epoch [2/150], Loss: 343.1680\n",
      "Epoch [3/150], Loss: 144.1875\n",
      "Epoch [4/150], Loss: 102.1104\n",
      "Epoch [5/150], Loss: 93.3748\n",
      "Epoch [6/150], Loss: 86.0775\n",
      "Epoch [7/150], Loss: 80.0294\n",
      "Epoch [8/150], Loss: 75.2255\n",
      "Epoch [9/150], Loss: 69.6776\n",
      "Epoch [10/150], Loss: 67.0005\n",
      "Epoch [11/150], Loss: 63.1334\n",
      "Epoch [12/150], Loss: 59.9546\n",
      "Epoch [13/150], Loss: 59.1159\n",
      "Epoch [14/150], Loss: 56.9518\n",
      "Epoch [15/150], Loss: 54.0686\n",
      "Epoch [16/150], Loss: 53.5103\n",
      "Epoch [17/150], Loss: 53.8251\n",
      "Epoch [18/150], Loss: 52.5564\n",
      "Epoch [19/150], Loss: 51.2586\n",
      "Epoch [20/150], Loss: 49.2527\n",
      "Epoch [21/150], Loss: 47.1330\n",
      "Epoch [22/150], Loss: 46.7318\n",
      "Epoch [23/150], Loss: 47.1354\n",
      "Epoch [24/150], Loss: 44.7765\n",
      "Epoch [25/150], Loss: 44.5840\n",
      "Epoch [26/150], Loss: 44.0976\n",
      "Epoch [27/150], Loss: 43.1236\n",
      "Epoch [28/150], Loss: 43.3622\n",
      "Epoch [29/150], Loss: 42.0399\n",
      "Epoch [30/150], Loss: 42.0325\n",
      "Epoch [31/150], Loss: 41.5994\n",
      "Epoch [32/150], Loss: 39.6789\n",
      "Epoch [33/150], Loss: 39.3788\n",
      "Epoch [34/150], Loss: 38.3463\n",
      "Epoch [35/150], Loss: 37.7039\n",
      "Epoch [36/150], Loss: 37.3593\n",
      "Epoch [37/150], Loss: 38.8418\n",
      "Epoch [38/150], Loss: 35.9365\n",
      "Epoch [39/150], Loss: 37.1444\n",
      "Epoch [40/150], Loss: 34.9796\n",
      "Epoch [41/150], Loss: 34.6854\n",
      "Epoch [42/150], Loss: 34.4594\n",
      "Epoch [43/150], Loss: 33.8114\n",
      "Epoch [44/150], Loss: 33.9577\n",
      "Epoch [45/150], Loss: 32.7971\n",
      "Epoch [46/150], Loss: 32.5615\n",
      "Epoch [47/150], Loss: 32.2075\n",
      "Epoch [48/150], Loss: 31.7264\n",
      "Epoch [49/150], Loss: 31.4797\n",
      "Epoch [50/150], Loss: 31.5403\n",
      "Epoch [51/150], Loss: 31.2720\n",
      "Epoch [52/150], Loss: 31.0691\n",
      "Epoch [53/150], Loss: 29.7795\n",
      "Epoch [54/150], Loss: 31.1119\n",
      "Epoch [55/150], Loss: 29.4943\n",
      "Epoch [56/150], Loss: 29.2172\n",
      "Epoch [57/150], Loss: 29.1135\n",
      "Epoch [58/150], Loss: 28.6400\n",
      "Epoch [59/150], Loss: 28.2601\n",
      "Epoch [60/150], Loss: 29.0258\n",
      "Epoch [61/150], Loss: 28.1101\n",
      "Epoch [62/150], Loss: 28.4494\n",
      "Epoch [63/150], Loss: 27.6090\n",
      "Epoch [64/150], Loss: 28.6768\n",
      "Epoch [65/150], Loss: 27.6458\n",
      "Epoch [66/150], Loss: 29.4157\n",
      "Epoch [67/150], Loss: 26.6887\n",
      "Epoch [68/150], Loss: 27.7711\n",
      "Epoch [69/150], Loss: 27.6765\n",
      "Epoch [70/150], Loss: 26.3051\n",
      "Epoch [71/150], Loss: 27.9517\n",
      "Epoch [72/150], Loss: 26.0570\n",
      "Epoch [73/150], Loss: 25.8100\n",
      "Epoch [74/150], Loss: 25.9609\n",
      "Epoch [75/150], Loss: 25.3407\n",
      "Epoch [76/150], Loss: 25.7548\n",
      "Epoch [77/150], Loss: 25.3494\n",
      "Epoch [78/150], Loss: 25.3434\n",
      "Epoch [79/150], Loss: 26.1813\n",
      "Epoch [80/150], Loss: 25.2712\n",
      "Epoch [81/150], Loss: 25.5758\n",
      "Epoch [82/150], Loss: 25.1374\n",
      "Epoch [83/150], Loss: 26.4379\n",
      "Epoch [84/150], Loss: 24.9067\n",
      "Epoch [85/150], Loss: 24.6213\n",
      "Epoch [86/150], Loss: 24.4967\n",
      "Epoch [87/150], Loss: 24.4529\n",
      "Epoch [88/150], Loss: 24.0979\n",
      "Epoch [89/150], Loss: 24.0898\n",
      "Epoch [90/150], Loss: 24.3394\n",
      "Epoch [91/150], Loss: 25.1614\n",
      "Epoch [92/150], Loss: 24.3206\n",
      "Epoch [93/150], Loss: 23.8404\n",
      "Epoch [94/150], Loss: 24.7790\n",
      "Epoch [95/150], Loss: 23.9103\n",
      "Epoch [96/150], Loss: 23.9322\n",
      "Epoch [97/150], Loss: 23.6885\n",
      "Epoch [98/150], Loss: 23.2377\n",
      "Epoch [99/150], Loss: 23.9895\n",
      "Epoch [100/150], Loss: 23.0105\n",
      "Epoch [101/150], Loss: 23.4323\n",
      "Epoch [102/150], Loss: 23.3492\n",
      "Epoch [103/150], Loss: 22.8397\n",
      "Epoch [104/150], Loss: 22.9673\n",
      "Epoch [105/150], Loss: 22.7202\n",
      "Epoch [106/150], Loss: 23.4901\n",
      "Epoch [107/150], Loss: 23.2443\n",
      "Epoch [108/150], Loss: 24.0266\n",
      "Epoch [109/150], Loss: 22.6992\n",
      "Epoch [110/150], Loss: 23.0457\n",
      "Epoch [111/150], Loss: 22.9101\n",
      "Epoch [112/150], Loss: 22.6908\n",
      "Epoch [113/150], Loss: 22.3481\n",
      "Epoch [114/150], Loss: 22.6175\n",
      "Epoch [115/150], Loss: 22.5933\n",
      "Epoch [116/150], Loss: 22.8179\n",
      "Epoch [117/150], Loss: 22.4482\n",
      "Epoch [118/150], Loss: 22.7247\n",
      "Epoch [119/150], Loss: 23.2650\n",
      "Epoch [120/150], Loss: 23.1766\n",
      "Epoch [121/150], Loss: 22.2255\n",
      "Epoch [122/150], Loss: 22.0578\n",
      "Epoch [123/150], Loss: 21.9070\n",
      "Epoch [124/150], Loss: 21.9268\n",
      "Epoch [125/150], Loss: 21.7105\n",
      "Epoch [126/150], Loss: 22.1815\n",
      "Epoch [127/150], Loss: 21.8048\n",
      "Epoch [128/150], Loss: 21.7347\n",
      "Epoch [129/150], Loss: 22.2166\n",
      "Epoch [130/150], Loss: 21.6070\n",
      "Epoch [131/150], Loss: 22.1598\n",
      "Epoch [132/150], Loss: 21.9583\n",
      "Epoch [133/150], Loss: 22.0714\n",
      "Epoch [134/150], Loss: 21.2509\n",
      "Epoch [135/150], Loss: 21.1444\n",
      "Epoch [136/150], Loss: 21.5290\n",
      "Epoch [137/150], Loss: 21.4651\n",
      "Epoch [138/150], Loss: 21.1806\n",
      "Epoch [139/150], Loss: 22.4497\n",
      "Epoch [140/150], Loss: 21.1965\n",
      "Epoch [141/150], Loss: 21.1698\n",
      "Epoch [142/150], Loss: 21.2112\n",
      "Epoch [143/150], Loss: 21.8926\n",
      "Epoch [144/150], Loss: 20.8738\n",
      "Epoch [145/150], Loss: 21.1518\n",
      "Epoch [146/150], Loss: 21.1175\n",
      "Epoch [147/150], Loss: 20.8684\n",
      "Epoch [148/150], Loss: 21.1807\n",
      "Epoch [149/150], Loss: 20.8893\n",
      "Epoch [150/150], Loss: 20.7519\n",
      "Validation RMSE: 12.106956720352173\n",
      "Iteration 6: Training NN with optimizer=sgd, hidden_size=32, lr=0.0001, batch_size=32, n_epochs=50\n",
      "Epoch [1/50], Loss: 593.7677\n",
      "Epoch [2/50], Loss: 588.5042\n",
      "Epoch [3/50], Loss: 580.5099\n",
      "Epoch [4/50], Loss: 569.3783\n",
      "Epoch [5/50], Loss: 564.8744\n",
      "Epoch [6/50], Loss: 553.7125\n",
      "Epoch [7/50], Loss: 546.7325\n",
      "Epoch [8/50], Loss: 531.0093\n",
      "Epoch [9/50], Loss: 516.9814\n",
      "Epoch [10/50], Loss: 503.7057\n",
      "Epoch [11/50], Loss: 483.3080\n",
      "Epoch [12/50], Loss: 467.7094\n",
      "Epoch [13/50], Loss: 450.1217\n",
      "Epoch [14/50], Loss: 423.3306\n",
      "Epoch [15/50], Loss: 397.4470\n",
      "Epoch [16/50], Loss: 376.7502\n",
      "Epoch [17/50], Loss: 345.1472\n",
      "Epoch [18/50], Loss: 322.8474\n",
      "Epoch [19/50], Loss: 288.6411\n",
      "Epoch [20/50], Loss: 259.0051\n",
      "Epoch [21/50], Loss: 237.4751\n",
      "Epoch [22/50], Loss: 210.9333\n",
      "Epoch [23/50], Loss: 190.2043\n",
      "Epoch [24/50], Loss: 170.2066\n",
      "Epoch [25/50], Loss: 152.2840\n",
      "Epoch [26/50], Loss: 141.8528\n",
      "Epoch [27/50], Loss: 130.6175\n",
      "Epoch [28/50], Loss: 122.4188\n",
      "Epoch [29/50], Loss: 116.3932\n",
      "Epoch [30/50], Loss: 114.9858\n",
      "Epoch [31/50], Loss: 109.2738\n",
      "Epoch [32/50], Loss: 106.5613\n",
      "Epoch [33/50], Loss: 104.8555\n",
      "Epoch [34/50], Loss: 102.4388\n",
      "Epoch [35/50], Loss: 101.2752\n",
      "Epoch [36/50], Loss: 98.7221\n",
      "Epoch [37/50], Loss: 98.4746\n",
      "Epoch [38/50], Loss: 98.2137\n",
      "Epoch [39/50], Loss: 96.1881\n",
      "Epoch [40/50], Loss: 95.5942\n",
      "Epoch [41/50], Loss: 95.0979\n",
      "Epoch [42/50], Loss: 92.7927\n",
      "Epoch [43/50], Loss: 92.0180\n",
      "Epoch [44/50], Loss: 90.6551\n",
      "Epoch [45/50], Loss: 90.8816\n",
      "Epoch [46/50], Loss: 92.4546\n",
      "Epoch [47/50], Loss: 89.2679\n",
      "Epoch [48/50], Loss: 89.9376\n",
      "Epoch [49/50], Loss: 88.6631\n",
      "Epoch [50/50], Loss: 88.0843\n",
      "Validation RMSE: 68.92743682861328\n",
      "Iteration 7: Training NN with optimizer=sgd, hidden_size=64, lr=0.0001, batch_size=16, n_epochs=50\n",
      "Epoch [1/50], Loss: 581.0024\n",
      "Epoch [2/50], Loss: 557.9072\n",
      "Epoch [3/50], Loss: 535.3519\n",
      "Epoch [4/50], Loss: 511.6541\n",
      "Epoch [5/50], Loss: 454.4880\n",
      "Epoch [6/50], Loss: 415.6043\n",
      "Epoch [7/50], Loss: 348.7691\n",
      "Epoch [8/50], Loss: 294.1925\n",
      "Epoch [9/50], Loss: 233.1564\n",
      "Epoch [10/50], Loss: 192.5536\n",
      "Epoch [11/50], Loss: 159.7793\n",
      "Epoch [12/50], Loss: 136.9030\n",
      "Epoch [13/50], Loss: 122.9128\n",
      "Epoch [14/50], Loss: 115.6928\n",
      "Epoch [15/50], Loss: 108.5057\n",
      "Epoch [16/50], Loss: 104.2923\n",
      "Epoch [17/50], Loss: 102.4917\n",
      "Epoch [18/50], Loss: 104.3921\n",
      "Epoch [19/50], Loss: 103.1901\n",
      "Epoch [20/50], Loss: 96.7284\n",
      "Epoch [21/50], Loss: 94.6542\n",
      "Epoch [22/50], Loss: 99.7098\n",
      "Epoch [23/50], Loss: 96.1055\n",
      "Epoch [24/50], Loss: 91.6360\n",
      "Epoch [25/50], Loss: 87.6180\n",
      "Epoch [26/50], Loss: 85.2640\n",
      "Epoch [27/50], Loss: 83.9218\n",
      "Epoch [28/50], Loss: 87.2309\n",
      "Epoch [29/50], Loss: 81.5045\n",
      "Epoch [30/50], Loss: 79.2420\n",
      "Epoch [31/50], Loss: 83.3290\n",
      "Epoch [32/50], Loss: 77.1723\n",
      "Epoch [33/50], Loss: 76.1976\n",
      "Epoch [34/50], Loss: 75.7596\n",
      "Epoch [35/50], Loss: 74.9165\n",
      "Epoch [36/50], Loss: 72.5352\n",
      "Epoch [37/50], Loss: 71.6001\n",
      "Epoch [38/50], Loss: 70.7433\n",
      "Epoch [39/50], Loss: 74.5192\n",
      "Epoch [40/50], Loss: 67.9639\n",
      "Epoch [41/50], Loss: 69.8299\n",
      "Epoch [42/50], Loss: 67.8477\n",
      "Epoch [43/50], Loss: 67.2933\n",
      "Epoch [44/50], Loss: 64.3465\n",
      "Epoch [45/50], Loss: 64.2841\n",
      "Epoch [46/50], Loss: 70.0675\n",
      "Epoch [47/50], Loss: 67.1115\n",
      "Epoch [48/50], Loss: 62.0627\n",
      "Epoch [49/50], Loss: 61.0079\n",
      "Epoch [50/50], Loss: 61.9596\n",
      "Validation RMSE: 47.99369893755232\n",
      "Iteration 8: Training NN with optimizer=sgd, hidden_size=64, lr=0.0001, batch_size=32, n_epochs=100\n",
      "Epoch [1/100], Loss: 575.7063\n",
      "Epoch [2/100], Loss: 572.7460\n",
      "Epoch [3/100], Loss: 546.7868\n",
      "Epoch [4/100], Loss: 532.0463\n",
      "Epoch [5/100], Loss: 517.7422\n",
      "Epoch [6/100], Loss: 504.9755\n",
      "Epoch [7/100], Loss: 473.6307\n",
      "Epoch [8/100], Loss: 450.0648\n",
      "Epoch [9/100], Loss: 426.4018\n",
      "Epoch [10/100], Loss: 403.1489\n",
      "Epoch [11/100], Loss: 369.9810\n",
      "Epoch [12/100], Loss: 342.7366\n",
      "Epoch [13/100], Loss: 308.0403\n",
      "Epoch [14/100], Loss: 280.2588\n",
      "Epoch [15/100], Loss: 248.4299\n",
      "Epoch [16/100], Loss: 222.0277\n",
      "Epoch [17/100], Loss: 201.7077\n",
      "Epoch [18/100], Loss: 184.0287\n",
      "Epoch [19/100], Loss: 164.5562\n",
      "Epoch [20/100], Loss: 150.1659\n",
      "Epoch [21/100], Loss: 141.5267\n",
      "Epoch [22/100], Loss: 129.9397\n",
      "Epoch [23/100], Loss: 123.3754\n",
      "Epoch [24/100], Loss: 120.4162\n",
      "Epoch [25/100], Loss: 113.0526\n",
      "Epoch [26/100], Loss: 112.5497\n",
      "Epoch [27/100], Loss: 110.0018\n",
      "Epoch [28/100], Loss: 106.7983\n",
      "Epoch [29/100], Loss: 107.4269\n",
      "Epoch [30/100], Loss: 103.7240\n",
      "Epoch [31/100], Loss: 103.3208\n",
      "Epoch [32/100], Loss: 102.4236\n",
      "Epoch [33/100], Loss: 100.6871\n",
      "Epoch [34/100], Loss: 98.5298\n",
      "Epoch [35/100], Loss: 99.7987\n",
      "Epoch [36/100], Loss: 97.9336\n",
      "Epoch [37/100], Loss: 98.7764\n",
      "Epoch [38/100], Loss: 97.2526\n",
      "Epoch [39/100], Loss: 97.0489\n",
      "Epoch [40/100], Loss: 95.3675\n",
      "Epoch [41/100], Loss: 92.7903\n",
      "Epoch [42/100], Loss: 93.1066\n",
      "Epoch [43/100], Loss: 91.9772\n",
      "Epoch [44/100], Loss: 90.9419\n",
      "Epoch [45/100], Loss: 89.0743\n",
      "Epoch [46/100], Loss: 92.2491\n",
      "Epoch [47/100], Loss: 89.2471\n",
      "Epoch [48/100], Loss: 86.9308\n",
      "Epoch [49/100], Loss: 87.5074\n",
      "Epoch [50/100], Loss: 85.7146\n",
      "Epoch [51/100], Loss: 85.3830\n",
      "Epoch [52/100], Loss: 84.0046\n",
      "Epoch [53/100], Loss: 84.5578\n",
      "Epoch [54/100], Loss: 82.7526\n",
      "Epoch [55/100], Loss: 82.2028\n",
      "Epoch [56/100], Loss: 81.3613\n",
      "Epoch [57/100], Loss: 80.8898\n",
      "Epoch [58/100], Loss: 82.2896\n",
      "Epoch [59/100], Loss: 79.7528\n",
      "Epoch [60/100], Loss: 82.0519\n",
      "Epoch [61/100], Loss: 80.7788\n",
      "Epoch [62/100], Loss: 79.8456\n",
      "Epoch [63/100], Loss: 78.6683\n",
      "Epoch [64/100], Loss: 77.1405\n",
      "Epoch [65/100], Loss: 77.7976\n",
      "Epoch [66/100], Loss: 76.6841\n",
      "Epoch [67/100], Loss: 75.0574\n",
      "Epoch [68/100], Loss: 78.7933\n",
      "Epoch [69/100], Loss: 76.7684\n",
      "Epoch [70/100], Loss: 73.7852\n",
      "Epoch [71/100], Loss: 74.7577\n",
      "Epoch [72/100], Loss: 73.8227\n",
      "Epoch [73/100], Loss: 72.9681\n",
      "Epoch [74/100], Loss: 73.7580\n",
      "Epoch [75/100], Loss: 71.9448\n",
      "Epoch [76/100], Loss: 71.6984\n",
      "Epoch [77/100], Loss: 70.8900\n",
      "Epoch [78/100], Loss: 70.4427\n",
      "Epoch [79/100], Loss: 70.3797\n",
      "Epoch [80/100], Loss: 69.0979\n",
      "Epoch [81/100], Loss: 69.3582\n",
      "Epoch [82/100], Loss: 67.8289\n",
      "Epoch [83/100], Loss: 67.0771\n",
      "Epoch [84/100], Loss: 69.4048\n",
      "Epoch [85/100], Loss: 68.0447\n",
      "Epoch [86/100], Loss: 67.4245\n",
      "Epoch [87/100], Loss: 68.0545\n",
      "Epoch [88/100], Loss: 67.0219\n",
      "Epoch [89/100], Loss: 66.2795\n",
      "Epoch [90/100], Loss: 64.8189\n",
      "Epoch [91/100], Loss: 65.9688\n",
      "Epoch [92/100], Loss: 64.9207\n",
      "Epoch [93/100], Loss: 66.9116\n",
      "Epoch [94/100], Loss: 64.1141\n",
      "Epoch [95/100], Loss: 62.9393\n",
      "Epoch [96/100], Loss: 63.8132\n",
      "Epoch [97/100], Loss: 63.1905\n",
      "Epoch [98/100], Loss: 62.8776\n",
      "Epoch [99/100], Loss: 63.5198\n",
      "Epoch [100/100], Loss: 61.8837\n",
      "Validation RMSE: 43.58838963508606\n",
      "Iteration 9: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=16, n_epochs=50\n",
      "Epoch [1/50], Loss: 510.5952\n",
      "Epoch [2/50], Loss: 167.2063\n",
      "Epoch [3/50], Loss: 92.7360\n",
      "Epoch [4/50], Loss: 85.8482\n",
      "Epoch [5/50], Loss: 71.3755\n",
      "Epoch [6/50], Loss: 60.9195\n",
      "Epoch [7/50], Loss: 56.3901\n",
      "Epoch [8/50], Loss: 51.7803\n",
      "Epoch [9/50], Loss: 49.6485\n",
      "Epoch [10/50], Loss: 47.5080\n",
      "Epoch [11/50], Loss: 46.3707\n",
      "Epoch [12/50], Loss: 44.3273\n",
      "Epoch [13/50], Loss: 42.9152\n",
      "Epoch [14/50], Loss: 42.7734\n",
      "Epoch [15/50], Loss: 40.1978\n",
      "Epoch [16/50], Loss: 40.9724\n",
      "Epoch [17/50], Loss: 37.9360\n",
      "Epoch [18/50], Loss: 36.2074\n",
      "Epoch [19/50], Loss: 35.8625\n",
      "Epoch [20/50], Loss: 34.7848\n",
      "Epoch [21/50], Loss: 33.9186\n",
      "Epoch [22/50], Loss: 33.0401\n",
      "Epoch [23/50], Loss: 37.1480\n",
      "Epoch [24/50], Loss: 36.1974\n",
      "Epoch [25/50], Loss: 32.3723\n",
      "Epoch [26/50], Loss: 31.2472\n",
      "Epoch [27/50], Loss: 29.5439\n",
      "Epoch [28/50], Loss: 28.6842\n",
      "Epoch [29/50], Loss: 30.6070\n",
      "Epoch [30/50], Loss: 28.5602\n",
      "Epoch [31/50], Loss: 27.4622\n",
      "Epoch [32/50], Loss: 27.3300\n",
      "Epoch [33/50], Loss: 26.6283\n",
      "Epoch [34/50], Loss: 28.8200\n",
      "Epoch [35/50], Loss: 25.6996\n",
      "Epoch [36/50], Loss: 27.3405\n",
      "Epoch [37/50], Loss: 25.7880\n",
      "Epoch [38/50], Loss: 24.8829\n",
      "Epoch [39/50], Loss: 25.1746\n",
      "Epoch [40/50], Loss: 24.3969\n",
      "Epoch [41/50], Loss: 24.2760\n",
      "Epoch [42/50], Loss: 24.0103\n",
      "Epoch [43/50], Loss: 24.0332\n",
      "Epoch [44/50], Loss: 24.0184\n",
      "Epoch [45/50], Loss: 24.0423\n",
      "Epoch [46/50], Loss: 23.7911\n",
      "Epoch [47/50], Loss: 23.7166\n",
      "Epoch [48/50], Loss: 23.8475\n",
      "Epoch [49/50], Loss: 22.9678\n",
      "Epoch [50/50], Loss: 25.1829\n",
      "Validation RMSE: 17.73495646885463\n",
      "Iteration 10: Training NN with optimizer=sgd, hidden_size=64, lr=0.01, batch_size=64, n_epochs=50\n",
      "Epoch [1/50], Loss: 292.7681\n",
      "Epoch [2/50], Loss: 79.1377\n",
      "Epoch [3/50], Loss: 62.6244\n",
      "Epoch [4/50], Loss: 57.9998\n",
      "Epoch [5/50], Loss: 63.8948\n",
      "Epoch [6/50], Loss: 49.7427\n",
      "Epoch [7/50], Loss: 39.1026\n",
      "Epoch [8/50], Loss: 35.0744\n",
      "Epoch [9/50], Loss: 41.6994\n",
      "Epoch [10/50], Loss: 69.7084\n",
      "Epoch [11/50], Loss: 31.2217\n",
      "Epoch [12/50], Loss: 35.2865\n",
      "Epoch [13/50], Loss: 42.0518\n",
      "Epoch [14/50], Loss: 37.2954\n",
      "Epoch [15/50], Loss: 28.5577\n",
      "Epoch [16/50], Loss: 39.8156\n",
      "Epoch [17/50], Loss: 26.1245\n",
      "Epoch [18/50], Loss: 33.9375\n",
      "Epoch [19/50], Loss: 30.6990\n",
      "Epoch [20/50], Loss: 30.4951\n",
      "Epoch [21/50], Loss: 39.3848\n",
      "Epoch [22/50], Loss: 32.7299\n",
      "Epoch [23/50], Loss: 28.0163\n",
      "Epoch [24/50], Loss: 23.3294\n",
      "Epoch [25/50], Loss: 26.6960\n",
      "Epoch [26/50], Loss: 33.2831\n",
      "Epoch [27/50], Loss: 25.2309\n",
      "Epoch [28/50], Loss: 40.2589\n",
      "Epoch [29/50], Loss: 29.8549\n",
      "Epoch [30/50], Loss: 24.1379\n",
      "Epoch [31/50], Loss: 50.4909\n",
      "Epoch [32/50], Loss: 23.1295\n",
      "Epoch [33/50], Loss: 25.6676\n",
      "Epoch [34/50], Loss: 29.2383\n",
      "Epoch [35/50], Loss: 40.9267\n",
      "Epoch [36/50], Loss: 21.8112\n",
      "Epoch [37/50], Loss: 20.8540\n",
      "Epoch [38/50], Loss: 24.7095\n",
      "Epoch [39/50], Loss: 23.0957\n",
      "Epoch [40/50], Loss: 27.5820\n",
      "Epoch [41/50], Loss: 22.7659\n",
      "Epoch [42/50], Loss: 21.9261\n",
      "Epoch [43/50], Loss: 23.4565\n",
      "Epoch [44/50], Loss: 23.9092\n",
      "Epoch [45/50], Loss: 30.9909\n",
      "Epoch [46/50], Loss: 19.5753\n",
      "Epoch [47/50], Loss: 19.1630\n",
      "Epoch [48/50], Loss: 30.9590\n",
      "Epoch [49/50], Loss: 22.5974\n",
      "Epoch [50/50], Loss: 26.4054\n",
      "Validation RMSE: 16.113028526306152\n",
      "Iteration 11: Training NN with optimizer=sgd, hidden_size=128, lr=0.01, batch_size=32, n_epochs=100\n",
      "Epoch [1/100], Loss: 186.5341\n",
      "Epoch [2/100], Loss: 61.5142\n",
      "Epoch [3/100], Loss: 47.2337\n",
      "Epoch [4/100], Loss: 56.0642\n",
      "Epoch [5/100], Loss: 35.2649\n",
      "Epoch [6/100], Loss: 40.7351\n",
      "Epoch [7/100], Loss: 38.7470\n",
      "Epoch [8/100], Loss: 49.4782\n",
      "Epoch [9/100], Loss: 45.6102\n",
      "Epoch [10/100], Loss: 46.7424\n",
      "Epoch [11/100], Loss: 28.5320\n",
      "Epoch [12/100], Loss: 30.6926\n",
      "Epoch [13/100], Loss: 27.0111\n",
      "Epoch [14/100], Loss: 35.3490\n",
      "Epoch [15/100], Loss: 28.2221\n",
      "Epoch [16/100], Loss: 25.0788\n",
      "Epoch [17/100], Loss: 27.5434\n",
      "Epoch [18/100], Loss: 30.8184\n",
      "Epoch [19/100], Loss: 28.5956\n",
      "Epoch [20/100], Loss: 27.7586\n",
      "Epoch [21/100], Loss: 24.2365\n",
      "Epoch [22/100], Loss: 31.0249\n",
      "Epoch [23/100], Loss: 22.0010\n",
      "Epoch [24/100], Loss: 21.0625\n",
      "Epoch [25/100], Loss: 21.1627\n",
      "Epoch [26/100], Loss: 32.0672\n",
      "Epoch [27/100], Loss: 38.9081\n",
      "Epoch [28/100], Loss: 21.4457\n",
      "Epoch [29/100], Loss: 20.9915\n",
      "Epoch [30/100], Loss: 21.7352\n",
      "Epoch [31/100], Loss: 30.0416\n",
      "Epoch [32/100], Loss: 23.8895\n",
      "Epoch [33/100], Loss: 28.6692\n",
      "Epoch [34/100], Loss: 22.7728\n",
      "Epoch [35/100], Loss: 21.0202\n",
      "Epoch [36/100], Loss: 18.6202\n",
      "Epoch [37/100], Loss: 21.5755\n",
      "Epoch [38/100], Loss: 19.1556\n",
      "Epoch [39/100], Loss: 18.6489\n",
      "Epoch [40/100], Loss: 18.0326\n",
      "Epoch [41/100], Loss: 27.5136\n",
      "Epoch [42/100], Loss: 41.2683\n",
      "Epoch [43/100], Loss: 19.2003\n",
      "Epoch [44/100], Loss: 27.0905\n",
      "Epoch [45/100], Loss: 23.7958\n",
      "Epoch [46/100], Loss: 18.2102\n",
      "Epoch [47/100], Loss: 21.1091\n",
      "Epoch [48/100], Loss: 23.7697\n",
      "Epoch [49/100], Loss: 18.7657\n",
      "Epoch [50/100], Loss: 20.4233\n",
      "Epoch [51/100], Loss: 29.0925\n",
      "Epoch [52/100], Loss: 18.8736\n",
      "Epoch [53/100], Loss: 18.3490\n",
      "Epoch [54/100], Loss: 17.6386\n",
      "Epoch [55/100], Loss: 22.4107\n",
      "Epoch [56/100], Loss: 16.7717\n",
      "Epoch [57/100], Loss: 22.8768\n",
      "Epoch [58/100], Loss: 20.1973\n",
      "Epoch [59/100], Loss: 16.5753\n",
      "Epoch [60/100], Loss: 15.7762\n",
      "Epoch [61/100], Loss: 14.3664\n",
      "Epoch [62/100], Loss: 14.8302\n",
      "Epoch [63/100], Loss: 15.6250\n",
      "Epoch [64/100], Loss: 22.6829\n",
      "Epoch [65/100], Loss: 15.8627\n",
      "Epoch [66/100], Loss: 16.5211\n",
      "Epoch [67/100], Loss: 22.2640\n",
      "Epoch [68/100], Loss: 19.8092\n",
      "Epoch [69/100], Loss: 13.7935\n",
      "Epoch [70/100], Loss: 16.6952\n",
      "Epoch [71/100], Loss: 15.4198\n",
      "Epoch [72/100], Loss: 15.0864\n",
      "Epoch [73/100], Loss: 13.8322\n",
      "Epoch [74/100], Loss: 13.1985\n",
      "Epoch [75/100], Loss: 14.1026\n",
      "Epoch [76/100], Loss: 15.9669\n",
      "Epoch [77/100], Loss: 13.8355\n",
      "Epoch [78/100], Loss: 15.0028\n",
      "Epoch [79/100], Loss: 15.2282\n",
      "Epoch [80/100], Loss: 19.5036\n",
      "Epoch [81/100], Loss: 13.4062\n",
      "Epoch [82/100], Loss: 13.3750\n",
      "Epoch [83/100], Loss: 12.7149\n",
      "Epoch [84/100], Loss: 13.2787\n",
      "Epoch [85/100], Loss: 18.4825\n",
      "Epoch [86/100], Loss: 13.3826\n",
      "Epoch [87/100], Loss: 14.0259\n",
      "Epoch [88/100], Loss: 14.8834\n",
      "Epoch [89/100], Loss: 13.1080\n",
      "Epoch [90/100], Loss: 17.2902\n",
      "Epoch [91/100], Loss: 12.2576\n",
      "Epoch [92/100], Loss: 15.5736\n",
      "Epoch [93/100], Loss: 11.8242\n",
      "Epoch [94/100], Loss: 11.7516\n",
      "Epoch [95/100], Loss: 11.1609\n",
      "Epoch [96/100], Loss: 12.6958\n",
      "Epoch [97/100], Loss: 15.0525\n",
      "Epoch [98/100], Loss: 12.8898\n",
      "Epoch [99/100], Loss: 13.4660\n",
      "Epoch [100/100], Loss: 12.4239\n",
      "Validation RMSE: 6.165801465511322\n",
      "Iteration 12: Training NN with optimizer=sgd, hidden_size=32, lr=0.0001, batch_size=32, n_epochs=100\n",
      "Epoch [1/100], Loss: 605.7396\n",
      "Epoch [2/100], Loss: 592.7074\n",
      "Epoch [3/100], Loss: 585.7136\n",
      "Epoch [4/100], Loss: 581.1216\n",
      "Epoch [5/100], Loss: 574.8695\n",
      "Epoch [6/100], Loss: 573.7079\n",
      "Epoch [7/100], Loss: 569.1877\n",
      "Epoch [8/100], Loss: 550.8329\n",
      "Epoch [9/100], Loss: 543.0934\n",
      "Epoch [10/100], Loss: 529.3994\n",
      "Epoch [11/100], Loss: 515.3912\n",
      "Epoch [12/100], Loss: 502.8362\n",
      "Epoch [13/100], Loss: 484.5072\n",
      "Epoch [14/100], Loss: 468.4403\n",
      "Epoch [15/100], Loss: 447.6063\n",
      "Epoch [16/100], Loss: 424.6795\n",
      "Epoch [17/100], Loss: 399.9214\n",
      "Epoch [18/100], Loss: 375.1133\n",
      "Epoch [19/100], Loss: 343.1714\n",
      "Epoch [20/100], Loss: 313.9083\n",
      "Epoch [21/100], Loss: 288.3162\n",
      "Epoch [22/100], Loss: 261.3956\n",
      "Epoch [23/100], Loss: 235.7396\n",
      "Epoch [24/100], Loss: 214.5522\n",
      "Epoch [25/100], Loss: 193.8676\n",
      "Epoch [26/100], Loss: 174.2202\n",
      "Epoch [27/100], Loss: 158.6819\n",
      "Epoch [28/100], Loss: 150.1744\n",
      "Epoch [29/100], Loss: 136.3927\n",
      "Epoch [30/100], Loss: 128.5258\n",
      "Epoch [31/100], Loss: 123.2038\n",
      "Epoch [32/100], Loss: 120.0004\n",
      "Epoch [33/100], Loss: 116.5677\n",
      "Epoch [34/100], Loss: 114.9489\n",
      "Epoch [35/100], Loss: 110.2045\n",
      "Epoch [36/100], Loss: 107.2251\n",
      "Epoch [37/100], Loss: 106.6449\n",
      "Epoch [38/100], Loss: 107.6182\n",
      "Epoch [39/100], Loss: 104.9489\n",
      "Epoch [40/100], Loss: 103.7746\n",
      "Epoch [41/100], Loss: 103.4498\n",
      "Epoch [42/100], Loss: 101.3998\n",
      "Epoch [43/100], Loss: 101.1708\n",
      "Epoch [44/100], Loss: 99.1915\n",
      "Epoch [45/100], Loss: 96.8006\n",
      "Epoch [46/100], Loss: 99.2093\n",
      "Epoch [47/100], Loss: 97.8899\n",
      "Epoch [48/100], Loss: 96.1063\n",
      "Epoch [49/100], Loss: 94.0773\n",
      "Epoch [50/100], Loss: 92.5408\n",
      "Epoch [51/100], Loss: 92.9675\n",
      "Epoch [52/100], Loss: 93.5439\n",
      "Epoch [53/100], Loss: 92.2271\n",
      "Epoch [54/100], Loss: 90.3310\n",
      "Epoch [55/100], Loss: 89.3347\n",
      "Epoch [56/100], Loss: 89.2342\n",
      "Epoch [57/100], Loss: 87.7250\n",
      "Epoch [58/100], Loss: 89.1310\n",
      "Epoch [59/100], Loss: 87.1869\n",
      "Epoch [60/100], Loss: 86.8022\n",
      "Epoch [61/100], Loss: 84.5000\n",
      "Epoch [62/100], Loss: 85.0677\n",
      "Epoch [63/100], Loss: 84.2758\n",
      "Epoch [64/100], Loss: 83.0382\n",
      "Epoch [65/100], Loss: 82.7963\n",
      "Epoch [66/100], Loss: 82.9706\n",
      "Epoch [67/100], Loss: 80.1336\n",
      "Epoch [68/100], Loss: 79.6903\n",
      "Epoch [69/100], Loss: 81.4915\n",
      "Epoch [70/100], Loss: 80.0258\n",
      "Epoch [71/100], Loss: 81.2284\n",
      "Epoch [72/100], Loss: 77.7078\n",
      "Epoch [73/100], Loss: 77.6421\n",
      "Epoch [74/100], Loss: 76.3945\n",
      "Epoch [75/100], Loss: 76.6238\n",
      "Epoch [76/100], Loss: 75.9218\n",
      "Epoch [77/100], Loss: 74.6232\n",
      "Epoch [78/100], Loss: 74.5681\n",
      "Epoch [79/100], Loss: 75.4231\n",
      "Epoch [80/100], Loss: 72.7252\n",
      "Epoch [81/100], Loss: 73.4931\n",
      "Epoch [82/100], Loss: 72.6416\n",
      "Epoch [83/100], Loss: 71.8880\n",
      "Epoch [84/100], Loss: 72.3007\n",
      "Epoch [85/100], Loss: 71.8591\n",
      "Epoch [86/100], Loss: 73.0228\n",
      "Epoch [87/100], Loss: 70.6373\n",
      "Epoch [88/100], Loss: 70.4214\n",
      "Epoch [89/100], Loss: 69.1494\n",
      "Epoch [90/100], Loss: 68.2601\n",
      "Epoch [91/100], Loss: 68.4611\n",
      "Epoch [92/100], Loss: 69.5657\n",
      "Epoch [93/100], Loss: 66.9766\n",
      "Epoch [94/100], Loss: 66.8050\n",
      "Epoch [95/100], Loss: 67.4386\n",
      "Epoch [96/100], Loss: 66.0433\n",
      "Epoch [97/100], Loss: 66.9118\n",
      "Epoch [98/100], Loss: 64.7359\n",
      "Epoch [99/100], Loss: 65.0250\n",
      "Epoch [100/100], Loss: 65.5241\n",
      "Validation RMSE: 47.45336389541626\n",
      "Iteration 13: Training NN with optimizer=sgd, hidden_size=64, lr=0.0001, batch_size=16, n_epochs=50\n",
      "Epoch [1/50], Loss: 588.8067\n",
      "Epoch [2/50], Loss: 566.5817\n",
      "Epoch [3/50], Loss: 556.6914\n",
      "Epoch [4/50], Loss: 525.9959\n",
      "Epoch [5/50], Loss: 482.7972\n",
      "Epoch [6/50], Loss: 441.2451\n",
      "Epoch [7/50], Loss: 398.6679\n",
      "Epoch [8/50], Loss: 324.1267\n",
      "Epoch [9/50], Loss: 261.1154\n",
      "Epoch [10/50], Loss: 212.7942\n",
      "Epoch [11/50], Loss: 171.9111\n",
      "Epoch [12/50], Loss: 140.3902\n",
      "Epoch [13/50], Loss: 121.3681\n",
      "Epoch [14/50], Loss: 112.2016\n",
      "Epoch [15/50], Loss: 104.2924\n",
      "Epoch [16/50], Loss: 99.7817\n",
      "Epoch [17/50], Loss: 97.5934\n",
      "Epoch [18/50], Loss: 94.2748\n",
      "Epoch [19/50], Loss: 92.1590\n",
      "Epoch [20/50], Loss: 91.4980\n",
      "Epoch [21/50], Loss: 90.5189\n",
      "Epoch [22/50], Loss: 87.9787\n",
      "Epoch [23/50], Loss: 85.9986\n",
      "Epoch [24/50], Loss: 84.7347\n",
      "Epoch [25/50], Loss: 85.7414\n",
      "Epoch [26/50], Loss: 83.1804\n",
      "Epoch [27/50], Loss: 81.1282\n",
      "Epoch [28/50], Loss: 85.8366\n",
      "Epoch [29/50], Loss: 83.3164\n",
      "Epoch [30/50], Loss: 81.2497\n",
      "Epoch [31/50], Loss: 77.7402\n",
      "Epoch [32/50], Loss: 74.9882\n",
      "Epoch [33/50], Loss: 73.9368\n",
      "Epoch [34/50], Loss: 72.2920\n",
      "Epoch [35/50], Loss: 70.6797\n",
      "Epoch [36/50], Loss: 70.4750\n",
      "Epoch [37/50], Loss: 68.9203\n",
      "Epoch [38/50], Loss: 67.9817\n",
      "Epoch [39/50], Loss: 68.3971\n",
      "Epoch [40/50], Loss: 66.6010\n",
      "Epoch [41/50], Loss: 65.7211\n",
      "Epoch [42/50], Loss: 65.0950\n",
      "Epoch [43/50], Loss: 64.5934\n",
      "Epoch [44/50], Loss: 63.5137\n",
      "Epoch [45/50], Loss: 63.2999\n",
      "Epoch [46/50], Loss: 62.5167\n",
      "Epoch [47/50], Loss: 64.4333\n",
      "Epoch [48/50], Loss: 67.6312\n",
      "Epoch [49/50], Loss: 59.8332\n",
      "Epoch [50/50], Loss: 60.1491\n",
      "Validation RMSE: 46.214387212480815\n",
      "Iteration 14: Training NN with optimizer=sgd, hidden_size=64, lr=0.0001, batch_size=32, n_epochs=100\n",
      "Epoch [1/100], Loss: 595.0161\n",
      "Epoch [2/100], Loss: 580.9468\n",
      "Epoch [3/100], Loss: 565.3953\n",
      "Epoch [4/100], Loss: 554.9863\n",
      "Epoch [5/100], Loss: 538.3588\n",
      "Epoch [6/100], Loss: 520.1040\n",
      "Epoch [7/100], Loss: 505.4729\n",
      "Epoch [8/100], Loss: 489.4105\n",
      "Epoch [9/100], Loss: 464.1300\n",
      "Epoch [10/100], Loss: 438.5046\n",
      "Epoch [11/100], Loss: 414.9546\n",
      "Epoch [12/100], Loss: 383.3808\n",
      "Epoch [13/100], Loss: 357.4516\n",
      "Epoch [14/100], Loss: 327.1785\n",
      "Epoch [15/100], Loss: 297.0602\n",
      "Epoch [16/100], Loss: 265.5182\n",
      "Epoch [17/100], Loss: 239.2388\n",
      "Epoch [18/100], Loss: 214.8186\n",
      "Epoch [19/100], Loss: 193.8632\n",
      "Epoch [20/100], Loss: 178.0239\n",
      "Epoch [21/100], Loss: 161.2977\n",
      "Epoch [22/100], Loss: 150.1772\n",
      "Epoch [23/100], Loss: 141.7241\n",
      "Epoch [24/100], Loss: 130.4593\n",
      "Epoch [25/100], Loss: 125.1115\n",
      "Epoch [26/100], Loss: 124.1873\n",
      "Epoch [27/100], Loss: 118.3405\n",
      "Epoch [28/100], Loss: 115.4007\n",
      "Epoch [29/100], Loss: 113.4683\n",
      "Epoch [30/100], Loss: 112.6056\n",
      "Epoch [31/100], Loss: 108.1159\n",
      "Epoch [32/100], Loss: 106.9973\n",
      "Epoch [33/100], Loss: 105.8599\n",
      "Epoch [34/100], Loss: 106.6379\n",
      "Epoch [35/100], Loss: 105.0975\n",
      "Epoch [36/100], Loss: 101.6086\n",
      "Epoch [37/100], Loss: 101.1778\n",
      "Epoch [38/100], Loss: 99.9847\n",
      "Epoch [39/100], Loss: 100.3440\n",
      "Epoch [40/100], Loss: 100.5528\n",
      "Epoch [41/100], Loss: 99.0645\n",
      "Epoch [42/100], Loss: 97.1568\n",
      "Epoch [43/100], Loss: 95.9964\n",
      "Epoch [44/100], Loss: 94.9731\n",
      "Epoch [45/100], Loss: 95.1209\n",
      "Epoch [46/100], Loss: 94.2375\n",
      "Epoch [47/100], Loss: 92.9400\n",
      "Epoch [48/100], Loss: 92.0877\n",
      "Epoch [49/100], Loss: 90.2973\n",
      "Epoch [50/100], Loss: 90.7369\n",
      "Epoch [51/100], Loss: 88.4031\n",
      "Epoch [52/100], Loss: 87.0204\n",
      "Epoch [53/100], Loss: 87.4844\n",
      "Epoch [54/100], Loss: 87.0438\n",
      "Epoch [55/100], Loss: 87.8139\n",
      "Epoch [56/100], Loss: 87.1891\n",
      "Epoch [57/100], Loss: 87.6050\n",
      "Epoch [58/100], Loss: 84.8190\n",
      "Epoch [59/100], Loss: 83.4234\n",
      "Epoch [60/100], Loss: 82.9070\n",
      "Epoch [61/100], Loss: 82.9084\n",
      "Epoch [62/100], Loss: 81.6802\n",
      "Epoch [63/100], Loss: 80.9356\n",
      "Epoch [64/100], Loss: 79.8955\n",
      "Epoch [65/100], Loss: 79.7187\n",
      "Epoch [66/100], Loss: 78.9124\n",
      "Epoch [67/100], Loss: 77.3563\n",
      "Epoch [68/100], Loss: 78.5880\n",
      "Epoch [69/100], Loss: 79.0508\n",
      "Epoch [70/100], Loss: 75.2692\n",
      "Epoch [71/100], Loss: 76.1960\n",
      "Epoch [72/100], Loss: 76.2185\n",
      "Epoch [73/100], Loss: 73.9720\n",
      "Epoch [74/100], Loss: 74.4840\n",
      "Epoch [75/100], Loss: 74.1933\n",
      "Epoch [76/100], Loss: 72.7522\n",
      "Epoch [77/100], Loss: 74.5706\n",
      "Epoch [78/100], Loss: 73.0768\n",
      "Epoch [79/100], Loss: 72.4785\n",
      "Epoch [80/100], Loss: 71.8371\n",
      "Epoch [81/100], Loss: 71.6742\n",
      "Epoch [82/100], Loss: 72.1676\n",
      "Epoch [83/100], Loss: 69.3774\n",
      "Epoch [84/100], Loss: 70.1621\n",
      "Epoch [85/100], Loss: 69.5481\n",
      "Epoch [86/100], Loss: 67.9184\n",
      "Epoch [87/100], Loss: 67.7318\n",
      "Epoch [88/100], Loss: 68.6822\n",
      "Epoch [89/100], Loss: 68.4294\n",
      "Epoch [90/100], Loss: 66.2889\n",
      "Epoch [91/100], Loss: 66.2367\n",
      "Epoch [92/100], Loss: 67.3026\n",
      "Epoch [93/100], Loss: 65.7102\n",
      "Epoch [94/100], Loss: 64.8869\n",
      "Epoch [95/100], Loss: 65.2956\n",
      "Epoch [96/100], Loss: 63.6765\n",
      "Epoch [97/100], Loss: 65.5470\n",
      "Epoch [98/100], Loss: 63.2355\n",
      "Epoch [99/100], Loss: 63.0256\n",
      "Epoch [100/100], Loss: 64.2736\n",
      "Validation RMSE: 46.28124141693115\n",
      "Iteration 15: Training NN with optimizer=sgd, hidden_size=32, lr=0.01, batch_size=32, n_epochs=150\n",
      "Epoch [1/150], Loss: 193.3503\n",
      "Epoch [2/150], Loss: 72.6528\n",
      "Epoch [3/150], Loss: 77.2171\n",
      "Epoch [4/150], Loss: 43.9510\n",
      "Epoch [5/150], Loss: 36.6777\n",
      "Epoch [6/150], Loss: 37.6011\n",
      "Epoch [7/150], Loss: 32.5932\n",
      "Epoch [8/150], Loss: 31.1542\n",
      "Epoch [9/150], Loss: 36.0223\n",
      "Epoch [10/150], Loss: 33.6663\n",
      "Epoch [11/150], Loss: 42.1939\n",
      "Epoch [12/150], Loss: 42.3795\n",
      "Epoch [13/150], Loss: 43.5941\n",
      "Epoch [14/150], Loss: 24.8642\n",
      "Epoch [15/150], Loss: 27.2181\n",
      "Epoch [16/150], Loss: 36.3192\n",
      "Epoch [17/150], Loss: 25.5564\n",
      "Epoch [18/150], Loss: 27.1486\n",
      "Epoch [19/150], Loss: 32.1214\n",
      "Epoch [20/150], Loss: 38.2472\n",
      "Epoch [21/150], Loss: 29.3256\n",
      "Epoch [22/150], Loss: 30.7635\n",
      "Epoch [23/150], Loss: 29.3668\n",
      "Epoch [24/150], Loss: 34.0588\n",
      "Epoch [25/150], Loss: 22.8930\n",
      "Epoch [26/150], Loss: 23.5211\n",
      "Epoch [27/150], Loss: 32.3006\n",
      "Epoch [28/150], Loss: 27.7231\n",
      "Epoch [29/150], Loss: 32.2002\n",
      "Epoch [30/150], Loss: 29.5001\n",
      "Epoch [31/150], Loss: 21.6537\n",
      "Epoch [32/150], Loss: 25.3487\n",
      "Epoch [33/150], Loss: 21.6614\n",
      "Epoch [34/150], Loss: 29.8657\n",
      "Epoch [35/150], Loss: 24.9492\n",
      "Epoch [36/150], Loss: 21.8714\n",
      "Epoch [37/150], Loss: 25.5189\n",
      "Epoch [38/150], Loss: 22.2979\n",
      "Epoch [39/150], Loss: 19.5852\n",
      "Epoch [40/150], Loss: 21.2677\n",
      "Epoch [41/150], Loss: 21.4975\n",
      "Epoch [42/150], Loss: 22.8563\n",
      "Epoch [43/150], Loss: 18.3608\n",
      "Epoch [44/150], Loss: 22.7860\n",
      "Epoch [45/150], Loss: 21.5899\n",
      "Epoch [46/150], Loss: 21.1211\n",
      "Epoch [47/150], Loss: 19.0469\n",
      "Epoch [48/150], Loss: 18.9716\n",
      "Epoch [49/150], Loss: 19.9664\n",
      "Epoch [50/150], Loss: 18.2847\n",
      "Epoch [51/150], Loss: 18.6877\n",
      "Epoch [52/150], Loss: 17.9258\n",
      "Epoch [53/150], Loss: 18.7142\n",
      "Epoch [54/150], Loss: 17.8358\n",
      "Epoch [55/150], Loss: 18.1995\n",
      "Epoch [56/150], Loss: 17.9370\n",
      "Epoch [57/150], Loss: 19.7493\n",
      "Epoch [58/150], Loss: 24.2779\n",
      "Epoch [59/150], Loss: 19.7268\n",
      "Epoch [60/150], Loss: 20.4149\n",
      "Epoch [61/150], Loss: 19.1177\n",
      "Epoch [62/150], Loss: 18.0189\n",
      "Epoch [63/150], Loss: 20.8403\n",
      "Epoch [64/150], Loss: 19.7443\n",
      "Epoch [65/150], Loss: 17.4209\n",
      "Epoch [66/150], Loss: 15.1926\n",
      "Epoch [67/150], Loss: 18.6893\n",
      "Epoch [68/150], Loss: 17.4532\n",
      "Epoch [69/150], Loss: 22.3831\n",
      "Epoch [70/150], Loss: 19.2740\n",
      "Epoch [71/150], Loss: 17.2091\n",
      "Epoch [72/150], Loss: 15.1541\n",
      "Epoch [73/150], Loss: 16.0416\n",
      "Epoch [74/150], Loss: 17.2805\n",
      "Epoch [75/150], Loss: 18.8528\n",
      "Epoch [76/150], Loss: 16.3606\n",
      "Epoch [77/150], Loss: 17.4861\n",
      "Epoch [78/150], Loss: 17.4559\n",
      "Epoch [79/150], Loss: 16.5695\n",
      "Epoch [80/150], Loss: 14.4888\n",
      "Epoch [81/150], Loss: 16.3954\n",
      "Epoch [82/150], Loss: 20.2182\n",
      "Epoch [83/150], Loss: 16.5258\n",
      "Epoch [84/150], Loss: 16.5294\n",
      "Epoch [85/150], Loss: 15.0681\n",
      "Epoch [86/150], Loss: 14.8948\n",
      "Epoch [87/150], Loss: 16.5562\n",
      "Epoch [88/150], Loss: 17.9593\n",
      "Epoch [89/150], Loss: 18.5845\n",
      "Epoch [90/150], Loss: 13.2620\n",
      "Epoch [91/150], Loss: 14.4984\n",
      "Epoch [92/150], Loss: 15.7248\n",
      "Epoch [93/150], Loss: 14.6938\n",
      "Epoch [94/150], Loss: 17.0688\n",
      "Epoch [95/150], Loss: 14.1830\n",
      "Epoch [96/150], Loss: 16.8543\n",
      "Epoch [97/150], Loss: 13.7075\n",
      "Epoch [98/150], Loss: 13.4925\n",
      "Epoch [99/150], Loss: 14.8904\n",
      "Epoch [100/150], Loss: 16.0325\n",
      "Epoch [101/150], Loss: 15.5771\n",
      "Epoch [102/150], Loss: 13.9878\n",
      "Epoch [103/150], Loss: 12.8715\n",
      "Epoch [104/150], Loss: 12.6199\n",
      "Epoch [105/150], Loss: 13.7934\n",
      "Epoch [106/150], Loss: 13.5565\n",
      "Epoch [107/150], Loss: 11.8216\n",
      "Epoch [108/150], Loss: 13.6781\n",
      "Epoch [109/150], Loss: 12.8449\n",
      "Epoch [110/150], Loss: 13.7347\n",
      "Epoch [111/150], Loss: 13.3102\n",
      "Epoch [112/150], Loss: 12.6610\n",
      "Epoch [113/150], Loss: 11.5275\n",
      "Epoch [114/150], Loss: 13.0275\n",
      "Epoch [115/150], Loss: 13.9705\n",
      "Epoch [116/150], Loss: 13.9338\n",
      "Epoch [117/150], Loss: 13.4365\n",
      "Epoch [118/150], Loss: 11.8946\n",
      "Epoch [119/150], Loss: 13.2525\n",
      "Epoch [120/150], Loss: 13.6102\n",
      "Epoch [121/150], Loss: 11.1247\n",
      "Epoch [122/150], Loss: 15.8624\n",
      "Epoch [123/150], Loss: 10.5448\n",
      "Epoch [124/150], Loss: 10.8903\n",
      "Epoch [125/150], Loss: 12.8829\n",
      "Epoch [126/150], Loss: 11.2912\n",
      "Epoch [127/150], Loss: 11.8730\n",
      "Epoch [128/150], Loss: 11.2859\n",
      "Epoch [129/150], Loss: 11.9328\n",
      "Epoch [130/150], Loss: 12.8728\n",
      "Epoch [131/150], Loss: 12.3366\n",
      "Epoch [132/150], Loss: 10.5857\n",
      "Epoch [133/150], Loss: 12.0375\n",
      "Epoch [134/150], Loss: 12.7831\n",
      "Epoch [135/150], Loss: 13.0235\n",
      "Epoch [136/150], Loss: 11.5296\n",
      "Epoch [137/150], Loss: 11.0681\n",
      "Epoch [138/150], Loss: 14.1563\n",
      "Epoch [139/150], Loss: 16.7361\n",
      "Epoch [140/150], Loss: 12.2344\n",
      "Epoch [141/150], Loss: 10.4220\n",
      "Epoch [142/150], Loss: 10.1011\n",
      "Epoch [143/150], Loss: 11.8778\n",
      "Epoch [144/150], Loss: 10.9049\n",
      "Epoch [145/150], Loss: 10.1658\n",
      "Epoch [146/150], Loss: 11.6792\n",
      "Epoch [147/150], Loss: 11.0380\n",
      "Epoch [148/150], Loss: 11.3940\n",
      "Epoch [149/150], Loss: 13.4880\n",
      "Epoch [150/150], Loss: 12.2619\n",
      "Validation RMSE: 5.7026718854904175\n",
      "Iteration 16: Training NN with optimizer=sgd, hidden_size=128, lr=0.01, batch_size=32, n_epochs=150\n",
      "Epoch [1/150], Loss: 199.5706\n",
      "Epoch [2/150], Loss: 59.8922\n",
      "Epoch [3/150], Loss: 53.4678\n",
      "Epoch [4/150], Loss: 49.3461\n",
      "Epoch [5/150], Loss: 40.7808\n",
      "Epoch [6/150], Loss: 46.1906\n",
      "Epoch [7/150], Loss: 35.7103\n",
      "Epoch [8/150], Loss: 47.2410\n",
      "Epoch [9/150], Loss: 31.0711\n",
      "Epoch [10/150], Loss: 29.7031\n",
      "Epoch [11/150], Loss: 42.5870\n",
      "Epoch [12/150], Loss: 33.3813\n",
      "Epoch [13/150], Loss: 25.8235\n",
      "Epoch [14/150], Loss: 28.4003\n",
      "Epoch [15/150], Loss: 25.2903\n",
      "Epoch [16/150], Loss: 26.3125\n",
      "Epoch [17/150], Loss: 25.7449\n",
      "Epoch [18/150], Loss: 28.5730\n",
      "Epoch [19/150], Loss: 29.8657\n",
      "Epoch [20/150], Loss: 27.1174\n",
      "Epoch [21/150], Loss: 23.4033\n",
      "Epoch [22/150], Loss: 28.6928\n",
      "Epoch [23/150], Loss: 22.9098\n",
      "Epoch [24/150], Loss: 24.1383\n",
      "Epoch [25/150], Loss: 21.9412\n",
      "Epoch [26/150], Loss: 32.5656\n",
      "Epoch [27/150], Loss: 21.9034\n",
      "Epoch [28/150], Loss: 21.3122\n",
      "Epoch [29/150], Loss: 22.5295\n",
      "Epoch [30/150], Loss: 21.3537\n",
      "Epoch [31/150], Loss: 20.2872\n",
      "Epoch [32/150], Loss: 22.2532\n",
      "Epoch [33/150], Loss: 21.3784\n",
      "Epoch [34/150], Loss: 22.0980\n",
      "Epoch [35/150], Loss: 24.9710\n",
      "Epoch [36/150], Loss: 21.8343\n",
      "Epoch [37/150], Loss: 18.7253\n",
      "Epoch [38/150], Loss: 21.4853\n",
      "Epoch [39/150], Loss: 20.8263\n",
      "Epoch [40/150], Loss: 20.7668\n",
      "Epoch [41/150], Loss: 18.9605\n",
      "Epoch [42/150], Loss: 18.0625\n",
      "Epoch [43/150], Loss: 19.5489\n",
      "Epoch [44/150], Loss: 18.7334\n",
      "Epoch [45/150], Loss: 16.4972\n",
      "Epoch [46/150], Loss: 17.3277\n",
      "Epoch [47/150], Loss: 19.6368\n",
      "Epoch [48/150], Loss: 23.9034\n",
      "Epoch [49/150], Loss: 17.3823\n",
      "Epoch [50/150], Loss: 18.2690\n",
      "Epoch [51/150], Loss: 18.3567\n",
      "Epoch [52/150], Loss: 22.8960\n",
      "Epoch [53/150], Loss: 19.4426\n",
      "Epoch [54/150], Loss: 16.2905\n",
      "Epoch [55/150], Loss: 23.7683\n",
      "Epoch [56/150], Loss: 28.2943\n",
      "Epoch [57/150], Loss: 17.7989\n",
      "Epoch [58/150], Loss: 15.4026\n",
      "Epoch [59/150], Loss: 14.2804\n",
      "Epoch [60/150], Loss: 16.9139\n",
      "Epoch [61/150], Loss: 19.1814\n",
      "Epoch [62/150], Loss: 15.7774\n",
      "Epoch [63/150], Loss: 15.5678\n",
      "Epoch [64/150], Loss: 26.5160\n",
      "Epoch [65/150], Loss: 14.7738\n",
      "Epoch [66/150], Loss: 16.6987\n",
      "Epoch [67/150], Loss: 22.4125\n",
      "Epoch [68/150], Loss: 14.9748\n",
      "Epoch [69/150], Loss: 14.7509\n",
      "Epoch [70/150], Loss: 14.7120\n",
      "Epoch [71/150], Loss: 17.7546\n",
      "Epoch [72/150], Loss: 17.3325\n",
      "Epoch [73/150], Loss: 13.9806\n",
      "Epoch [74/150], Loss: 13.4001\n",
      "Epoch [75/150], Loss: 14.5085\n",
      "Epoch [76/150], Loss: 15.2866\n",
      "Epoch [77/150], Loss: 15.6592\n",
      "Epoch [78/150], Loss: 15.0064\n",
      "Epoch [79/150], Loss: 16.1409\n",
      "Epoch [80/150], Loss: 14.1913\n",
      "Epoch [81/150], Loss: 14.4160\n",
      "Epoch [82/150], Loss: 13.9084\n",
      "Epoch [83/150], Loss: 17.7663\n",
      "Epoch [84/150], Loss: 13.1125\n",
      "Epoch [85/150], Loss: 15.3761\n",
      "Epoch [86/150], Loss: 12.3672\n",
      "Epoch [87/150], Loss: 14.2829\n",
      "Epoch [88/150], Loss: 16.8353\n",
      "Epoch [89/150], Loss: 15.0264\n",
      "Epoch [90/150], Loss: 12.4786\n",
      "Epoch [91/150], Loss: 12.5567\n",
      "Epoch [92/150], Loss: 14.0145\n",
      "Epoch [93/150], Loss: 12.2546\n",
      "Epoch [94/150], Loss: 12.0609\n",
      "Epoch [95/150], Loss: 11.2384\n",
      "Epoch [96/150], Loss: 11.9720\n",
      "Epoch [97/150], Loss: 12.2493\n",
      "Epoch [98/150], Loss: 12.7921\n",
      "Epoch [99/150], Loss: 11.3310\n",
      "Epoch [100/150], Loss: 14.9142\n",
      "Epoch [101/150], Loss: 17.1905\n",
      "Epoch [102/150], Loss: 13.0220\n",
      "Epoch [103/150], Loss: 11.0114\n",
      "Epoch [104/150], Loss: 12.1589\n",
      "Epoch [105/150], Loss: 12.1715\n",
      "Epoch [106/150], Loss: 11.6097\n",
      "Epoch [107/150], Loss: 11.2508\n",
      "Epoch [108/150], Loss: 10.5049\n",
      "Epoch [109/150], Loss: 12.0696\n",
      "Epoch [110/150], Loss: 12.4878\n",
      "Epoch [111/150], Loss: 13.2370\n",
      "Epoch [112/150], Loss: 14.7064\n",
      "Epoch [113/150], Loss: 10.1435\n",
      "Epoch [114/150], Loss: 11.6467\n",
      "Epoch [115/150], Loss: 10.4576\n",
      "Epoch [116/150], Loss: 10.7316\n",
      "Epoch [117/150], Loss: 12.6006\n",
      "Epoch [118/150], Loss: 9.8932\n",
      "Epoch [119/150], Loss: 11.4772\n",
      "Epoch [120/150], Loss: 13.3378\n",
      "Epoch [121/150], Loss: 10.1295\n",
      "Epoch [122/150], Loss: 11.1193\n",
      "Epoch [123/150], Loss: 13.8183\n",
      "Epoch [124/150], Loss: 11.4439\n",
      "Epoch [125/150], Loss: 9.7383\n",
      "Epoch [126/150], Loss: 10.2977\n",
      "Epoch [127/150], Loss: 12.9441\n",
      "Epoch [128/150], Loss: 10.1714\n",
      "Epoch [129/150], Loss: 11.5268\n",
      "Epoch [130/150], Loss: 11.1245\n",
      "Epoch [131/150], Loss: 9.5303\n",
      "Epoch [132/150], Loss: 10.2859\n",
      "Epoch [133/150], Loss: 12.6307\n",
      "Epoch [134/150], Loss: 12.9261\n",
      "Epoch [135/150], Loss: 10.1507\n",
      "Epoch [136/150], Loss: 11.8322\n",
      "Epoch [137/150], Loss: 10.5266\n",
      "Epoch [138/150], Loss: 11.4546\n",
      "Epoch [139/150], Loss: 9.4226\n",
      "Epoch [140/150], Loss: 8.8073\n",
      "Epoch [141/150], Loss: 10.9898\n",
      "Epoch [142/150], Loss: 11.2626\n",
      "Epoch [143/150], Loss: 9.7242\n",
      "Epoch [144/150], Loss: 12.0513\n",
      "Epoch [145/150], Loss: 10.0077\n",
      "Epoch [146/150], Loss: 10.1858\n",
      "Epoch [147/150], Loss: 8.9354\n",
      "Epoch [148/150], Loss: 8.7506\n",
      "Epoch [149/150], Loss: 10.7079\n",
      "Epoch [150/150], Loss: 9.9267\n",
      "Validation RMSE: 6.550928711891174\n",
      "Iteration 17: Training NN with optimizer=sgd, hidden_size=32, lr=0.01, batch_size=64, n_epochs=50\n",
      "Epoch [1/50], Loss: 312.1729\n",
      "Epoch [2/50], Loss: 80.5455\n",
      "Epoch [3/50], Loss: 56.7596\n",
      "Epoch [4/50], Loss: 54.0228\n",
      "Epoch [5/50], Loss: 45.9868\n",
      "Epoch [6/50], Loss: 43.6690\n",
      "Epoch [7/50], Loss: 71.2413\n",
      "Epoch [8/50], Loss: 42.7178\n",
      "Epoch [9/50], Loss: 62.8159\n",
      "Epoch [10/50], Loss: 60.5016\n",
      "Epoch [11/50], Loss: 40.9824\n",
      "Epoch [12/50], Loss: 36.3319\n",
      "Epoch [13/50], Loss: 34.4264\n",
      "Epoch [14/50], Loss: 34.2287\n",
      "Epoch [15/50], Loss: 28.4848\n",
      "Epoch [16/50], Loss: 28.4863\n",
      "Epoch [17/50], Loss: 25.7641\n",
      "Epoch [18/50], Loss: 25.9444\n",
      "Epoch [19/50], Loss: 57.7383\n",
      "Epoch [20/50], Loss: 39.4081\n",
      "Epoch [21/50], Loss: 27.8923\n",
      "Epoch [22/50], Loss: 53.6808\n",
      "Epoch [23/50], Loss: 53.0466\n",
      "Epoch [24/50], Loss: 33.0603\n",
      "Epoch [25/50], Loss: 25.4728\n",
      "Epoch [26/50], Loss: 30.3962\n",
      "Epoch [27/50], Loss: 33.7101\n",
      "Epoch [28/50], Loss: 25.0458\n",
      "Epoch [29/50], Loss: 31.4173\n",
      "Epoch [30/50], Loss: 33.6635\n",
      "Epoch [31/50], Loss: 23.6898\n",
      "Epoch [32/50], Loss: 27.6644\n",
      "Epoch [33/50], Loss: 21.3175\n",
      "Epoch [34/50], Loss: 25.1975\n",
      "Epoch [35/50], Loss: 28.4762\n",
      "Epoch [36/50], Loss: 23.2878\n",
      "Epoch [37/50], Loss: 24.3169\n",
      "Epoch [38/50], Loss: 41.4527\n",
      "Epoch [39/50], Loss: 22.4345\n",
      "Epoch [40/50], Loss: 24.4491\n",
      "Epoch [41/50], Loss: 25.9081\n",
      "Epoch [42/50], Loss: 22.5630\n",
      "Epoch [43/50], Loss: 20.7215\n",
      "Epoch [44/50], Loss: 30.7826\n",
      "Epoch [45/50], Loss: 25.1296\n",
      "Epoch [46/50], Loss: 25.4998\n",
      "Epoch [47/50], Loss: 23.4045\n",
      "Epoch [48/50], Loss: 19.5647\n",
      "Epoch [49/50], Loss: 18.9953\n",
      "Epoch [50/50], Loss: 19.9296\n",
      "Validation RMSE: 12.537595272064209\n",
      "Iteration 18: Training NN with optimizer=sgd, hidden_size=64, lr=0.01, batch_size=16, n_epochs=150\n",
      "Epoch [1/150], Loss: 129.0048\n",
      "Epoch [2/150], Loss: 79.4875\n",
      "Epoch [3/150], Loss: 51.6746\n",
      "Epoch [4/150], Loss: 46.6465\n",
      "Epoch [5/150], Loss: 41.1258\n",
      "Epoch [6/150], Loss: 36.8610\n",
      "Epoch [7/150], Loss: 42.6229\n",
      "Epoch [8/150], Loss: 30.3345\n",
      "Epoch [9/150], Loss: 39.0729\n",
      "Epoch [10/150], Loss: 30.0218\n",
      "Epoch [11/150], Loss: 25.0601\n",
      "Epoch [12/150], Loss: 26.3097\n",
      "Epoch [13/150], Loss: 24.5429\n",
      "Epoch [14/150], Loss: 35.6997\n",
      "Epoch [15/150], Loss: 24.6725\n",
      "Epoch [16/150], Loss: 23.2133\n",
      "Epoch [17/150], Loss: 23.3746\n",
      "Epoch [18/150], Loss: 27.9686\n",
      "Epoch [19/150], Loss: 29.7192\n",
      "Epoch [20/150], Loss: 28.4339\n",
      "Epoch [21/150], Loss: 26.8255\n",
      "Epoch [22/150], Loss: 23.5628\n",
      "Epoch [23/150], Loss: 25.8924\n",
      "Epoch [24/150], Loss: 20.9125\n",
      "Epoch [25/150], Loss: 20.8583\n",
      "Epoch [26/150], Loss: 21.4245\n",
      "Epoch [27/150], Loss: 22.8341\n",
      "Epoch [28/150], Loss: 21.9919\n",
      "Epoch [29/150], Loss: 19.3086\n",
      "Epoch [30/150], Loss: 23.1215\n",
      "Epoch [31/150], Loss: 24.4378\n",
      "Epoch [32/150], Loss: 18.6523\n",
      "Epoch [33/150], Loss: 23.2623\n",
      "Epoch [34/150], Loss: 38.2404\n",
      "Epoch [35/150], Loss: 22.3477\n",
      "Epoch [36/150], Loss: 25.2669\n",
      "Epoch [37/150], Loss: 19.6992\n",
      "Epoch [38/150], Loss: 18.2355\n",
      "Epoch [39/150], Loss: 22.2402\n",
      "Epoch [40/150], Loss: 16.1929\n",
      "Epoch [41/150], Loss: 18.1011\n",
      "Epoch [42/150], Loss: 17.8624\n",
      "Epoch [43/150], Loss: 19.5458\n",
      "Epoch [44/150], Loss: 17.1545\n",
      "Epoch [45/150], Loss: 19.9535\n",
      "Epoch [46/150], Loss: 16.1666\n",
      "Epoch [47/150], Loss: 13.6333\n",
      "Epoch [48/150], Loss: 19.1858\n",
      "Epoch [49/150], Loss: 37.7480\n",
      "Epoch [50/150], Loss: 16.3866\n",
      "Epoch [51/150], Loss: 13.4485\n",
      "Epoch [52/150], Loss: 13.2202\n",
      "Epoch [53/150], Loss: 14.2726\n",
      "Epoch [54/150], Loss: 15.3300\n",
      "Epoch [55/150], Loss: 14.1725\n",
      "Epoch [56/150], Loss: 13.7563\n",
      "Epoch [57/150], Loss: 13.3940\n",
      "Epoch [58/150], Loss: 13.8019\n",
      "Epoch [59/150], Loss: 14.5804\n",
      "Epoch [60/150], Loss: 16.0431\n",
      "Epoch [61/150], Loss: 13.2973\n",
      "Epoch [62/150], Loss: 12.0144\n",
      "Epoch [63/150], Loss: 12.5119\n",
      "Epoch [64/150], Loss: 12.2787\n",
      "Epoch [65/150], Loss: 12.5145\n",
      "Epoch [66/150], Loss: 12.4105\n",
      "Epoch [67/150], Loss: 13.0043\n",
      "Epoch [68/150], Loss: 11.8438\n",
      "Epoch [69/150], Loss: 12.9342\n",
      "Epoch [70/150], Loss: 12.0537\n",
      "Epoch [71/150], Loss: 11.4132\n",
      "Epoch [72/150], Loss: 12.9705\n",
      "Epoch [73/150], Loss: 11.2284\n",
      "Epoch [74/150], Loss: 12.9899\n",
      "Epoch [75/150], Loss: 12.0233\n",
      "Epoch [76/150], Loss: 10.6663\n",
      "Epoch [77/150], Loss: 11.3691\n",
      "Epoch [78/150], Loss: 12.5224\n",
      "Epoch [79/150], Loss: 11.6432\n",
      "Epoch [80/150], Loss: 10.2202\n",
      "Epoch [81/150], Loss: 10.5769\n",
      "Epoch [82/150], Loss: 11.0550\n",
      "Epoch [83/150], Loss: 12.6490\n",
      "Epoch [84/150], Loss: 10.5140\n",
      "Epoch [85/150], Loss: 10.3650\n",
      "Epoch [86/150], Loss: 11.8954\n",
      "Epoch [87/150], Loss: 9.9683\n",
      "Epoch [88/150], Loss: 10.6313\n",
      "Epoch [89/150], Loss: 10.5723\n",
      "Epoch [90/150], Loss: 10.2512\n",
      "Epoch [91/150], Loss: 11.2407\n",
      "Epoch [92/150], Loss: 9.7113\n",
      "Epoch [93/150], Loss: 10.1171\n",
      "Epoch [94/150], Loss: 11.5662\n",
      "Epoch [95/150], Loss: 10.7272\n",
      "Epoch [96/150], Loss: 10.1506\n",
      "Epoch [97/150], Loss: 9.7784\n",
      "Epoch [98/150], Loss: 10.2601\n",
      "Epoch [99/150], Loss: 12.4430\n",
      "Epoch [100/150], Loss: 16.8734\n",
      "Epoch [101/150], Loss: 10.2824\n",
      "Epoch [102/150], Loss: 10.2313\n",
      "Epoch [103/150], Loss: 9.7385\n",
      "Epoch [104/150], Loss: 11.5645\n",
      "Epoch [105/150], Loss: 10.0071\n",
      "Epoch [106/150], Loss: 10.0172\n",
      "Epoch [107/150], Loss: 10.0014\n",
      "Epoch [108/150], Loss: 10.7575\n",
      "Epoch [109/150], Loss: 9.0252\n",
      "Epoch [110/150], Loss: 9.0166\n",
      "Epoch [111/150], Loss: 9.7734\n",
      "Epoch [112/150], Loss: 9.7561\n",
      "Epoch [113/150], Loss: 13.1248\n",
      "Epoch [114/150], Loss: 9.1604\n",
      "Epoch [115/150], Loss: 8.8331\n",
      "Epoch [116/150], Loss: 9.0487\n",
      "Epoch [117/150], Loss: 10.5743\n",
      "Epoch [118/150], Loss: 9.0562\n",
      "Epoch [119/150], Loss: 9.8236\n",
      "Epoch [120/150], Loss: 9.8063\n",
      "Epoch [121/150], Loss: 9.2926\n",
      "Epoch [122/150], Loss: 9.0451\n",
      "Epoch [123/150], Loss: 9.3361\n",
      "Epoch [124/150], Loss: 8.9697\n",
      "Epoch [125/150], Loss: 8.8492\n",
      "Epoch [126/150], Loss: 9.2395\n",
      "Epoch [127/150], Loss: 8.8383\n",
      "Epoch [128/150], Loss: 10.0585\n",
      "Epoch [129/150], Loss: 8.6569\n",
      "Epoch [130/150], Loss: 9.1447\n",
      "Epoch [131/150], Loss: 9.9264\n",
      "Epoch [132/150], Loss: 9.1746\n",
      "Epoch [133/150], Loss: 9.4739\n",
      "Epoch [134/150], Loss: 9.9319\n",
      "Epoch [135/150], Loss: 8.5569\n",
      "Epoch [136/150], Loss: 8.0756\n",
      "Epoch [137/150], Loss: 9.6235\n",
      "Epoch [138/150], Loss: 8.2009\n",
      "Epoch [139/150], Loss: 8.9378\n",
      "Epoch [140/150], Loss: 8.7848\n",
      "Epoch [141/150], Loss: 8.1120\n",
      "Epoch [142/150], Loss: 9.9377\n",
      "Epoch [143/150], Loss: 7.6367\n",
      "Epoch [144/150], Loss: 8.0841\n",
      "Epoch [145/150], Loss: 8.8954\n",
      "Epoch [146/150], Loss: 8.9750\n",
      "Epoch [147/150], Loss: 7.7018\n",
      "Epoch [148/150], Loss: 8.3122\n",
      "Epoch [149/150], Loss: 8.0523\n",
      "Epoch [150/150], Loss: 9.9675\n",
      "Validation RMSE: 8.791058949061803\n",
      "Iteration 19: Training NN with optimizer=sgd, hidden_size=128, lr=0.0001, batch_size=16, n_epochs=50\n",
      "Epoch [1/50], Loss: 584.0443\n",
      "Epoch [2/50], Loss: 534.4084\n",
      "Epoch [3/50], Loss: 481.9814\n",
      "Epoch [4/50], Loss: 447.2199\n",
      "Epoch [5/50], Loss: 371.4871\n",
      "Epoch [6/50], Loss: 327.7989\n",
      "Epoch [7/50], Loss: 252.4318\n",
      "Epoch [8/50], Loss: 201.5639\n",
      "Epoch [9/50], Loss: 165.3334\n",
      "Epoch [10/50], Loss: 138.9592\n",
      "Epoch [11/50], Loss: 125.8971\n",
      "Epoch [12/50], Loss: 118.0591\n",
      "Epoch [13/50], Loss: 107.0860\n",
      "Epoch [14/50], Loss: 106.3923\n",
      "Epoch [15/50], Loss: 102.6170\n",
      "Epoch [16/50], Loss: 100.2908\n",
      "Epoch [17/50], Loss: 98.6997\n",
      "Epoch [18/50], Loss: 100.1845\n",
      "Epoch [19/50], Loss: 94.2867\n",
      "Epoch [20/50], Loss: 93.0408\n",
      "Epoch [21/50], Loss: 89.6808\n",
      "Epoch [22/50], Loss: 86.8728\n",
      "Epoch [23/50], Loss: 86.6594\n",
      "Epoch [24/50], Loss: 88.1437\n",
      "Epoch [25/50], Loss: 84.6909\n",
      "Epoch [26/50], Loss: 82.0626\n",
      "Epoch [27/50], Loss: 80.5230\n",
      "Epoch [28/50], Loss: 82.9282\n",
      "Epoch [29/50], Loss: 78.5072\n",
      "Epoch [30/50], Loss: 78.4479\n",
      "Epoch [31/50], Loss: 74.9486\n",
      "Epoch [32/50], Loss: 73.6479\n",
      "Epoch [33/50], Loss: 73.0701\n",
      "Epoch [34/50], Loss: 71.6104\n",
      "Epoch [35/50], Loss: 77.1917\n",
      "Epoch [36/50], Loss: 68.6910\n",
      "Epoch [37/50], Loss: 69.9197\n",
      "Epoch [38/50], Loss: 67.2512\n",
      "Epoch [39/50], Loss: 66.0720\n",
      "Epoch [40/50], Loss: 66.7598\n",
      "Epoch [41/50], Loss: 63.6909\n",
      "Epoch [42/50], Loss: 64.1088\n",
      "Epoch [43/50], Loss: 63.5507\n",
      "Epoch [44/50], Loss: 62.5322\n",
      "Epoch [45/50], Loss: 65.8215\n",
      "Epoch [46/50], Loss: 66.2809\n",
      "Epoch [47/50], Loss: 61.3961\n",
      "Epoch [48/50], Loss: 59.4219\n",
      "Epoch [49/50], Loss: 59.8713\n",
      "Epoch [50/50], Loss: 59.2341\n",
      "Validation RMSE: 44.64894880567278\n",
      "Iteration 20: Training NN with optimizer=sgd, hidden_size=128, lr=0.001, batch_size=64, n_epochs=100\n",
      "Epoch [1/100], Loss: 530.3082\n",
      "Epoch [2/100], Loss: 426.8845\n",
      "Epoch [3/100], Loss: 252.7270\n",
      "Epoch [4/100], Loss: 146.0983\n",
      "Epoch [5/100], Loss: 119.5866\n",
      "Epoch [6/100], Loss: 99.3517\n",
      "Epoch [7/100], Loss: 94.8978\n",
      "Epoch [8/100], Loss: 83.6106\n",
      "Epoch [9/100], Loss: 80.8964\n",
      "Epoch [10/100], Loss: 86.5234\n",
      "Epoch [11/100], Loss: 77.1378\n",
      "Epoch [12/100], Loss: 76.6849\n",
      "Epoch [13/100], Loss: 72.3220\n",
      "Epoch [14/100], Loss: 67.7629\n",
      "Epoch [15/100], Loss: 67.1403\n",
      "Epoch [16/100], Loss: 62.6246\n",
      "Epoch [17/100], Loss: 59.0012\n",
      "Epoch [18/100], Loss: 59.2178\n",
      "Epoch [19/100], Loss: 56.2991\n",
      "Epoch [20/100], Loss: 56.6178\n",
      "Epoch [21/100], Loss: 53.6580\n",
      "Epoch [22/100], Loss: 57.2185\n",
      "Epoch [23/100], Loss: 51.7565\n",
      "Epoch [24/100], Loss: 51.8934\n",
      "Epoch [25/100], Loss: 54.2692\n",
      "Epoch [26/100], Loss: 56.1330\n",
      "Epoch [27/100], Loss: 53.5556\n",
      "Epoch [28/100], Loss: 51.7865\n",
      "Epoch [29/100], Loss: 50.0989\n",
      "Epoch [30/100], Loss: 46.3181\n",
      "Epoch [31/100], Loss: 46.7050\n",
      "Epoch [32/100], Loss: 46.5432\n",
      "Epoch [33/100], Loss: 47.2357\n",
      "Epoch [34/100], Loss: 45.4698\n",
      "Epoch [35/100], Loss: 43.5783\n",
      "Epoch [36/100], Loss: 49.1399\n",
      "Epoch [37/100], Loss: 43.2841\n",
      "Epoch [38/100], Loss: 42.8264\n",
      "Epoch [39/100], Loss: 42.3579\n",
      "Epoch [40/100], Loss: 41.9329\n",
      "Epoch [41/100], Loss: 43.8571\n",
      "Epoch [42/100], Loss: 45.8287\n",
      "Epoch [43/100], Loss: 39.9360\n",
      "Epoch [44/100], Loss: 42.1803\n",
      "Epoch [45/100], Loss: 45.1001\n",
      "Epoch [46/100], Loss: 40.6907\n",
      "Epoch [47/100], Loss: 42.1767\n",
      "Epoch [48/100], Loss: 50.9399\n",
      "Epoch [49/100], Loss: 40.8469\n",
      "Epoch [50/100], Loss: 40.4813\n",
      "Epoch [51/100], Loss: 40.7704\n",
      "Epoch [52/100], Loss: 41.1222\n",
      "Epoch [53/100], Loss: 38.7821\n",
      "Epoch [54/100], Loss: 44.7112\n",
      "Epoch [55/100], Loss: 36.8155\n",
      "Epoch [56/100], Loss: 41.9611\n",
      "Epoch [57/100], Loss: 38.5397\n",
      "Epoch [58/100], Loss: 35.8702\n",
      "Epoch [59/100], Loss: 36.4481\n",
      "Epoch [60/100], Loss: 40.6820\n",
      "Epoch [61/100], Loss: 35.6735\n",
      "Epoch [62/100], Loss: 37.1116\n",
      "Epoch [63/100], Loss: 36.1041\n",
      "Epoch [64/100], Loss: 40.4808\n",
      "Epoch [65/100], Loss: 33.3325\n",
      "Epoch [66/100], Loss: 34.8969\n",
      "Epoch [67/100], Loss: 33.1580\n",
      "Epoch [68/100], Loss: 36.8832\n",
      "Epoch [69/100], Loss: 33.1908\n",
      "Epoch [70/100], Loss: 32.7404\n",
      "Epoch [71/100], Loss: 36.8266\n",
      "Epoch [72/100], Loss: 33.9122\n",
      "Epoch [73/100], Loss: 37.4445\n",
      "Epoch [74/100], Loss: 31.7878\n",
      "Epoch [75/100], Loss: 36.6089\n",
      "Epoch [76/100], Loss: 31.5832\n",
      "Epoch [77/100], Loss: 32.3254\n",
      "Epoch [78/100], Loss: 31.0369\n",
      "Epoch [79/100], Loss: 34.3596\n",
      "Epoch [80/100], Loss: 33.4262\n",
      "Epoch [81/100], Loss: 30.8116\n",
      "Epoch [82/100], Loss: 30.9132\n",
      "Epoch [83/100], Loss: 31.7470\n",
      "Epoch [84/100], Loss: 30.4059\n",
      "Epoch [85/100], Loss: 30.5787\n",
      "Epoch [86/100], Loss: 31.3555\n",
      "Epoch [87/100], Loss: 29.9033\n",
      "Epoch [88/100], Loss: 28.6860\n",
      "Epoch [89/100], Loss: 29.9391\n",
      "Epoch [90/100], Loss: 32.9007\n",
      "Epoch [91/100], Loss: 29.2140\n",
      "Epoch [92/100], Loss: 31.2754\n",
      "Epoch [93/100], Loss: 27.6123\n",
      "Epoch [94/100], Loss: 27.9823\n",
      "Epoch [95/100], Loss: 29.1007\n",
      "Epoch [96/100], Loss: 29.4962\n",
      "Epoch [97/100], Loss: 27.2493\n",
      "Epoch [98/100], Loss: 30.8952\n",
      "Epoch [99/100], Loss: 29.3380\n",
      "Epoch [100/100], Loss: 28.4896\n",
      "Validation RMSE: 16.62361240386963\n",
      "Iteration 21: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=64, n_epochs=50\n",
      "Epoch [1/50], Loss: 595.2828\n",
      "Epoch [2/50], Loss: 532.1582\n",
      "Epoch [3/50], Loss: 423.7693\n",
      "Epoch [4/50], Loss: 285.3811\n",
      "Epoch [5/50], Loss: 160.5195\n",
      "Epoch [6/50], Loss: 113.5174\n",
      "Epoch [7/50], Loss: 100.8488\n",
      "Epoch [8/50], Loss: 103.7049\n",
      "Epoch [9/50], Loss: 88.7365\n",
      "Epoch [10/50], Loss: 87.0571\n",
      "Epoch [11/50], Loss: 82.0386\n",
      "Epoch [12/50], Loss: 78.6744\n",
      "Epoch [13/50], Loss: 75.4298\n",
      "Epoch [14/50], Loss: 75.9998\n",
      "Epoch [15/50], Loss: 69.8731\n",
      "Epoch [16/50], Loss: 63.7254\n",
      "Epoch [17/50], Loss: 68.3273\n",
      "Epoch [18/50], Loss: 64.0009\n",
      "Epoch [19/50], Loss: 65.8199\n",
      "Epoch [20/50], Loss: 57.9219\n",
      "Epoch [21/50], Loss: 58.9777\n",
      "Epoch [22/50], Loss: 59.7045\n",
      "Epoch [23/50], Loss: 54.3526\n",
      "Epoch [24/50], Loss: 56.9531\n",
      "Epoch [25/50], Loss: 55.7762\n",
      "Epoch [26/50], Loss: 53.4750\n",
      "Epoch [27/50], Loss: 57.6896\n",
      "Epoch [28/50], Loss: 52.0554\n",
      "Epoch [29/50], Loss: 48.4649\n",
      "Epoch [30/50], Loss: 55.7642\n",
      "Epoch [31/50], Loss: 47.5958\n",
      "Epoch [32/50], Loss: 51.5635\n",
      "Epoch [33/50], Loss: 47.0188\n",
      "Epoch [34/50], Loss: 46.5371\n",
      "Epoch [35/50], Loss: 47.3810\n",
      "Epoch [36/50], Loss: 49.5719\n",
      "Epoch [37/50], Loss: 46.7267\n",
      "Epoch [38/50], Loss: 43.5036\n",
      "Epoch [39/50], Loss: 45.9103\n",
      "Epoch [40/50], Loss: 44.8914\n",
      "Epoch [41/50], Loss: 42.4598\n",
      "Epoch [42/50], Loss: 48.2058\n",
      "Epoch [43/50], Loss: 47.2219\n",
      "Epoch [44/50], Loss: 43.2657\n",
      "Epoch [45/50], Loss: 40.4522\n",
      "Epoch [46/50], Loss: 42.5232\n",
      "Epoch [47/50], Loss: 42.1760\n",
      "Epoch [48/50], Loss: 40.0545\n",
      "Epoch [49/50], Loss: 41.7307\n",
      "Epoch [50/50], Loss: 45.9655\n",
      "Validation RMSE: 27.681320190429688\n",
      "Iteration 22: Training NN with optimizer=sgd, hidden_size=128, lr=0.0001, batch_size=16, n_epochs=50\n",
      "Epoch [1/50], Loss: 576.1090\n",
      "Epoch [2/50], Loss: 548.0121\n",
      "Epoch [3/50], Loss: 505.0258\n",
      "Epoch [4/50], Loss: 459.2320\n",
      "Epoch [5/50], Loss: 400.9407\n",
      "Epoch [6/50], Loss: 343.0275\n",
      "Epoch [7/50], Loss: 287.6353\n",
      "Epoch [8/50], Loss: 238.6098\n",
      "Epoch [9/50], Loss: 188.9887\n",
      "Epoch [10/50], Loss: 158.2197\n",
      "Epoch [11/50], Loss: 142.6686\n",
      "Epoch [12/50], Loss: 127.9287\n",
      "Epoch [13/50], Loss: 117.3839\n",
      "Epoch [14/50], Loss: 112.0971\n",
      "Epoch [15/50], Loss: 107.7851\n",
      "Epoch [16/50], Loss: 109.5091\n",
      "Epoch [17/50], Loss: 102.6250\n",
      "Epoch [18/50], Loss: 101.4242\n",
      "Epoch [19/50], Loss: 98.3845\n",
      "Epoch [20/50], Loss: 95.2517\n",
      "Epoch [21/50], Loss: 95.7451\n",
      "Epoch [22/50], Loss: 91.6985\n",
      "Epoch [23/50], Loss: 89.8789\n",
      "Epoch [24/50], Loss: 87.9794\n",
      "Epoch [25/50], Loss: 87.5145\n",
      "Epoch [26/50], Loss: 85.7347\n",
      "Epoch [27/50], Loss: 85.8887\n",
      "Epoch [28/50], Loss: 86.2547\n",
      "Epoch [29/50], Loss: 82.3456\n",
      "Epoch [30/50], Loss: 80.5055\n",
      "Epoch [31/50], Loss: 77.9271\n",
      "Epoch [32/50], Loss: 76.4453\n",
      "Epoch [33/50], Loss: 78.0867\n",
      "Epoch [34/50], Loss: 75.9963\n",
      "Epoch [35/50], Loss: 72.6015\n",
      "Epoch [36/50], Loss: 71.0921\n",
      "Epoch [37/50], Loss: 71.5716\n",
      "Epoch [38/50], Loss: 69.7875\n",
      "Epoch [39/50], Loss: 74.3738\n",
      "Epoch [40/50], Loss: 67.5480\n",
      "Epoch [41/50], Loss: 66.7131\n",
      "Epoch [42/50], Loss: 65.9564\n",
      "Epoch [43/50], Loss: 64.2104\n",
      "Epoch [44/50], Loss: 68.6835\n",
      "Epoch [45/50], Loss: 73.6030\n",
      "Epoch [46/50], Loss: 61.8527\n",
      "Epoch [47/50], Loss: 61.9895\n",
      "Epoch [48/50], Loss: 61.8973\n",
      "Epoch [49/50], Loss: 60.7094\n",
      "Epoch [50/50], Loss: 59.2357\n",
      "Validation RMSE: 47.29557228088379\n",
      "Iteration 23: Training NN with optimizer=sgd, hidden_size=128, lr=0.0001, batch_size=16, n_epochs=100\n",
      "Epoch [1/100], Loss: 577.0836\n",
      "Epoch [2/100], Loss: 544.0445\n",
      "Epoch [3/100], Loss: 492.4387\n",
      "Epoch [4/100], Loss: 452.3509\n",
      "Epoch [5/100], Loss: 382.8452\n",
      "Epoch [6/100], Loss: 331.6427\n",
      "Epoch [7/100], Loss: 268.5459\n",
      "Epoch [8/100], Loss: 220.9909\n",
      "Epoch [9/100], Loss: 169.8802\n",
      "Epoch [10/100], Loss: 143.9293\n",
      "Epoch [11/100], Loss: 123.8767\n",
      "Epoch [12/100], Loss: 117.5765\n",
      "Epoch [13/100], Loss: 106.7492\n",
      "Epoch [14/100], Loss: 102.1081\n",
      "Epoch [15/100], Loss: 100.2076\n",
      "Epoch [16/100], Loss: 96.8891\n",
      "Epoch [17/100], Loss: 94.2050\n",
      "Epoch [18/100], Loss: 93.2065\n",
      "Epoch [19/100], Loss: 90.5876\n",
      "Epoch [20/100], Loss: 89.6503\n",
      "Epoch [21/100], Loss: 88.0878\n",
      "Epoch [22/100], Loss: 86.8068\n",
      "Epoch [23/100], Loss: 89.0702\n",
      "Epoch [24/100], Loss: 84.3361\n",
      "Epoch [25/100], Loss: 86.7547\n",
      "Epoch [26/100], Loss: 80.9020\n",
      "Epoch [27/100], Loss: 78.4675\n",
      "Epoch [28/100], Loss: 77.2739\n",
      "Epoch [29/100], Loss: 75.9649\n",
      "Epoch [30/100], Loss: 74.3825\n",
      "Epoch [31/100], Loss: 75.0353\n",
      "Epoch [32/100], Loss: 72.7187\n",
      "Epoch [33/100], Loss: 71.6888\n",
      "Epoch [34/100], Loss: 72.8158\n",
      "Epoch [35/100], Loss: 69.4344\n",
      "Epoch [36/100], Loss: 69.1539\n",
      "Epoch [37/100], Loss: 68.8529\n",
      "Epoch [38/100], Loss: 70.3825\n",
      "Epoch [39/100], Loss: 65.5784\n",
      "Epoch [40/100], Loss: 68.8442\n",
      "Epoch [41/100], Loss: 68.5414\n",
      "Epoch [42/100], Loss: 67.9541\n",
      "Epoch [43/100], Loss: 61.9922\n",
      "Epoch [44/100], Loss: 65.8058\n",
      "Epoch [45/100], Loss: 61.5123\n",
      "Epoch [46/100], Loss: 62.0280\n",
      "Epoch [47/100], Loss: 60.5891\n",
      "Epoch [48/100], Loss: 60.9032\n",
      "Epoch [49/100], Loss: 59.4944\n",
      "Epoch [50/100], Loss: 57.6851\n",
      "Epoch [51/100], Loss: 57.3078\n",
      "Epoch [52/100], Loss: 57.6269\n",
      "Epoch [53/100], Loss: 56.7754\n",
      "Epoch [54/100], Loss: 55.9107\n",
      "Epoch [55/100], Loss: 59.6443\n",
      "Epoch [56/100], Loss: 57.5640\n",
      "Epoch [57/100], Loss: 54.4385\n",
      "Epoch [58/100], Loss: 55.9740\n",
      "Epoch [59/100], Loss: 54.8197\n",
      "Epoch [60/100], Loss: 56.0229\n",
      "Epoch [61/100], Loss: 53.0493\n",
      "Epoch [62/100], Loss: 53.6831\n",
      "Epoch [63/100], Loss: 53.6512\n",
      "Epoch [64/100], Loss: 51.8827\n",
      "Epoch [65/100], Loss: 52.2370\n",
      "Epoch [66/100], Loss: 54.3679\n",
      "Epoch [67/100], Loss: 57.7882\n",
      "Epoch [68/100], Loss: 53.8670\n",
      "Epoch [69/100], Loss: 50.5483\n",
      "Epoch [70/100], Loss: 51.4578\n",
      "Epoch [71/100], Loss: 50.1949\n",
      "Epoch [72/100], Loss: 49.9308\n",
      "Epoch [73/100], Loss: 49.1842\n",
      "Epoch [74/100], Loss: 50.3152\n",
      "Epoch [75/100], Loss: 48.7521\n",
      "Epoch [76/100], Loss: 49.1132\n",
      "Epoch [77/100], Loss: 48.7854\n",
      "Epoch [78/100], Loss: 48.2375\n",
      "Epoch [79/100], Loss: 55.9712\n",
      "Epoch [80/100], Loss: 49.0482\n",
      "Epoch [81/100], Loss: 47.4506\n",
      "Epoch [82/100], Loss: 47.5029\n",
      "Epoch [83/100], Loss: 47.7927\n",
      "Epoch [84/100], Loss: 46.9655\n",
      "Epoch [85/100], Loss: 46.9607\n",
      "Epoch [86/100], Loss: 47.5234\n",
      "Epoch [87/100], Loss: 46.8874\n",
      "Epoch [88/100], Loss: 46.2074\n",
      "Epoch [89/100], Loss: 48.9520\n",
      "Epoch [90/100], Loss: 49.4445\n",
      "Epoch [91/100], Loss: 45.5439\n",
      "Epoch [92/100], Loss: 45.9714\n",
      "Epoch [93/100], Loss: 45.3040\n",
      "Epoch [94/100], Loss: 45.2267\n",
      "Epoch [95/100], Loss: 45.1449\n",
      "Epoch [96/100], Loss: 44.5133\n",
      "Epoch [97/100], Loss: 44.5420\n",
      "Epoch [98/100], Loss: 48.3405\n",
      "Epoch [99/100], Loss: 44.0292\n",
      "Epoch [100/100], Loss: 44.1337\n",
      "Validation RMSE: 30.124846935272217\n",
      "Iteration 24: Training NN with optimizer=sgd, hidden_size=64, lr=0.0001, batch_size=32, n_epochs=100\n",
      "Epoch [1/100], Loss: 583.9609\n",
      "Epoch [2/100], Loss: 571.3808\n",
      "Epoch [3/100], Loss: 565.1170\n",
      "Epoch [4/100], Loss: 540.4595\n",
      "Epoch [5/100], Loss: 531.2330\n",
      "Epoch [6/100], Loss: 517.0499\n",
      "Epoch [7/100], Loss: 506.0136\n",
      "Epoch [8/100], Loss: 485.8714\n",
      "Epoch [9/100], Loss: 457.1037\n",
      "Epoch [10/100], Loss: 428.9724\n",
      "Epoch [11/100], Loss: 410.2236\n",
      "Epoch [12/100], Loss: 383.3179\n",
      "Epoch [13/100], Loss: 348.2941\n",
      "Epoch [14/100], Loss: 320.1794\n",
      "Epoch [15/100], Loss: 292.1733\n",
      "Epoch [16/100], Loss: 267.3968\n",
      "Epoch [17/100], Loss: 237.0797\n",
      "Epoch [18/100], Loss: 211.5354\n",
      "Epoch [19/100], Loss: 191.4027\n",
      "Epoch [20/100], Loss: 171.4552\n",
      "Epoch [21/100], Loss: 157.1224\n",
      "Epoch [22/100], Loss: 140.9478\n",
      "Epoch [23/100], Loss: 132.0463\n",
      "Epoch [24/100], Loss: 124.9260\n",
      "Epoch [25/100], Loss: 117.8280\n",
      "Epoch [26/100], Loss: 115.2057\n",
      "Epoch [27/100], Loss: 111.3749\n",
      "Epoch [28/100], Loss: 106.4333\n",
      "Epoch [29/100], Loss: 106.9686\n",
      "Epoch [30/100], Loss: 102.6673\n",
      "Epoch [31/100], Loss: 102.6554\n",
      "Epoch [32/100], Loss: 102.3287\n",
      "Epoch [33/100], Loss: 98.7918\n",
      "Epoch [34/100], Loss: 96.9950\n",
      "Epoch [35/100], Loss: 96.7298\n",
      "Epoch [36/100], Loss: 95.3858\n",
      "Epoch [37/100], Loss: 94.2605\n",
      "Epoch [38/100], Loss: 92.8155\n",
      "Epoch [39/100], Loss: 94.4921\n",
      "Epoch [40/100], Loss: 92.1509\n",
      "Epoch [41/100], Loss: 91.4605\n",
      "Epoch [42/100], Loss: 90.5992\n",
      "Epoch [43/100], Loss: 89.5210\n",
      "Epoch [44/100], Loss: 88.0755\n",
      "Epoch [45/100], Loss: 88.6930\n",
      "Epoch [46/100], Loss: 86.6462\n",
      "Epoch [47/100], Loss: 87.7682\n",
      "Epoch [48/100], Loss: 86.2472\n",
      "Epoch [49/100], Loss: 84.0818\n",
      "Epoch [50/100], Loss: 86.6723\n",
      "Epoch [51/100], Loss: 83.3365\n",
      "Epoch [52/100], Loss: 84.0473\n",
      "Epoch [53/100], Loss: 82.4765\n",
      "Epoch [54/100], Loss: 81.8561\n",
      "Epoch [55/100], Loss: 80.9926\n",
      "Epoch [56/100], Loss: 79.1366\n",
      "Epoch [57/100], Loss: 79.1064\n",
      "Epoch [58/100], Loss: 79.1216\n",
      "Epoch [59/100], Loss: 78.8953\n",
      "Epoch [60/100], Loss: 76.6320\n",
      "Epoch [61/100], Loss: 77.7799\n",
      "Epoch [62/100], Loss: 75.4680\n",
      "Epoch [63/100], Loss: 76.9057\n",
      "Epoch [64/100], Loss: 74.4788\n",
      "Epoch [65/100], Loss: 73.9368\n",
      "Epoch [66/100], Loss: 73.9050\n",
      "Epoch [67/100], Loss: 73.8524\n",
      "Epoch [68/100], Loss: 73.4879\n",
      "Epoch [69/100], Loss: 73.1462\n",
      "Epoch [70/100], Loss: 71.4666\n",
      "Epoch [71/100], Loss: 70.6790\n",
      "Epoch [72/100], Loss: 72.6836\n",
      "Epoch [73/100], Loss: 70.5693\n",
      "Epoch [74/100], Loss: 71.1129\n",
      "Epoch [75/100], Loss: 70.4832\n",
      "Epoch [76/100], Loss: 70.4408\n",
      "Epoch [77/100], Loss: 69.6201\n",
      "Epoch [78/100], Loss: 69.6126\n",
      "Epoch [79/100], Loss: 68.9489\n",
      "Epoch [80/100], Loss: 68.6160\n",
      "Epoch [81/100], Loss: 67.0793\n",
      "Epoch [82/100], Loss: 66.1724\n",
      "Epoch [83/100], Loss: 66.0202\n",
      "Epoch [84/100], Loss: 67.2364\n",
      "Epoch [85/100], Loss: 64.5054\n",
      "Epoch [86/100], Loss: 64.3549\n",
      "Epoch [87/100], Loss: 65.7143\n",
      "Epoch [88/100], Loss: 64.2155\n",
      "Epoch [89/100], Loss: 63.6759\n",
      "Epoch [90/100], Loss: 62.9689\n",
      "Epoch [91/100], Loss: 63.0786\n",
      "Epoch [92/100], Loss: 62.0325\n",
      "Epoch [93/100], Loss: 63.8230\n",
      "Epoch [94/100], Loss: 61.5260\n",
      "Epoch [95/100], Loss: 62.3147\n",
      "Epoch [96/100], Loss: 63.1930\n",
      "Epoch [97/100], Loss: 61.3238\n",
      "Epoch [98/100], Loss: 60.8339\n",
      "Epoch [99/100], Loss: 60.6666\n",
      "Epoch [100/100], Loss: 59.8768\n",
      "Validation RMSE: 43.11773443222046\n",
      "Iteration 25: Training NN with optimizer=sgd, hidden_size=32, lr=0.0001, batch_size=64, n_epochs=100\n",
      "Epoch [1/100], Loss: 589.5302\n",
      "Epoch [2/100], Loss: 580.3279\n",
      "Epoch [3/100], Loss: 587.4107\n",
      "Epoch [4/100], Loss: 567.6898\n",
      "Epoch [5/100], Loss: 577.0986\n",
      "Epoch [6/100], Loss: 580.7552\n",
      "Epoch [7/100], Loss: 588.2581\n",
      "Epoch [8/100], Loss: 553.7996\n",
      "Epoch [9/100], Loss: 572.2582\n",
      "Epoch [10/100], Loss: 563.9360\n",
      "Epoch [11/100], Loss: 547.5377\n",
      "Epoch [12/100], Loss: 554.2724\n",
      "Epoch [13/100], Loss: 543.2658\n",
      "Epoch [14/100], Loss: 523.2268\n",
      "Epoch [15/100], Loss: 517.9467\n",
      "Epoch [16/100], Loss: 508.7075\n",
      "Epoch [17/100], Loss: 499.5723\n",
      "Epoch [18/100], Loss: 485.2385\n",
      "Epoch [19/100], Loss: 480.8563\n",
      "Epoch [20/100], Loss: 467.0216\n",
      "Epoch [21/100], Loss: 452.4774\n",
      "Epoch [22/100], Loss: 443.5542\n",
      "Epoch [23/100], Loss: 445.0552\n",
      "Epoch [24/100], Loss: 414.8203\n",
      "Epoch [25/100], Loss: 431.4512\n",
      "Epoch [26/100], Loss: 426.8687\n",
      "Epoch [27/100], Loss: 383.1131\n",
      "Epoch [28/100], Loss: 386.7961\n",
      "Epoch [29/100], Loss: 366.9271\n",
      "Epoch [30/100], Loss: 354.4573\n",
      "Epoch [31/100], Loss: 333.3240\n",
      "Epoch [32/100], Loss: 302.9443\n",
      "Epoch [33/100], Loss: 285.3541\n",
      "Epoch [34/100], Loss: 284.5521\n",
      "Epoch [35/100], Loss: 255.3262\n",
      "Epoch [36/100], Loss: 254.2946\n",
      "Epoch [37/100], Loss: 233.2397\n",
      "Epoch [38/100], Loss: 229.9292\n",
      "Epoch [39/100], Loss: 206.5407\n",
      "Epoch [40/100], Loss: 200.0524\n",
      "Epoch [41/100], Loss: 187.6004\n",
      "Epoch [42/100], Loss: 173.0919\n",
      "Epoch [43/100], Loss: 162.7704\n",
      "Epoch [44/100], Loss: 159.6162\n",
      "Epoch [45/100], Loss: 151.8072\n",
      "Epoch [46/100], Loss: 141.5744\n",
      "Epoch [47/100], Loss: 145.2804\n",
      "Epoch [48/100], Loss: 140.3740\n",
      "Epoch [49/100], Loss: 129.4177\n",
      "Epoch [50/100], Loss: 133.7511\n",
      "Epoch [51/100], Loss: 134.0337\n",
      "Epoch [52/100], Loss: 120.2884\n",
      "Epoch [53/100], Loss: 120.9530\n",
      "Epoch [54/100], Loss: 110.7748\n",
      "Epoch [55/100], Loss: 123.1410\n",
      "Epoch [56/100], Loss: 107.9135\n",
      "Epoch [57/100], Loss: 108.1969\n",
      "Epoch [58/100], Loss: 114.9469\n",
      "Epoch [59/100], Loss: 101.5023\n",
      "Epoch [60/100], Loss: 112.3469\n",
      "Epoch [61/100], Loss: 102.8019\n",
      "Epoch [62/100], Loss: 102.4558\n",
      "Epoch [63/100], Loss: 104.7574\n",
      "Epoch [64/100], Loss: 103.7061\n",
      "Epoch [65/100], Loss: 103.7011\n",
      "Epoch [66/100], Loss: 98.5418\n",
      "Epoch [67/100], Loss: 98.8764\n",
      "Epoch [68/100], Loss: 101.5299\n",
      "Epoch [69/100], Loss: 99.3371\n",
      "Epoch [70/100], Loss: 101.9957\n",
      "Epoch [71/100], Loss: 99.6753\n",
      "Epoch [72/100], Loss: 103.9723\n",
      "Epoch [73/100], Loss: 99.5898\n",
      "Epoch [74/100], Loss: 100.5996\n",
      "Epoch [75/100], Loss: 98.0718\n",
      "Epoch [76/100], Loss: 94.8894\n",
      "Epoch [77/100], Loss: 95.0330\n",
      "Epoch [78/100], Loss: 104.9511\n",
      "Epoch [79/100], Loss: 95.4251\n",
      "Epoch [80/100], Loss: 91.4285\n",
      "Epoch [81/100], Loss: 104.3678\n",
      "Epoch [82/100], Loss: 95.2105\n",
      "Epoch [83/100], Loss: 91.3107\n",
      "Epoch [84/100], Loss: 91.7539\n",
      "Epoch [85/100], Loss: 90.9029\n",
      "Epoch [86/100], Loss: 87.1252\n",
      "Epoch [87/100], Loss: 97.3386\n",
      "Epoch [88/100], Loss: 96.3530\n",
      "Epoch [89/100], Loss: 91.3505\n",
      "Epoch [90/100], Loss: 92.7953\n",
      "Epoch [91/100], Loss: 93.6418\n",
      "Epoch [92/100], Loss: 93.6866\n",
      "Epoch [93/100], Loss: 93.3759\n",
      "Epoch [94/100], Loss: 89.8247\n",
      "Epoch [95/100], Loss: 87.0095\n",
      "Epoch [96/100], Loss: 88.6142\n",
      "Epoch [97/100], Loss: 85.2323\n",
      "Epoch [98/100], Loss: 82.9787\n",
      "Epoch [99/100], Loss: 89.1511\n",
      "Epoch [100/100], Loss: 88.7185\n",
      "Validation RMSE: 73.80392456054688\n",
      "Iteration 26: Training NN with optimizer=sgd, hidden_size=64, lr=0.001, batch_size=64, n_epochs=150\n",
      "Epoch [1/150], Loss: 578.3192\n",
      "Epoch [2/150], Loss: 476.0584\n",
      "Epoch [3/150], Loss: 375.4507\n",
      "Epoch [4/150], Loss: 221.7714\n",
      "Epoch [5/150], Loss: 136.8553\n",
      "Epoch [6/150], Loss: 107.6994\n",
      "Epoch [7/150], Loss: 96.9868\n",
      "Epoch [8/150], Loss: 95.3692\n",
      "Epoch [9/150], Loss: 90.3474\n",
      "Epoch [10/150], Loss: 83.6347\n",
      "Epoch [11/150], Loss: 77.5862\n",
      "Epoch [12/150], Loss: 79.1275\n",
      "Epoch [13/150], Loss: 73.9357\n",
      "Epoch [14/150], Loss: 71.9850\n",
      "Epoch [15/150], Loss: 67.0575\n",
      "Epoch [16/150], Loss: 65.4245\n",
      "Epoch [17/150], Loss: 60.3404\n",
      "Epoch [18/150], Loss: 62.9418\n",
      "Epoch [19/150], Loss: 64.6069\n",
      "Epoch [20/150], Loss: 57.5940\n",
      "Epoch [21/150], Loss: 57.3267\n",
      "Epoch [22/150], Loss: 53.4895\n",
      "Epoch [23/150], Loss: 56.5919\n",
      "Epoch [24/150], Loss: 53.3879\n",
      "Epoch [25/150], Loss: 52.3302\n",
      "Epoch [26/150], Loss: 54.0974\n",
      "Epoch [27/150], Loss: 50.3394\n",
      "Epoch [28/150], Loss: 47.9445\n",
      "Epoch [29/150], Loss: 47.5771\n",
      "Epoch [30/150], Loss: 50.2799\n",
      "Epoch [31/150], Loss: 50.3808\n",
      "Epoch [32/150], Loss: 45.6758\n",
      "Epoch [33/150], Loss: 44.9605\n",
      "Epoch [34/150], Loss: 47.3212\n",
      "Epoch [35/150], Loss: 51.7123\n",
      "Epoch [36/150], Loss: 47.5995\n",
      "Epoch [37/150], Loss: 43.7165\n",
      "Epoch [38/150], Loss: 47.7771\n",
      "Epoch [39/150], Loss: 45.0376\n",
      "Epoch [40/150], Loss: 42.5214\n",
      "Epoch [41/150], Loss: 41.1274\n",
      "Epoch [42/150], Loss: 46.2517\n",
      "Epoch [43/150], Loss: 44.7784\n",
      "Epoch [44/150], Loss: 41.3253\n",
      "Epoch [45/150], Loss: 47.7755\n",
      "Epoch [46/150], Loss: 40.4962\n",
      "Epoch [47/150], Loss: 39.3793\n",
      "Epoch [48/150], Loss: 44.6438\n",
      "Epoch [49/150], Loss: 40.8782\n",
      "Epoch [50/150], Loss: 38.1589\n",
      "Epoch [51/150], Loss: 37.5482\n",
      "Epoch [52/150], Loss: 38.5574\n",
      "Epoch [53/150], Loss: 37.1141\n",
      "Epoch [54/150], Loss: 36.9859\n",
      "Epoch [55/150], Loss: 41.8118\n",
      "Epoch [56/150], Loss: 36.4994\n",
      "Epoch [57/150], Loss: 38.3555\n",
      "Epoch [58/150], Loss: 37.2240\n",
      "Epoch [59/150], Loss: 36.7725\n",
      "Epoch [60/150], Loss: 40.2613\n",
      "Epoch [61/150], Loss: 37.7567\n",
      "Epoch [62/150], Loss: 36.8377\n",
      "Epoch [63/150], Loss: 35.3945\n",
      "Epoch [64/150], Loss: 34.0745\n",
      "Epoch [65/150], Loss: 36.4741\n",
      "Epoch [66/150], Loss: 36.0244\n",
      "Epoch [67/150], Loss: 33.6760\n",
      "Epoch [68/150], Loss: 37.3369\n",
      "Epoch [69/150], Loss: 33.3455\n",
      "Epoch [70/150], Loss: 35.1837\n",
      "Epoch [71/150], Loss: 32.3701\n",
      "Epoch [72/150], Loss: 32.6313\n",
      "Epoch [73/150], Loss: 33.3614\n",
      "Epoch [74/150], Loss: 32.5559\n",
      "Epoch [75/150], Loss: 34.5930\n",
      "Epoch [76/150], Loss: 31.9799\n",
      "Epoch [77/150], Loss: 32.7662\n",
      "Epoch [78/150], Loss: 30.2602\n",
      "Epoch [79/150], Loss: 30.5547\n",
      "Epoch [80/150], Loss: 30.2926\n",
      "Epoch [81/150], Loss: 32.6519\n",
      "Epoch [82/150], Loss: 31.0545\n",
      "Epoch [83/150], Loss: 29.7823\n",
      "Epoch [84/150], Loss: 29.9863\n",
      "Epoch [85/150], Loss: 32.0978\n",
      "Epoch [86/150], Loss: 29.7658\n",
      "Epoch [87/150], Loss: 29.4077\n",
      "Epoch [88/150], Loss: 29.1408\n",
      "Epoch [89/150], Loss: 28.9794\n",
      "Epoch [90/150], Loss: 28.4302\n",
      "Epoch [91/150], Loss: 33.5424\n",
      "Epoch [92/150], Loss: 29.2440\n",
      "Epoch [93/150], Loss: 28.4922\n",
      "Epoch [94/150], Loss: 29.7264\n",
      "Epoch [95/150], Loss: 31.2924\n",
      "Epoch [96/150], Loss: 29.0748\n",
      "Epoch [97/150], Loss: 27.3179\n",
      "Epoch [98/150], Loss: 27.5229\n",
      "Epoch [99/150], Loss: 27.2962\n",
      "Epoch [100/150], Loss: 28.6850\n",
      "Epoch [101/150], Loss: 27.0752\n",
      "Epoch [102/150], Loss: 26.3058\n",
      "Epoch [103/150], Loss: 26.7551\n",
      "Epoch [104/150], Loss: 27.9259\n",
      "Epoch [105/150], Loss: 27.8690\n",
      "Epoch [106/150], Loss: 26.1905\n",
      "Epoch [107/150], Loss: 29.2007\n",
      "Epoch [108/150], Loss: 26.6093\n",
      "Epoch [109/150], Loss: 27.7547\n",
      "Epoch [110/150], Loss: 26.7859\n",
      "Epoch [111/150], Loss: 26.8195\n",
      "Epoch [112/150], Loss: 26.1406\n",
      "Epoch [113/150], Loss: 26.4169\n",
      "Epoch [114/150], Loss: 25.5417\n",
      "Epoch [115/150], Loss: 30.1405\n",
      "Epoch [116/150], Loss: 25.7225\n",
      "Epoch [117/150], Loss: 25.4431\n",
      "Epoch [118/150], Loss: 24.6502\n",
      "Epoch [119/150], Loss: 27.3694\n",
      "Epoch [120/150], Loss: 25.6464\n",
      "Epoch [121/150], Loss: 25.7950\n",
      "Epoch [122/150], Loss: 26.4797\n",
      "Epoch [123/150], Loss: 26.2592\n",
      "Epoch [124/150], Loss: 27.8331\n",
      "Epoch [125/150], Loss: 24.5865\n",
      "Epoch [126/150], Loss: 26.1542\n",
      "Epoch [127/150], Loss: 26.4493\n",
      "Epoch [128/150], Loss: 26.4833\n",
      "Epoch [129/150], Loss: 23.7240\n",
      "Epoch [130/150], Loss: 24.3626\n",
      "Epoch [131/150], Loss: 23.4378\n",
      "Epoch [132/150], Loss: 24.4474\n",
      "Epoch [133/150], Loss: 24.3058\n",
      "Epoch [134/150], Loss: 25.2898\n",
      "Epoch [135/150], Loss: 25.8507\n",
      "Epoch [136/150], Loss: 24.1346\n",
      "Epoch [137/150], Loss: 24.5487\n",
      "Epoch [138/150], Loss: 24.6118\n",
      "Epoch [139/150], Loss: 25.3460\n",
      "Epoch [140/150], Loss: 27.0594\n",
      "Epoch [141/150], Loss: 23.0383\n",
      "Epoch [142/150], Loss: 23.8802\n",
      "Epoch [143/150], Loss: 25.2877\n",
      "Epoch [144/150], Loss: 24.9951\n",
      "Epoch [145/150], Loss: 30.2028\n",
      "Epoch [146/150], Loss: 24.9693\n",
      "Epoch [147/150], Loss: 22.8781\n",
      "Epoch [148/150], Loss: 23.0770\n",
      "Epoch [149/150], Loss: 32.0011\n",
      "Epoch [150/150], Loss: 22.9599\n",
      "Validation RMSE: 13.965038299560547\n",
      "Iteration 27: Training NN with optimizer=sgd, hidden_size=64, lr=0.001, batch_size=16, n_epochs=50\n",
      "Epoch [1/50], Loss: 390.0748\n",
      "Epoch [2/50], Loss: 116.4981\n",
      "Epoch [3/50], Loss: 89.2808\n",
      "Epoch [4/50], Loss: 77.8256\n",
      "Epoch [5/50], Loss: 66.4517\n",
      "Epoch [6/50], Loss: 64.2671\n",
      "Epoch [7/50], Loss: 54.9839\n",
      "Epoch [8/50], Loss: 51.8743\n",
      "Epoch [9/50], Loss: 51.8231\n",
      "Epoch [10/50], Loss: 48.3399\n",
      "Epoch [11/50], Loss: 49.7369\n",
      "Epoch [12/50], Loss: 44.0159\n",
      "Epoch [13/50], Loss: 42.7699\n",
      "Epoch [14/50], Loss: 41.7811\n",
      "Epoch [15/50], Loss: 39.5276\n",
      "Epoch [16/50], Loss: 39.2235\n",
      "Epoch [17/50], Loss: 38.1764\n",
      "Epoch [18/50], Loss: 36.4591\n",
      "Epoch [19/50], Loss: 38.3703\n",
      "Epoch [20/50], Loss: 34.6570\n",
      "Epoch [21/50], Loss: 33.6209\n",
      "Epoch [22/50], Loss: 33.2044\n",
      "Epoch [23/50], Loss: 31.9659\n",
      "Epoch [24/50], Loss: 30.8930\n",
      "Epoch [25/50], Loss: 30.3483\n",
      "Epoch [26/50], Loss: 29.6818\n",
      "Epoch [27/50], Loss: 29.2469\n",
      "Epoch [28/50], Loss: 29.5219\n",
      "Epoch [29/50], Loss: 29.8349\n",
      "Epoch [30/50], Loss: 28.0278\n",
      "Epoch [31/50], Loss: 34.7809\n",
      "Epoch [32/50], Loss: 28.6381\n",
      "Epoch [33/50], Loss: 26.7335\n",
      "Epoch [34/50], Loss: 26.1605\n",
      "Epoch [35/50], Loss: 27.8296\n",
      "Epoch [36/50], Loss: 25.4774\n",
      "Epoch [37/50], Loss: 25.6006\n",
      "Epoch [38/50], Loss: 24.6989\n",
      "Epoch [39/50], Loss: 24.3332\n",
      "Epoch [40/50], Loss: 25.0312\n",
      "Epoch [41/50], Loss: 24.8346\n",
      "Epoch [42/50], Loss: 23.6919\n",
      "Epoch [43/50], Loss: 23.7730\n",
      "Epoch [44/50], Loss: 23.9173\n",
      "Epoch [45/50], Loss: 23.5179\n",
      "Epoch [46/50], Loss: 23.5939\n",
      "Epoch [47/50], Loss: 23.0741\n",
      "Epoch [48/50], Loss: 22.8596\n",
      "Epoch [49/50], Loss: 22.9574\n",
      "Epoch [50/50], Loss: 22.6641\n",
      "Validation RMSE: 12.79346193586077\n",
      "Iteration 28: Training NN with optimizer=sgd, hidden_size=128, lr=0.01, batch_size=16, n_epochs=100\n",
      "Epoch [1/100], Loss: 124.0086\n",
      "Epoch [2/100], Loss: 100.2006\n",
      "Epoch [3/100], Loss: 54.3215\n",
      "Epoch [4/100], Loss: 48.1278\n",
      "Epoch [5/100], Loss: 43.2993\n",
      "Epoch [6/100], Loss: 31.0860\n",
      "Epoch [7/100], Loss: 37.0145\n",
      "Epoch [8/100], Loss: 32.8157\n",
      "Epoch [9/100], Loss: 32.2325\n",
      "Epoch [10/100], Loss: 26.3722\n",
      "Epoch [11/100], Loss: 36.2580\n",
      "Epoch [12/100], Loss: 28.1241\n",
      "Epoch [13/100], Loss: 23.2782\n",
      "Epoch [14/100], Loss: 24.4411\n",
      "Epoch [15/100], Loss: 27.2204\n",
      "Epoch [16/100], Loss: 31.2358\n",
      "Epoch [17/100], Loss: 28.5540\n",
      "Epoch [18/100], Loss: 32.4893\n",
      "Epoch [19/100], Loss: 21.5869\n",
      "Epoch [20/100], Loss: 26.1584\n",
      "Epoch [21/100], Loss: 26.9192\n",
      "Epoch [22/100], Loss: 20.7441\n",
      "Epoch [23/100], Loss: 22.4244\n",
      "Epoch [24/100], Loss: 20.7205\n",
      "Epoch [25/100], Loss: 21.3932\n",
      "Epoch [26/100], Loss: 23.4221\n",
      "Epoch [27/100], Loss: 28.2280\n",
      "Epoch [28/100], Loss: 25.7313\n",
      "Epoch [29/100], Loss: 20.2322\n",
      "Epoch [30/100], Loss: 17.8354\n",
      "Epoch [31/100], Loss: 22.7129\n",
      "Epoch [32/100], Loss: 19.2798\n",
      "Epoch [33/100], Loss: 17.7010\n",
      "Epoch [34/100], Loss: 19.8957\n",
      "Epoch [35/100], Loss: 16.8367\n",
      "Epoch [36/100], Loss: 17.0419\n",
      "Epoch [37/100], Loss: 18.8276\n",
      "Epoch [38/100], Loss: 15.9464\n",
      "Epoch [39/100], Loss: 15.5556\n",
      "Epoch [40/100], Loss: 16.1400\n",
      "Epoch [41/100], Loss: 13.7758\n",
      "Epoch [42/100], Loss: 19.4727\n",
      "Epoch [43/100], Loss: 15.6918\n",
      "Epoch [44/100], Loss: 17.1859\n",
      "Epoch [45/100], Loss: 17.3967\n",
      "Epoch [46/100], Loss: 14.4457\n",
      "Epoch [47/100], Loss: 16.3150\n",
      "Epoch [48/100], Loss: 13.4763\n",
      "Epoch [49/100], Loss: 13.0595\n",
      "Epoch [50/100], Loss: 18.2349\n",
      "Epoch [51/100], Loss: 15.6807\n",
      "Epoch [52/100], Loss: 14.4435\n",
      "Epoch [53/100], Loss: 14.4972\n",
      "Epoch [54/100], Loss: 13.5670\n",
      "Epoch [55/100], Loss: 13.5818\n",
      "Epoch [56/100], Loss: 12.4649\n",
      "Epoch [57/100], Loss: 12.0254\n",
      "Epoch [58/100], Loss: 12.5698\n",
      "Epoch [59/100], Loss: 11.5277\n",
      "Epoch [60/100], Loss: 12.3088\n",
      "Epoch [61/100], Loss: 13.3401\n",
      "Epoch [62/100], Loss: 12.4286\n",
      "Epoch [63/100], Loss: 14.2928\n",
      "Epoch [64/100], Loss: 16.7693\n",
      "Epoch [65/100], Loss: 13.8365\n",
      "Epoch [66/100], Loss: 12.8030\n",
      "Epoch [67/100], Loss: 12.7087\n",
      "Epoch [68/100], Loss: 12.1073\n",
      "Epoch [69/100], Loss: 12.4744\n",
      "Epoch [70/100], Loss: 12.3426\n",
      "Epoch [71/100], Loss: 11.6645\n",
      "Epoch [72/100], Loss: 10.7099\n",
      "Epoch [73/100], Loss: 14.3115\n",
      "Epoch [74/100], Loss: 11.9972\n",
      "Epoch [75/100], Loss: 11.8529\n",
      "Epoch [76/100], Loss: 13.2540\n",
      "Epoch [77/100], Loss: 10.5398\n",
      "Epoch [78/100], Loss: 12.5123\n",
      "Epoch [79/100], Loss: 10.3693\n",
      "Epoch [80/100], Loss: 13.0933\n",
      "Epoch [81/100], Loss: 12.6942\n",
      "Epoch [82/100], Loss: 11.2545\n",
      "Epoch [83/100], Loss: 12.1644\n",
      "Epoch [84/100], Loss: 13.5618\n",
      "Epoch [85/100], Loss: 11.3102\n",
      "Epoch [86/100], Loss: 11.3327\n",
      "Epoch [87/100], Loss: 9.9773\n",
      "Epoch [88/100], Loss: 9.8323\n",
      "Epoch [89/100], Loss: 9.9966\n",
      "Epoch [90/100], Loss: 10.7199\n",
      "Epoch [91/100], Loss: 10.6765\n",
      "Epoch [92/100], Loss: 10.3707\n",
      "Epoch [93/100], Loss: 10.6283\n",
      "Epoch [94/100], Loss: 10.8199\n",
      "Epoch [95/100], Loss: 11.1853\n",
      "Epoch [96/100], Loss: 9.8360\n",
      "Epoch [97/100], Loss: 9.0381\n",
      "Epoch [98/100], Loss: 10.0756\n",
      "Epoch [99/100], Loss: 9.2436\n",
      "Epoch [100/100], Loss: 9.7818\n",
      "Validation RMSE: 20.052980014256068\n",
      "Iteration 29: Training NN with optimizer=sgd, hidden_size=128, lr=0.0001, batch_size=64, n_epochs=100\n",
      "Epoch [1/100], Loss: 580.3600\n",
      "Epoch [2/100], Loss: 591.8220\n",
      "Epoch [3/100], Loss: 575.6098\n",
      "Epoch [4/100], Loss: 552.9505\n",
      "Epoch [5/100], Loss: 568.2189\n",
      "Epoch [6/100], Loss: 567.4782\n",
      "Epoch [7/100], Loss: 530.6936\n",
      "Epoch [8/100], Loss: 510.2672\n",
      "Epoch [9/100], Loss: 525.4801\n",
      "Epoch [10/100], Loss: 499.4460\n",
      "Epoch [11/100], Loss: 486.4607\n",
      "Epoch [12/100], Loss: 472.1235\n",
      "Epoch [13/100], Loss: 464.4211\n",
      "Epoch [14/100], Loss: 447.3727\n",
      "Epoch [15/100], Loss: 425.6862\n",
      "Epoch [16/100], Loss: 416.6666\n",
      "Epoch [17/100], Loss: 404.1894\n",
      "Epoch [18/100], Loss: 406.2599\n",
      "Epoch [19/100], Loss: 382.8756\n",
      "Epoch [20/100], Loss: 371.1268\n",
      "Epoch [21/100], Loss: 348.2451\n",
      "Epoch [22/100], Loss: 312.3842\n",
      "Epoch [23/100], Loss: 308.4517\n",
      "Epoch [24/100], Loss: 291.5961\n",
      "Epoch [25/100], Loss: 283.5161\n",
      "Epoch [26/100], Loss: 265.3562\n",
      "Epoch [27/100], Loss: 257.3099\n",
      "Epoch [28/100], Loss: 223.8589\n",
      "Epoch [29/100], Loss: 210.3713\n",
      "Epoch [30/100], Loss: 215.1841\n",
      "Epoch [31/100], Loss: 195.2675\n",
      "Epoch [32/100], Loss: 185.0102\n",
      "Epoch [33/100], Loss: 167.5885\n",
      "Epoch [34/100], Loss: 171.9135\n",
      "Epoch [35/100], Loss: 152.1777\n",
      "Epoch [36/100], Loss: 146.2860\n",
      "Epoch [37/100], Loss: 141.7631\n",
      "Epoch [38/100], Loss: 132.2021\n",
      "Epoch [39/100], Loss: 136.2591\n",
      "Epoch [40/100], Loss: 127.0437\n",
      "Epoch [41/100], Loss: 117.8526\n",
      "Epoch [42/100], Loss: 115.4655\n",
      "Epoch [43/100], Loss: 117.3899\n",
      "Epoch [44/100], Loss: 116.5331\n",
      "Epoch [45/100], Loss: 113.5541\n",
      "Epoch [46/100], Loss: 110.7525\n",
      "Epoch [47/100], Loss: 108.8975\n",
      "Epoch [48/100], Loss: 108.7744\n",
      "Epoch [49/100], Loss: 102.0861\n",
      "Epoch [50/100], Loss: 100.6512\n",
      "Epoch [51/100], Loss: 99.7928\n",
      "Epoch [52/100], Loss: 101.9138\n",
      "Epoch [53/100], Loss: 101.9985\n",
      "Epoch [54/100], Loss: 96.2159\n",
      "Epoch [55/100], Loss: 101.7635\n",
      "Epoch [56/100], Loss: 99.4803\n",
      "Epoch [57/100], Loss: 98.0029\n",
      "Epoch [58/100], Loss: 98.9222\n",
      "Epoch [59/100], Loss: 94.9569\n",
      "Epoch [60/100], Loss: 100.1974\n",
      "Epoch [61/100], Loss: 100.0958\n",
      "Epoch [62/100], Loss: 92.3709\n",
      "Epoch [63/100], Loss: 98.3707\n",
      "Epoch [64/100], Loss: 92.2045\n",
      "Epoch [65/100], Loss: 95.4042\n",
      "Epoch [66/100], Loss: 91.2710\n",
      "Epoch [67/100], Loss: 90.1077\n",
      "Epoch [68/100], Loss: 87.7418\n",
      "Epoch [69/100], Loss: 88.2955\n",
      "Epoch [70/100], Loss: 90.1342\n",
      "Epoch [71/100], Loss: 88.2303\n",
      "Epoch [72/100], Loss: 86.8661\n",
      "Epoch [73/100], Loss: 89.8612\n",
      "Epoch [74/100], Loss: 87.2973\n",
      "Epoch [75/100], Loss: 86.3450\n",
      "Epoch [76/100], Loss: 84.7204\n",
      "Epoch [77/100], Loss: 83.4081\n",
      "Epoch [78/100], Loss: 86.4028\n",
      "Epoch [79/100], Loss: 93.1316\n",
      "Epoch [80/100], Loss: 86.9654\n",
      "Epoch [81/100], Loss: 83.8419\n",
      "Epoch [82/100], Loss: 84.6316\n",
      "Epoch [83/100], Loss: 82.5921\n",
      "Epoch [84/100], Loss: 86.7586\n",
      "Epoch [85/100], Loss: 83.1201\n",
      "Epoch [86/100], Loss: 83.2849\n",
      "Epoch [87/100], Loss: 78.5006\n",
      "Epoch [88/100], Loss: 81.5185\n",
      "Epoch [89/100], Loss: 81.3542\n",
      "Epoch [90/100], Loss: 81.2228\n",
      "Epoch [91/100], Loss: 80.6821\n",
      "Epoch [92/100], Loss: 82.5447\n",
      "Epoch [93/100], Loss: 79.0524\n",
      "Epoch [94/100], Loss: 85.6849\n",
      "Epoch [95/100], Loss: 77.4115\n",
      "Epoch [96/100], Loss: 76.5277\n",
      "Epoch [97/100], Loss: 79.6856\n",
      "Epoch [98/100], Loss: 81.4086\n",
      "Epoch [99/100], Loss: 83.1001\n",
      "Epoch [100/100], Loss: 86.4872\n",
      "Validation RMSE: 65.72188949584961\n",
      "Iteration 30: Training NN with optimizer=sgd, hidden_size=32, lr=0.0001, batch_size=32, n_epochs=150\n",
      "Epoch [1/150], Loss: 578.2719\n",
      "Epoch [2/150], Loss: 569.0989\n",
      "Epoch [3/150], Loss: 560.0727\n",
      "Epoch [4/150], Loss: 556.8513\n",
      "Epoch [5/150], Loss: 539.4907\n",
      "Epoch [6/150], Loss: 530.1559\n",
      "Epoch [7/150], Loss: 518.3948\n",
      "Epoch [8/150], Loss: 503.0326\n",
      "Epoch [9/150], Loss: 488.3192\n",
      "Epoch [10/150], Loss: 465.2113\n",
      "Epoch [11/150], Loss: 449.3379\n",
      "Epoch [12/150], Loss: 423.4048\n",
      "Epoch [13/150], Loss: 399.3957\n",
      "Epoch [14/150], Loss: 380.5096\n",
      "Epoch [15/150], Loss: 344.4695\n",
      "Epoch [16/150], Loss: 314.9198\n",
      "Epoch [17/150], Loss: 288.4456\n",
      "Epoch [18/150], Loss: 264.2842\n",
      "Epoch [19/150], Loss: 237.2805\n",
      "Epoch [20/150], Loss: 213.3992\n",
      "Epoch [21/150], Loss: 190.2423\n",
      "Epoch [22/150], Loss: 175.4893\n",
      "Epoch [23/150], Loss: 155.3148\n",
      "Epoch [24/150], Loss: 145.1080\n",
      "Epoch [25/150], Loss: 135.7222\n",
      "Epoch [26/150], Loss: 131.5899\n",
      "Epoch [27/150], Loss: 119.8191\n",
      "Epoch [28/150], Loss: 116.2774\n",
      "Epoch [29/150], Loss: 112.3926\n",
      "Epoch [30/150], Loss: 110.7558\n",
      "Epoch [31/150], Loss: 107.8762\n",
      "Epoch [32/150], Loss: 107.1834\n",
      "Epoch [33/150], Loss: 105.7973\n",
      "Epoch [34/150], Loss: 103.4728\n",
      "Epoch [35/150], Loss: 100.8128\n",
      "Epoch [36/150], Loss: 101.0986\n",
      "Epoch [37/150], Loss: 103.0087\n",
      "Epoch [38/150], Loss: 99.0873\n",
      "Epoch [39/150], Loss: 97.5521\n",
      "Epoch [40/150], Loss: 97.4913\n",
      "Epoch [41/150], Loss: 96.0154\n",
      "Epoch [42/150], Loss: 95.2466\n",
      "Epoch [43/150], Loss: 93.1155\n",
      "Epoch [44/150], Loss: 93.0336\n",
      "Epoch [45/150], Loss: 91.9980\n",
      "Epoch [46/150], Loss: 91.1753\n",
      "Epoch [47/150], Loss: 90.8593\n",
      "Epoch [48/150], Loss: 89.1366\n",
      "Epoch [49/150], Loss: 87.9983\n",
      "Epoch [50/150], Loss: 88.2945\n",
      "Epoch [51/150], Loss: 86.4123\n",
      "Epoch [52/150], Loss: 86.1549\n",
      "Epoch [53/150], Loss: 85.2114\n",
      "Epoch [54/150], Loss: 85.6645\n",
      "Epoch [55/150], Loss: 84.5494\n",
      "Epoch [56/150], Loss: 83.6648\n",
      "Epoch [57/150], Loss: 82.9275\n",
      "Epoch [58/150], Loss: 83.0844\n",
      "Epoch [59/150], Loss: 81.3767\n",
      "Epoch [60/150], Loss: 81.0226\n",
      "Epoch [61/150], Loss: 79.6835\n",
      "Epoch [62/150], Loss: 80.1352\n",
      "Epoch [63/150], Loss: 80.2836\n",
      "Epoch [64/150], Loss: 80.5939\n",
      "Epoch [65/150], Loss: 77.9214\n",
      "Epoch [66/150], Loss: 77.6305\n",
      "Epoch [67/150], Loss: 77.9969\n",
      "Epoch [68/150], Loss: 76.6394\n",
      "Epoch [69/150], Loss: 74.7984\n",
      "Epoch [70/150], Loss: 75.4763\n",
      "Epoch [71/150], Loss: 73.6229\n",
      "Epoch [72/150], Loss: 73.0351\n",
      "Epoch [73/150], Loss: 74.7018\n",
      "Epoch [74/150], Loss: 72.5582\n",
      "Epoch [75/150], Loss: 71.5371\n",
      "Epoch [76/150], Loss: 71.5793\n",
      "Epoch [77/150], Loss: 70.9175\n",
      "Epoch [78/150], Loss: 74.0145\n",
      "Epoch [79/150], Loss: 71.3667\n",
      "Epoch [80/150], Loss: 69.5002\n",
      "Epoch [81/150], Loss: 69.5399\n",
      "Epoch [82/150], Loss: 69.9084\n",
      "Epoch [83/150], Loss: 70.8637\n",
      "Epoch [84/150], Loss: 67.7445\n",
      "Epoch [85/150], Loss: 68.5131\n",
      "Epoch [86/150], Loss: 67.3796\n",
      "Epoch [87/150], Loss: 66.6609\n",
      "Epoch [88/150], Loss: 66.6121\n",
      "Epoch [89/150], Loss: 66.4628\n",
      "Epoch [90/150], Loss: 66.7342\n",
      "Epoch [91/150], Loss: 65.0604\n",
      "Epoch [92/150], Loss: 65.4338\n",
      "Epoch [93/150], Loss: 65.8268\n",
      "Epoch [94/150], Loss: 63.9187\n",
      "Epoch [95/150], Loss: 65.4503\n",
      "Epoch [96/150], Loss: 64.5182\n",
      "Epoch [97/150], Loss: 62.5634\n",
      "Epoch [98/150], Loss: 64.4950\n",
      "Epoch [99/150], Loss: 62.0205\n",
      "Epoch [100/150], Loss: 61.4459\n",
      "Epoch [101/150], Loss: 61.3870\n",
      "Epoch [102/150], Loss: 61.2971\n",
      "Epoch [103/150], Loss: 61.4911\n",
      "Epoch [104/150], Loss: 60.5609\n",
      "Epoch [105/150], Loss: 61.6650\n",
      "Epoch [106/150], Loss: 60.0794\n",
      "Epoch [107/150], Loss: 61.0900\n",
      "Epoch [108/150], Loss: 60.7631\n",
      "Epoch [109/150], Loss: 60.4645\n",
      "Epoch [110/150], Loss: 60.6561\n",
      "Epoch [111/150], Loss: 60.5267\n",
      "Epoch [112/150], Loss: 58.2586\n",
      "Epoch [113/150], Loss: 59.8319\n",
      "Epoch [114/150], Loss: 59.9899\n",
      "Epoch [115/150], Loss: 57.5118\n",
      "Epoch [116/150], Loss: 58.8781\n",
      "Epoch [117/150], Loss: 57.9139\n",
      "Epoch [118/150], Loss: 57.1659\n",
      "Epoch [119/150], Loss: 56.8827\n",
      "Epoch [120/150], Loss: 57.5757\n",
      "Epoch [121/150], Loss: 56.0126\n",
      "Epoch [122/150], Loss: 60.0722\n",
      "Epoch [123/150], Loss: 56.1474\n",
      "Epoch [124/150], Loss: 57.7170\n",
      "Epoch [125/150], Loss: 56.4532\n",
      "Epoch [126/150], Loss: 58.5105\n",
      "Epoch [127/150], Loss: 56.1047\n",
      "Epoch [128/150], Loss: 55.2052\n",
      "Epoch [129/150], Loss: 55.5454\n",
      "Epoch [130/150], Loss: 56.1955\n",
      "Epoch [131/150], Loss: 55.1445\n",
      "Epoch [132/150], Loss: 54.5207\n",
      "Epoch [133/150], Loss: 54.1386\n",
      "Epoch [134/150], Loss: 54.1618\n",
      "Epoch [135/150], Loss: 54.8360\n",
      "Epoch [136/150], Loss: 54.6931\n",
      "Epoch [137/150], Loss: 53.7357\n",
      "Epoch [138/150], Loss: 56.6615\n",
      "Epoch [139/150], Loss: 52.9030\n",
      "Epoch [140/150], Loss: 54.0210\n",
      "Epoch [141/150], Loss: 54.0022\n",
      "Epoch [142/150], Loss: 54.5738\n",
      "Epoch [143/150], Loss: 53.3951\n",
      "Epoch [144/150], Loss: 53.1685\n",
      "Epoch [145/150], Loss: 53.0843\n",
      "Epoch [146/150], Loss: 52.9254\n",
      "Epoch [147/150], Loss: 53.1058\n",
      "Epoch [148/150], Loss: 51.9116\n",
      "Epoch [149/150], Loss: 52.6022\n",
      "Epoch [150/150], Loss: 51.1355\n",
      "Validation RMSE: 34.45029163360596\n",
      "Iteration 31: Training NN with optimizer=sgd, hidden_size=128, lr=0.01, batch_size=64, n_epochs=50\n",
      "Epoch [1/50], Loss: 244.1865\n",
      "Epoch [2/50], Loss: 69.3611\n",
      "Epoch [3/50], Loss: 53.0524\n",
      "Epoch [4/50], Loss: 64.8993\n",
      "Epoch [5/50], Loss: 45.1446\n",
      "Epoch [6/50], Loss: 42.7185\n",
      "Epoch [7/50], Loss: 39.9657\n",
      "Epoch [8/50], Loss: 83.3247\n",
      "Epoch [9/50], Loss: 34.8398\n",
      "Epoch [10/50], Loss: 35.3248\n",
      "Epoch [11/50], Loss: 30.2091\n",
      "Epoch [12/50], Loss: 30.0657\n",
      "Epoch [13/50], Loss: 55.3044\n",
      "Epoch [14/50], Loss: 52.5515\n",
      "Epoch [15/50], Loss: 38.1848\n",
      "Epoch [16/50], Loss: 30.3359\n",
      "Epoch [17/50], Loss: 29.0840\n",
      "Epoch [18/50], Loss: 32.9803\n",
      "Epoch [19/50], Loss: 42.6291\n",
      "Epoch [20/50], Loss: 26.0796\n",
      "Epoch [21/50], Loss: 25.4792\n",
      "Epoch [22/50], Loss: 29.6188\n",
      "Epoch [23/50], Loss: 21.6553\n",
      "Epoch [24/50], Loss: 25.4164\n",
      "Epoch [25/50], Loss: 31.4640\n",
      "Epoch [26/50], Loss: 34.7023\n",
      "Epoch [27/50], Loss: 25.3389\n",
      "Epoch [28/50], Loss: 25.3552\n",
      "Epoch [29/50], Loss: 26.0275\n",
      "Epoch [30/50], Loss: 24.6607\n",
      "Epoch [31/50], Loss: 24.4740\n",
      "Epoch [32/50], Loss: 41.5561\n",
      "Epoch [33/50], Loss: 32.4051\n",
      "Epoch [34/50], Loss: 21.3354\n",
      "Epoch [35/50], Loss: 21.7288\n",
      "Epoch [36/50], Loss: 24.2760\n",
      "Epoch [37/50], Loss: 32.0236\n",
      "Epoch [38/50], Loss: 36.0449\n",
      "Epoch [39/50], Loss: 25.1098\n",
      "Epoch [40/50], Loss: 22.4217\n",
      "Epoch [41/50], Loss: 29.5716\n",
      "Epoch [42/50], Loss: 23.8175\n",
      "Epoch [43/50], Loss: 21.0286\n",
      "Epoch [44/50], Loss: 22.5374\n",
      "Epoch [45/50], Loss: 26.1595\n",
      "Epoch [46/50], Loss: 33.5899\n",
      "Epoch [47/50], Loss: 29.6052\n",
      "Epoch [48/50], Loss: 25.7034\n",
      "Epoch [49/50], Loss: 41.5828\n",
      "Epoch [50/50], Loss: 20.1849\n",
      "Validation RMSE: 15.80356502532959\n",
      "Iteration 32: Training NN with optimizer=sgd, hidden_size=64, lr=0.001, batch_size=32, n_epochs=100\n",
      "Epoch [1/100], Loss: 539.8940\n",
      "Epoch [2/100], Loss: 308.5496\n",
      "Epoch [3/100], Loss: 129.9816\n",
      "Epoch [4/100], Loss: 102.2426\n",
      "Epoch [5/100], Loss: 91.1952\n",
      "Epoch [6/100], Loss: 82.5936\n",
      "Epoch [7/100], Loss: 79.2102\n",
      "Epoch [8/100], Loss: 71.2315\n",
      "Epoch [9/100], Loss: 69.4653\n",
      "Epoch [10/100], Loss: 63.3389\n",
      "Epoch [11/100], Loss: 60.5633\n",
      "Epoch [12/100], Loss: 57.6924\n",
      "Epoch [13/100], Loss: 53.7379\n",
      "Epoch [14/100], Loss: 52.8036\n",
      "Epoch [15/100], Loss: 52.7629\n",
      "Epoch [16/100], Loss: 48.8711\n",
      "Epoch [17/100], Loss: 48.0303\n",
      "Epoch [18/100], Loss: 46.3139\n",
      "Epoch [19/100], Loss: 45.2872\n",
      "Epoch [20/100], Loss: 46.8102\n",
      "Epoch [21/100], Loss: 43.8211\n",
      "Epoch [22/100], Loss: 43.3183\n",
      "Epoch [23/100], Loss: 42.6817\n",
      "Epoch [24/100], Loss: 44.2154\n",
      "Epoch [25/100], Loss: 42.1409\n",
      "Epoch [26/100], Loss: 40.0433\n",
      "Epoch [27/100], Loss: 39.9308\n",
      "Epoch [28/100], Loss: 38.9381\n",
      "Epoch [29/100], Loss: 38.2362\n",
      "Epoch [30/100], Loss: 38.0150\n",
      "Epoch [31/100], Loss: 37.7000\n",
      "Epoch [32/100], Loss: 36.2338\n",
      "Epoch [33/100], Loss: 35.8534\n",
      "Epoch [34/100], Loss: 34.8909\n",
      "Epoch [35/100], Loss: 34.7625\n",
      "Epoch [36/100], Loss: 34.5724\n",
      "Epoch [37/100], Loss: 34.3375\n",
      "Epoch [38/100], Loss: 33.5658\n",
      "Epoch [39/100], Loss: 35.1338\n",
      "Epoch [40/100], Loss: 32.5254\n",
      "Epoch [41/100], Loss: 32.5222\n",
      "Epoch [42/100], Loss: 31.6856\n",
      "Epoch [43/100], Loss: 31.5093\n",
      "Epoch [44/100], Loss: 30.7377\n",
      "Epoch [45/100], Loss: 30.7809\n",
      "Epoch [46/100], Loss: 30.8444\n",
      "Epoch [47/100], Loss: 29.6382\n",
      "Epoch [48/100], Loss: 29.4500\n",
      "Epoch [49/100], Loss: 29.3794\n",
      "Epoch [50/100], Loss: 30.2508\n",
      "Epoch [51/100], Loss: 28.9857\n",
      "Epoch [52/100], Loss: 29.2276\n",
      "Epoch [53/100], Loss: 28.6708\n",
      "Epoch [54/100], Loss: 27.6257\n",
      "Epoch [55/100], Loss: 27.4997\n",
      "Epoch [56/100], Loss: 27.1292\n",
      "Epoch [57/100], Loss: 26.7092\n",
      "Epoch [58/100], Loss: 27.0606\n",
      "Epoch [59/100], Loss: 27.3069\n",
      "Epoch [60/100], Loss: 26.5875\n",
      "Epoch [61/100], Loss: 26.1896\n",
      "Epoch [62/100], Loss: 26.2404\n",
      "Epoch [63/100], Loss: 25.9785\n",
      "Epoch [64/100], Loss: 25.7574\n",
      "Epoch [65/100], Loss: 25.8594\n",
      "Epoch [66/100], Loss: 25.5239\n",
      "Epoch [67/100], Loss: 25.3179\n",
      "Epoch [68/100], Loss: 25.1397\n",
      "Epoch [69/100], Loss: 25.1111\n",
      "Epoch [70/100], Loss: 24.6110\n",
      "Epoch [71/100], Loss: 25.0652\n",
      "Epoch [72/100], Loss: 24.6061\n",
      "Epoch [73/100], Loss: 24.1894\n",
      "Epoch [74/100], Loss: 24.4088\n",
      "Epoch [75/100], Loss: 24.2430\n",
      "Epoch [76/100], Loss: 23.8959\n",
      "Epoch [77/100], Loss: 23.8834\n",
      "Epoch [78/100], Loss: 23.5843\n",
      "Epoch [79/100], Loss: 23.5567\n",
      "Epoch [80/100], Loss: 23.9809\n",
      "Epoch [81/100], Loss: 24.6727\n",
      "Epoch [82/100], Loss: 23.7423\n",
      "Epoch [83/100], Loss: 23.4430\n",
      "Epoch [84/100], Loss: 23.1806\n",
      "Epoch [85/100], Loss: 23.0450\n",
      "Epoch [86/100], Loss: 24.0677\n",
      "Epoch [87/100], Loss: 22.7940\n",
      "Epoch [88/100], Loss: 23.3095\n",
      "Epoch [89/100], Loss: 22.7400\n",
      "Epoch [90/100], Loss: 22.9993\n",
      "Epoch [91/100], Loss: 22.5897\n",
      "Epoch [92/100], Loss: 23.5809\n",
      "Epoch [93/100], Loss: 22.5407\n",
      "Epoch [94/100], Loss: 22.3796\n",
      "Epoch [95/100], Loss: 22.5360\n",
      "Epoch [96/100], Loss: 22.8791\n",
      "Epoch [97/100], Loss: 22.9745\n",
      "Epoch [98/100], Loss: 22.2449\n",
      "Epoch [99/100], Loss: 23.3553\n",
      "Epoch [100/100], Loss: 22.0247\n",
      "Validation RMSE: 12.139877557754517\n",
      "Iteration 33: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=16, n_epochs=50\n",
      "Epoch [1/50], Loss: 488.9752\n",
      "Epoch [2/50], Loss: 143.0298\n",
      "Epoch [3/50], Loss: 87.6297\n",
      "Epoch [4/50], Loss: 72.5975\n",
      "Epoch [5/50], Loss: 69.6626\n",
      "Epoch [6/50], Loss: 56.7055\n",
      "Epoch [7/50], Loss: 51.0656\n",
      "Epoch [8/50], Loss: 49.4729\n",
      "Epoch [9/50], Loss: 47.8276\n",
      "Epoch [10/50], Loss: 46.5544\n",
      "Epoch [11/50], Loss: 46.1362\n",
      "Epoch [12/50], Loss: 47.4520\n",
      "Epoch [13/50], Loss: 41.0655\n",
      "Epoch [14/50], Loss: 39.1572\n",
      "Epoch [15/50], Loss: 37.7449\n",
      "Epoch [16/50], Loss: 36.3929\n",
      "Epoch [17/50], Loss: 35.4811\n",
      "Epoch [18/50], Loss: 34.4002\n",
      "Epoch [19/50], Loss: 33.9170\n",
      "Epoch [20/50], Loss: 33.5292\n",
      "Epoch [21/50], Loss: 32.3433\n",
      "Epoch [22/50], Loss: 31.4348\n",
      "Epoch [23/50], Loss: 33.5851\n",
      "Epoch [24/50], Loss: 32.7053\n",
      "Epoch [25/50], Loss: 29.6512\n",
      "Epoch [26/50], Loss: 29.7799\n",
      "Epoch [27/50], Loss: 29.1748\n",
      "Epoch [28/50], Loss: 28.4217\n",
      "Epoch [29/50], Loss: 27.6673\n",
      "Epoch [30/50], Loss: 27.4908\n",
      "Epoch [31/50], Loss: 28.7101\n",
      "Epoch [32/50], Loss: 26.6091\n",
      "Epoch [33/50], Loss: 26.6457\n",
      "Epoch [34/50], Loss: 25.7181\n",
      "Epoch [35/50], Loss: 25.5547\n",
      "Epoch [36/50], Loss: 25.5459\n",
      "Epoch [37/50], Loss: 27.7614\n",
      "Epoch [38/50], Loss: 30.0209\n",
      "Epoch [39/50], Loss: 25.4693\n",
      "Epoch [40/50], Loss: 24.4145\n",
      "Epoch [41/50], Loss: 24.8844\n",
      "Epoch [42/50], Loss: 24.0261\n",
      "Epoch [43/50], Loss: 23.8874\n",
      "Epoch [44/50], Loss: 25.8968\n",
      "Epoch [45/50], Loss: 25.8800\n",
      "Epoch [46/50], Loss: 23.5107\n",
      "Epoch [47/50], Loss: 24.0801\n",
      "Epoch [48/50], Loss: 23.8863\n",
      "Epoch [49/50], Loss: 23.9546\n",
      "Epoch [50/50], Loss: 22.9661\n",
      "Validation RMSE: 13.875581196376256\n",
      "Iteration 34: Training NN with optimizer=sgd, hidden_size=64, lr=0.01, batch_size=32, n_epochs=150\n",
      "Epoch [1/150], Loss: 177.6313\n",
      "Epoch [2/150], Loss: 58.8678\n",
      "Epoch [3/150], Loss: 63.7448\n",
      "Epoch [4/150], Loss: 60.0743\n",
      "Epoch [5/150], Loss: 34.3724\n",
      "Epoch [6/150], Loss: 50.5607\n",
      "Epoch [7/150], Loss: 46.6293\n",
      "Epoch [8/150], Loss: 40.4111\n",
      "Epoch [9/150], Loss: 33.7407\n",
      "Epoch [10/150], Loss: 39.6397\n",
      "Epoch [11/150], Loss: 30.8166\n",
      "Epoch [12/150], Loss: 34.5936\n",
      "Epoch [13/150], Loss: 33.3880\n",
      "Epoch [14/150], Loss: 31.6846\n",
      "Epoch [15/150], Loss: 29.4326\n",
      "Epoch [16/150], Loss: 26.1111\n",
      "Epoch [17/150], Loss: 28.5102\n",
      "Epoch [18/150], Loss: 23.1759\n",
      "Epoch [19/150], Loss: 32.9248\n",
      "Epoch [20/150], Loss: 28.3405\n",
      "Epoch [21/150], Loss: 25.9533\n",
      "Epoch [22/150], Loss: 23.2064\n",
      "Epoch [23/150], Loss: 23.2259\n",
      "Epoch [24/150], Loss: 25.4433\n",
      "Epoch [25/150], Loss: 28.4346\n",
      "Epoch [26/150], Loss: 22.1126\n",
      "Epoch [27/150], Loss: 21.8012\n",
      "Epoch [28/150], Loss: 25.9560\n",
      "Epoch [29/150], Loss: 33.0576\n",
      "Epoch [30/150], Loss: 20.1322\n",
      "Epoch [31/150], Loss: 23.1588\n",
      "Epoch [32/150], Loss: 20.6006\n",
      "Epoch [33/150], Loss: 21.1150\n",
      "Epoch [34/150], Loss: 24.6935\n",
      "Epoch [35/150], Loss: 30.1271\n",
      "Epoch [36/150], Loss: 20.9026\n",
      "Epoch [37/150], Loss: 26.6715\n",
      "Epoch [38/150], Loss: 20.0051\n",
      "Epoch [39/150], Loss: 19.0651\n",
      "Epoch [40/150], Loss: 19.5811\n",
      "Epoch [41/150], Loss: 19.1036\n",
      "Epoch [42/150], Loss: 22.0077\n",
      "Epoch [43/150], Loss: 18.6238\n",
      "Epoch [44/150], Loss: 20.4029\n",
      "Epoch [45/150], Loss: 19.8769\n",
      "Epoch [46/150], Loss: 20.7124\n",
      "Epoch [47/150], Loss: 19.3242\n",
      "Epoch [48/150], Loss: 19.3216\n",
      "Epoch [49/150], Loss: 19.8267\n",
      "Epoch [50/150], Loss: 22.3374\n",
      "Epoch [51/150], Loss: 17.3434\n",
      "Epoch [52/150], Loss: 19.6604\n",
      "Epoch [53/150], Loss: 26.2517\n",
      "Epoch [54/150], Loss: 16.7880\n",
      "Epoch [55/150], Loss: 18.1595\n",
      "Epoch [56/150], Loss: 20.0137\n",
      "Epoch [57/150], Loss: 17.5293\n",
      "Epoch [58/150], Loss: 22.8335\n",
      "Epoch [59/150], Loss: 22.4794\n",
      "Epoch [60/150], Loss: 18.8764\n",
      "Epoch [61/150], Loss: 16.5220\n",
      "Epoch [62/150], Loss: 17.8995\n",
      "Epoch [63/150], Loss: 15.2183\n",
      "Epoch [64/150], Loss: 21.5241\n",
      "Epoch [65/150], Loss: 16.6651\n",
      "Epoch [66/150], Loss: 18.7107\n",
      "Epoch [67/150], Loss: 21.8261\n",
      "Epoch [68/150], Loss: 15.5978\n",
      "Epoch [69/150], Loss: 16.1477\n",
      "Epoch [70/150], Loss: 14.4661\n",
      "Epoch [71/150], Loss: 20.3796\n",
      "Epoch [72/150], Loss: 16.5675\n",
      "Epoch [73/150], Loss: 16.7700\n",
      "Epoch [74/150], Loss: 21.8363\n",
      "Epoch [75/150], Loss: 16.7254\n",
      "Epoch [76/150], Loss: 13.9919\n",
      "Epoch [77/150], Loss: 29.1268\n",
      "Epoch [78/150], Loss: 15.9834\n",
      "Epoch [79/150], Loss: 13.8245\n",
      "Epoch [80/150], Loss: 16.1633\n",
      "Epoch [81/150], Loss: 14.0576\n",
      "Epoch [82/150], Loss: 16.4228\n",
      "Epoch [83/150], Loss: 14.0822\n",
      "Epoch [84/150], Loss: 14.5644\n",
      "Epoch [85/150], Loss: 18.7408\n",
      "Epoch [86/150], Loss: 13.9649\n",
      "Epoch [87/150], Loss: 12.9576\n",
      "Epoch [88/150], Loss: 20.7568\n",
      "Epoch [89/150], Loss: 15.6944\n",
      "Epoch [90/150], Loss: 13.0348\n",
      "Epoch [91/150], Loss: 12.9107\n",
      "Epoch [92/150], Loss: 11.9666\n",
      "Epoch [93/150], Loss: 13.7199\n",
      "Epoch [94/150], Loss: 13.0802\n",
      "Epoch [95/150], Loss: 17.5186\n",
      "Epoch [96/150], Loss: 11.8443\n",
      "Epoch [97/150], Loss: 13.6598\n",
      "Epoch [98/150], Loss: 13.0437\n",
      "Epoch [99/150], Loss: 11.6097\n",
      "Epoch [100/150], Loss: 12.8703\n",
      "Epoch [101/150], Loss: 11.8775\n",
      "Epoch [102/150], Loss: 12.8173\n",
      "Epoch [103/150], Loss: 14.1107\n",
      "Epoch [104/150], Loss: 14.1671\n",
      "Epoch [105/150], Loss: 12.0755\n",
      "Epoch [106/150], Loss: 12.9153\n",
      "Epoch [107/150], Loss: 12.6435\n",
      "Epoch [108/150], Loss: 15.9823\n",
      "Epoch [109/150], Loss: 12.7015\n",
      "Epoch [110/150], Loss: 12.2491\n",
      "Epoch [111/150], Loss: 14.9115\n",
      "Epoch [112/150], Loss: 19.0806\n",
      "Epoch [113/150], Loss: 13.2726\n",
      "Epoch [114/150], Loss: 11.0687\n",
      "Epoch [115/150], Loss: 10.8175\n",
      "Epoch [116/150], Loss: 13.7977\n",
      "Epoch [117/150], Loss: 14.3239\n",
      "Epoch [118/150], Loss: 12.6408\n",
      "Epoch [119/150], Loss: 17.0659\n",
      "Epoch [120/150], Loss: 12.0222\n",
      "Epoch [121/150], Loss: 12.8099\n",
      "Epoch [122/150], Loss: 10.4015\n",
      "Epoch [123/150], Loss: 10.7843\n",
      "Epoch [124/150], Loss: 13.3477\n",
      "Epoch [125/150], Loss: 10.2044\n",
      "Epoch [126/150], Loss: 11.0676\n",
      "Epoch [127/150], Loss: 10.9825\n",
      "Epoch [128/150], Loss: 12.6680\n",
      "Epoch [129/150], Loss: 12.7830\n",
      "Epoch [130/150], Loss: 13.9413\n",
      "Epoch [131/150], Loss: 11.1782\n",
      "Epoch [132/150], Loss: 14.5875\n",
      "Epoch [133/150], Loss: 11.1203\n",
      "Epoch [134/150], Loss: 11.2208\n",
      "Epoch [135/150], Loss: 10.9748\n",
      "Epoch [136/150], Loss: 10.9581\n",
      "Epoch [137/150], Loss: 12.1888\n",
      "Epoch [138/150], Loss: 11.9226\n",
      "Epoch [139/150], Loss: 10.6738\n",
      "Epoch [140/150], Loss: 11.3932\n",
      "Epoch [141/150], Loss: 9.8862\n",
      "Epoch [142/150], Loss: 9.7158\n",
      "Epoch [143/150], Loss: 9.6016\n",
      "Epoch [144/150], Loss: 11.0522\n",
      "Epoch [145/150], Loss: 10.7303\n",
      "Epoch [146/150], Loss: 8.9142\n",
      "Epoch [147/150], Loss: 11.7716\n",
      "Epoch [148/150], Loss: 9.7974\n",
      "Epoch [149/150], Loss: 9.5524\n",
      "Epoch [150/150], Loss: 10.4823\n",
      "Validation RMSE: 6.603385925292969\n",
      "Iteration 35: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=64, n_epochs=50\n",
      "Epoch [1/50], Loss: 581.9686\n",
      "Epoch [2/50], Loss: 533.9771\n",
      "Epoch [3/50], Loss: 397.8099\n",
      "Epoch [4/50], Loss: 254.7941\n",
      "Epoch [5/50], Loss: 154.8823\n",
      "Epoch [6/50], Loss: 119.4917\n",
      "Epoch [7/50], Loss: 106.4392\n",
      "Epoch [8/50], Loss: 100.9679\n",
      "Epoch [9/50], Loss: 102.9351\n",
      "Epoch [10/50], Loss: 93.3128\n",
      "Epoch [11/50], Loss: 94.1051\n",
      "Epoch [12/50], Loss: 85.4384\n",
      "Epoch [13/50], Loss: 78.9877\n",
      "Epoch [14/50], Loss: 78.6104\n",
      "Epoch [15/50], Loss: 72.9195\n",
      "Epoch [16/50], Loss: 73.8130\n",
      "Epoch [17/50], Loss: 70.7868\n",
      "Epoch [18/50], Loss: 72.5507\n",
      "Epoch [19/50], Loss: 65.3135\n",
      "Epoch [20/50], Loss: 63.4940\n",
      "Epoch [21/50], Loss: 63.1037\n",
      "Epoch [22/50], Loss: 61.8605\n",
      "Epoch [23/50], Loss: 60.5150\n",
      "Epoch [24/50], Loss: 59.9630\n",
      "Epoch [25/50], Loss: 57.9112\n",
      "Epoch [26/50], Loss: 55.8651\n",
      "Epoch [27/50], Loss: 55.6952\n",
      "Epoch [28/50], Loss: 57.1490\n",
      "Epoch [29/50], Loss: 52.1060\n",
      "Epoch [30/50], Loss: 50.5454\n",
      "Epoch [31/50], Loss: 58.2213\n",
      "Epoch [32/50], Loss: 50.7987\n",
      "Epoch [33/50], Loss: 49.6273\n",
      "Epoch [34/50], Loss: 54.3613\n",
      "Epoch [35/50], Loss: 51.7358\n",
      "Epoch [36/50], Loss: 55.6176\n",
      "Epoch [37/50], Loss: 49.1287\n",
      "Epoch [38/50], Loss: 46.8197\n",
      "Epoch [39/50], Loss: 47.6809\n",
      "Epoch [40/50], Loss: 45.9609\n",
      "Epoch [41/50], Loss: 44.2454\n",
      "Epoch [42/50], Loss: 45.6523\n",
      "Epoch [43/50], Loss: 44.7100\n",
      "Epoch [44/50], Loss: 48.2386\n",
      "Epoch [45/50], Loss: 46.5114\n",
      "Epoch [46/50], Loss: 43.5278\n",
      "Epoch [47/50], Loss: 45.2265\n",
      "Epoch [48/50], Loss: 40.7900\n",
      "Epoch [49/50], Loss: 41.9593\n",
      "Epoch [50/50], Loss: 41.7968\n",
      "Validation RMSE: 29.611157417297363\n",
      "Iteration 36: Training NN with optimizer=sgd, hidden_size=32, lr=0.0001, batch_size=64, n_epochs=100\n",
      "Epoch [1/100], Loss: 585.8591\n",
      "Epoch [2/100], Loss: 598.8894\n",
      "Epoch [3/100], Loss: 597.3766\n",
      "Epoch [4/100], Loss: 613.6746\n",
      "Epoch [5/100], Loss: 596.6365\n",
      "Epoch [6/100], Loss: 576.2312\n",
      "Epoch [7/100], Loss: 560.2578\n",
      "Epoch [8/100], Loss: 583.9421\n",
      "Epoch [9/100], Loss: 566.0313\n",
      "Epoch [10/100], Loss: 571.4948\n",
      "Epoch [11/100], Loss: 559.2062\n",
      "Epoch [12/100], Loss: 551.4243\n",
      "Epoch [13/100], Loss: 542.0571\n",
      "Epoch [14/100], Loss: 521.7273\n",
      "Epoch [15/100], Loss: 532.7002\n",
      "Epoch [16/100], Loss: 524.5148\n",
      "Epoch [17/100], Loss: 497.4683\n",
      "Epoch [18/100], Loss: 501.0029\n",
      "Epoch [19/100], Loss: 474.6329\n",
      "Epoch [20/100], Loss: 474.8010\n",
      "Epoch [21/100], Loss: 490.7661\n",
      "Epoch [22/100], Loss: 461.2957\n",
      "Epoch [23/100], Loss: 434.1777\n",
      "Epoch [24/100], Loss: 443.5247\n",
      "Epoch [25/100], Loss: 433.6239\n",
      "Epoch [26/100], Loss: 423.1272\n",
      "Epoch [27/100], Loss: 387.3671\n",
      "Epoch [28/100], Loss: 375.9275\n",
      "Epoch [29/100], Loss: 379.0360\n",
      "Epoch [30/100], Loss: 365.2042\n",
      "Epoch [31/100], Loss: 351.0328\n",
      "Epoch [32/100], Loss: 320.1086\n",
      "Epoch [33/100], Loss: 325.2418\n",
      "Epoch [34/100], Loss: 300.1632\n",
      "Epoch [35/100], Loss: 269.6381\n",
      "Epoch [36/100], Loss: 262.2580\n",
      "Epoch [37/100], Loss: 266.0486\n",
      "Epoch [38/100], Loss: 247.1760\n",
      "Epoch [39/100], Loss: 220.1855\n",
      "Epoch [40/100], Loss: 212.6219\n",
      "Epoch [41/100], Loss: 192.6640\n",
      "Epoch [42/100], Loss: 187.1137\n",
      "Epoch [43/100], Loss: 170.4283\n",
      "Epoch [44/100], Loss: 167.4101\n",
      "Epoch [45/100], Loss: 165.2216\n",
      "Epoch [46/100], Loss: 149.6298\n",
      "Epoch [47/100], Loss: 144.9267\n",
      "Epoch [48/100], Loss: 137.4093\n",
      "Epoch [49/100], Loss: 137.6942\n",
      "Epoch [50/100], Loss: 130.5862\n",
      "Epoch [51/100], Loss: 133.3423\n",
      "Epoch [52/100], Loss: 127.3418\n",
      "Epoch [53/100], Loss: 120.5642\n",
      "Epoch [54/100], Loss: 123.0034\n",
      "Epoch [55/100], Loss: 117.5615\n",
      "Epoch [56/100], Loss: 123.3394\n",
      "Epoch [57/100], Loss: 125.3794\n",
      "Epoch [58/100], Loss: 117.6595\n",
      "Epoch [59/100], Loss: 111.7398\n",
      "Epoch [60/100], Loss: 114.5904\n",
      "Epoch [61/100], Loss: 109.1827\n",
      "Epoch [62/100], Loss: 111.7303\n",
      "Epoch [63/100], Loss: 111.9213\n",
      "Epoch [64/100], Loss: 112.5344\n",
      "Epoch [65/100], Loss: 111.6948\n",
      "Epoch [66/100], Loss: 105.9371\n",
      "Epoch [67/100], Loss: 111.8486\n",
      "Epoch [68/100], Loss: 104.0171\n",
      "Epoch [69/100], Loss: 110.7378\n",
      "Epoch [70/100], Loss: 103.7964\n",
      "Epoch [71/100], Loss: 101.1672\n",
      "Epoch [72/100], Loss: 104.6404\n",
      "Epoch [73/100], Loss: 108.5471\n",
      "Epoch [74/100], Loss: 110.3715\n",
      "Epoch [75/100], Loss: 102.2601\n",
      "Epoch [76/100], Loss: 98.5156\n",
      "Epoch [77/100], Loss: 103.5736\n",
      "Epoch [78/100], Loss: 98.1637\n",
      "Epoch [79/100], Loss: 98.0560\n",
      "Epoch [80/100], Loss: 99.0028\n",
      "Epoch [81/100], Loss: 100.3150\n",
      "Epoch [82/100], Loss: 99.6098\n",
      "Epoch [83/100], Loss: 94.4432\n",
      "Epoch [84/100], Loss: 99.7224\n",
      "Epoch [85/100], Loss: 96.4625\n",
      "Epoch [86/100], Loss: 92.0178\n",
      "Epoch [87/100], Loss: 91.8874\n",
      "Epoch [88/100], Loss: 101.1095\n",
      "Epoch [89/100], Loss: 95.6020\n",
      "Epoch [90/100], Loss: 99.0696\n",
      "Epoch [91/100], Loss: 94.9475\n",
      "Epoch [92/100], Loss: 103.4994\n",
      "Epoch [93/100], Loss: 94.1877\n",
      "Epoch [94/100], Loss: 91.1825\n",
      "Epoch [95/100], Loss: 97.6627\n",
      "Epoch [96/100], Loss: 89.7183\n",
      "Epoch [97/100], Loss: 92.0088\n",
      "Epoch [98/100], Loss: 90.7269\n",
      "Epoch [99/100], Loss: 101.1382\n",
      "Epoch [100/100], Loss: 96.7704\n",
      "Validation RMSE: 76.81359100341797\n",
      "Iteration 37: Training NN with optimizer=sgd, hidden_size=64, lr=0.0001, batch_size=16, n_epochs=150\n",
      "Epoch [1/150], Loss: 581.6092\n",
      "Epoch [2/150], Loss: 558.8253\n",
      "Epoch [3/150], Loss: 504.1383\n",
      "Epoch [4/150], Loss: 448.3854\n",
      "Epoch [5/150], Loss: 392.1911\n",
      "Epoch [6/150], Loss: 333.8358\n",
      "Epoch [7/150], Loss: 280.9356\n",
      "Epoch [8/150], Loss: 220.1804\n",
      "Epoch [9/150], Loss: 184.9541\n",
      "Epoch [10/150], Loss: 147.6578\n",
      "Epoch [11/150], Loss: 125.4617\n",
      "Epoch [12/150], Loss: 113.4920\n",
      "Epoch [13/150], Loss: 106.2138\n",
      "Epoch [14/150], Loss: 103.4979\n",
      "Epoch [15/150], Loss: 99.4471\n",
      "Epoch [16/150], Loss: 102.6810\n",
      "Epoch [17/150], Loss: 97.5898\n",
      "Epoch [18/150], Loss: 93.3217\n",
      "Epoch [19/150], Loss: 96.6083\n",
      "Epoch [20/150], Loss: 95.3880\n",
      "Epoch [21/150], Loss: 88.3929\n",
      "Epoch [22/150], Loss: 86.4701\n",
      "Epoch [23/150], Loss: 84.8214\n",
      "Epoch [24/150], Loss: 83.7479\n",
      "Epoch [25/150], Loss: 82.8356\n",
      "Epoch [26/150], Loss: 80.6412\n",
      "Epoch [27/150], Loss: 88.3100\n",
      "Epoch [28/150], Loss: 78.2441\n",
      "Epoch [29/150], Loss: 78.1523\n",
      "Epoch [30/150], Loss: 75.4752\n",
      "Epoch [31/150], Loss: 74.1727\n",
      "Epoch [32/150], Loss: 78.2450\n",
      "Epoch [33/150], Loss: 72.6461\n",
      "Epoch [34/150], Loss: 70.7091\n",
      "Epoch [35/150], Loss: 70.2954\n",
      "Epoch [36/150], Loss: 71.5795\n",
      "Epoch [37/150], Loss: 68.1784\n",
      "Epoch [38/150], Loss: 67.7021\n",
      "Epoch [39/150], Loss: 65.7688\n",
      "Epoch [40/150], Loss: 64.9436\n",
      "Epoch [41/150], Loss: 65.2170\n",
      "Epoch [42/150], Loss: 63.2050\n",
      "Epoch [43/150], Loss: 62.5777\n",
      "Epoch [44/150], Loss: 66.7627\n",
      "Epoch [45/150], Loss: 64.7792\n",
      "Epoch [46/150], Loss: 62.0117\n",
      "Epoch [47/150], Loss: 59.6181\n",
      "Epoch [48/150], Loss: 59.7407\n",
      "Epoch [49/150], Loss: 64.3665\n",
      "Epoch [50/150], Loss: 58.4010\n",
      "Epoch [51/150], Loss: 57.5927\n",
      "Epoch [52/150], Loss: 60.2481\n",
      "Epoch [53/150], Loss: 56.4886\n",
      "Epoch [54/150], Loss: 58.9051\n",
      "Epoch [55/150], Loss: 55.2760\n",
      "Epoch [56/150], Loss: 54.7013\n",
      "Epoch [57/150], Loss: 54.6670\n",
      "Epoch [58/150], Loss: 54.2964\n",
      "Epoch [59/150], Loss: 53.3198\n",
      "Epoch [60/150], Loss: 53.2801\n",
      "Epoch [61/150], Loss: 52.7592\n",
      "Epoch [62/150], Loss: 52.0887\n",
      "Epoch [63/150], Loss: 52.3095\n",
      "Epoch [64/150], Loss: 51.3673\n",
      "Epoch [65/150], Loss: 51.8421\n",
      "Epoch [66/150], Loss: 51.0219\n",
      "Epoch [67/150], Loss: 50.4664\n",
      "Epoch [68/150], Loss: 52.6203\n",
      "Epoch [69/150], Loss: 50.8354\n",
      "Epoch [70/150], Loss: 49.7417\n",
      "Epoch [71/150], Loss: 50.2996\n",
      "Epoch [72/150], Loss: 49.7734\n",
      "Epoch [73/150], Loss: 48.9419\n",
      "Epoch [74/150], Loss: 49.3624\n",
      "Epoch [75/150], Loss: 48.7379\n",
      "Epoch [76/150], Loss: 48.6277\n",
      "Epoch [77/150], Loss: 49.7300\n",
      "Epoch [78/150], Loss: 50.3277\n",
      "Epoch [79/150], Loss: 47.9810\n",
      "Epoch [80/150], Loss: 47.8122\n",
      "Epoch [81/150], Loss: 47.3194\n",
      "Epoch [82/150], Loss: 50.3010\n",
      "Epoch [83/150], Loss: 48.1074\n",
      "Epoch [84/150], Loss: 46.6839\n",
      "Epoch [85/150], Loss: 46.5146\n",
      "Epoch [86/150], Loss: 46.9498\n",
      "Epoch [87/150], Loss: 46.0074\n",
      "Epoch [88/150], Loss: 46.1071\n",
      "Epoch [89/150], Loss: 47.6910\n",
      "Epoch [90/150], Loss: 45.4534\n",
      "Epoch [91/150], Loss: 47.4359\n",
      "Epoch [92/150], Loss: 45.5745\n",
      "Epoch [93/150], Loss: 46.4436\n",
      "Epoch [94/150], Loss: 45.8271\n",
      "Epoch [95/150], Loss: 44.7168\n",
      "Epoch [96/150], Loss: 44.3749\n",
      "Epoch [97/150], Loss: 44.6900\n",
      "Epoch [98/150], Loss: 44.0350\n",
      "Epoch [99/150], Loss: 43.9369\n",
      "Epoch [100/150], Loss: 45.0148\n",
      "Epoch [101/150], Loss: 44.6214\n",
      "Epoch [102/150], Loss: 43.6058\n",
      "Epoch [103/150], Loss: 43.4273\n",
      "Epoch [104/150], Loss: 44.1348\n",
      "Epoch [105/150], Loss: 42.8230\n",
      "Epoch [106/150], Loss: 43.8608\n",
      "Epoch [107/150], Loss: 43.6295\n",
      "Epoch [108/150], Loss: 42.8563\n",
      "Epoch [109/150], Loss: 43.5539\n",
      "Epoch [110/150], Loss: 42.2888\n",
      "Epoch [111/150], Loss: 42.2051\n",
      "Epoch [112/150], Loss: 41.9075\n",
      "Epoch [113/150], Loss: 45.2908\n",
      "Epoch [114/150], Loss: 42.0605\n",
      "Epoch [115/150], Loss: 41.8971\n",
      "Epoch [116/150], Loss: 42.8199\n",
      "Epoch [117/150], Loss: 45.4218\n",
      "Epoch [118/150], Loss: 41.9863\n",
      "Epoch [119/150], Loss: 40.8987\n",
      "Epoch [120/150], Loss: 41.4277\n",
      "Epoch [121/150], Loss: 40.8564\n",
      "Epoch [122/150], Loss: 42.9228\n",
      "Epoch [123/150], Loss: 41.7349\n",
      "Epoch [124/150], Loss: 40.2934\n",
      "Epoch [125/150], Loss: 40.3401\n",
      "Epoch [126/150], Loss: 40.8867\n",
      "Epoch [127/150], Loss: 40.3754\n",
      "Epoch [128/150], Loss: 40.4839\n",
      "Epoch [129/150], Loss: 39.5954\n",
      "Epoch [130/150], Loss: 40.1628\n",
      "Epoch [131/150], Loss: 40.0497\n",
      "Epoch [132/150], Loss: 39.4275\n",
      "Epoch [133/150], Loss: 39.8343\n",
      "Epoch [134/150], Loss: 39.0439\n",
      "Epoch [135/150], Loss: 39.6187\n",
      "Epoch [136/150], Loss: 39.0299\n",
      "Epoch [137/150], Loss: 39.5783\n",
      "Epoch [138/150], Loss: 38.6118\n",
      "Epoch [139/150], Loss: 38.8075\n",
      "Epoch [140/150], Loss: 38.6943\n",
      "Epoch [141/150], Loss: 39.0987\n",
      "Epoch [142/150], Loss: 38.4693\n",
      "Epoch [143/150], Loss: 38.5759\n",
      "Epoch [144/150], Loss: 38.0158\n",
      "Epoch [145/150], Loss: 38.8240\n",
      "Epoch [146/150], Loss: 38.6563\n",
      "Epoch [147/150], Loss: 38.3860\n",
      "Epoch [148/150], Loss: 38.7420\n",
      "Epoch [149/150], Loss: 37.9422\n",
      "Epoch [150/150], Loss: 37.5386\n",
      "Validation RMSE: 23.350383145468577\n",
      "Iteration 38: Training NN with optimizer=sgd, hidden_size=32, lr=0.01, batch_size=64, n_epochs=150\n",
      "Epoch [1/150], Loss: 305.2806\n",
      "Epoch [2/150], Loss: 77.2363\n",
      "Epoch [3/150], Loss: 57.6964\n",
      "Epoch [4/150], Loss: 52.9834\n",
      "Epoch [5/150], Loss: 45.0108\n",
      "Epoch [6/150], Loss: 48.8948\n",
      "Epoch [7/150], Loss: 74.8582\n",
      "Epoch [8/150], Loss: 38.9046\n",
      "Epoch [9/150], Loss: 36.1769\n",
      "Epoch [10/150], Loss: 30.2983\n",
      "Epoch [11/150], Loss: 60.7015\n",
      "Epoch [12/150], Loss: 30.7109\n",
      "Epoch [13/150], Loss: 29.8740\n",
      "Epoch [14/150], Loss: 44.5410\n",
      "Epoch [15/150], Loss: 42.4108\n",
      "Epoch [16/150], Loss: 39.2515\n",
      "Epoch [17/150], Loss: 53.9501\n",
      "Epoch [18/150], Loss: 27.3121\n",
      "Epoch [19/150], Loss: 35.5227\n",
      "Epoch [20/150], Loss: 30.8502\n",
      "Epoch [21/150], Loss: 32.8034\n",
      "Epoch [22/150], Loss: 33.3357\n",
      "Epoch [23/150], Loss: 41.7603\n",
      "Epoch [24/150], Loss: 26.0043\n",
      "Epoch [25/150], Loss: 27.0046\n",
      "Epoch [26/150], Loss: 34.0067\n",
      "Epoch [27/150], Loss: 26.0107\n",
      "Epoch [28/150], Loss: 29.8653\n",
      "Epoch [29/150], Loss: 37.0360\n",
      "Epoch [30/150], Loss: 29.2194\n",
      "Epoch [31/150], Loss: 25.5024\n",
      "Epoch [32/150], Loss: 37.7206\n",
      "Epoch [33/150], Loss: 33.0362\n",
      "Epoch [34/150], Loss: 26.2928\n",
      "Epoch [35/150], Loss: 23.0235\n",
      "Epoch [36/150], Loss: 21.2306\n",
      "Epoch [37/150], Loss: 39.0097\n",
      "Epoch [38/150], Loss: 23.5179\n",
      "Epoch [39/150], Loss: 25.1210\n",
      "Epoch [40/150], Loss: 29.4048\n",
      "Epoch [41/150], Loss: 23.7469\n",
      "Epoch [42/150], Loss: 22.7262\n",
      "Epoch [43/150], Loss: 28.1082\n",
      "Epoch [44/150], Loss: 31.0607\n",
      "Epoch [45/150], Loss: 23.7746\n",
      "Epoch [46/150], Loss: 31.0802\n",
      "Epoch [47/150], Loss: 21.4075\n",
      "Epoch [48/150], Loss: 21.7710\n",
      "Epoch [49/150], Loss: 36.1624\n",
      "Epoch [50/150], Loss: 28.7743\n",
      "Epoch [51/150], Loss: 29.5797\n",
      "Epoch [52/150], Loss: 33.7122\n",
      "Epoch [53/150], Loss: 23.7524\n",
      "Epoch [54/150], Loss: 31.1194\n",
      "Epoch [55/150], Loss: 27.4261\n",
      "Epoch [56/150], Loss: 26.3213\n",
      "Epoch [57/150], Loss: 26.0975\n",
      "Epoch [58/150], Loss: 22.0039\n",
      "Epoch [59/150], Loss: 18.8573\n",
      "Epoch [60/150], Loss: 19.2080\n",
      "Epoch [61/150], Loss: 19.3017\n",
      "Epoch [62/150], Loss: 19.2968\n",
      "Epoch [63/150], Loss: 22.2531\n",
      "Epoch [64/150], Loss: 18.3975\n",
      "Epoch [65/150], Loss: 20.4886\n",
      "Epoch [66/150], Loss: 23.2292\n",
      "Epoch [67/150], Loss: 19.6270\n",
      "Epoch [68/150], Loss: 20.7274\n",
      "Epoch [69/150], Loss: 18.7763\n",
      "Epoch [70/150], Loss: 21.0044\n",
      "Epoch [71/150], Loss: 22.9057\n",
      "Epoch [72/150], Loss: 18.5721\n",
      "Epoch [73/150], Loss: 22.9468\n",
      "Epoch [74/150], Loss: 17.6882\n",
      "Epoch [75/150], Loss: 17.2317\n",
      "Epoch [76/150], Loss: 17.4439\n",
      "Epoch [77/150], Loss: 23.4264\n",
      "Epoch [78/150], Loss: 19.3893\n",
      "Epoch [79/150], Loss: 20.5092\n",
      "Epoch [80/150], Loss: 18.3818\n",
      "Epoch [81/150], Loss: 18.6830\n",
      "Epoch [82/150], Loss: 37.9696\n",
      "Epoch [83/150], Loss: 22.8129\n",
      "Epoch [84/150], Loss: 17.5772\n",
      "Epoch [85/150], Loss: 17.5428\n",
      "Epoch [86/150], Loss: 16.6897\n",
      "Epoch [87/150], Loss: 17.4975\n",
      "Epoch [88/150], Loss: 17.0264\n",
      "Epoch [89/150], Loss: 22.3826\n",
      "Epoch [90/150], Loss: 17.3637\n",
      "Epoch [91/150], Loss: 15.9599\n",
      "Epoch [92/150], Loss: 19.3289\n",
      "Epoch [93/150], Loss: 18.1834\n",
      "Epoch [94/150], Loss: 22.8159\n",
      "Epoch [95/150], Loss: 21.6003\n",
      "Epoch [96/150], Loss: 23.8938\n",
      "Epoch [97/150], Loss: 21.4605\n",
      "Epoch [98/150], Loss: 17.6946\n",
      "Epoch [99/150], Loss: 26.6459\n",
      "Epoch [100/150], Loss: 16.1171\n",
      "Epoch [101/150], Loss: 16.0672\n",
      "Epoch [102/150], Loss: 17.2494\n",
      "Epoch [103/150], Loss: 19.4428\n",
      "Epoch [104/150], Loss: 20.8139\n",
      "Epoch [105/150], Loss: 15.1795\n",
      "Epoch [106/150], Loss: 18.8569\n",
      "Epoch [107/150], Loss: 15.5410\n",
      "Epoch [108/150], Loss: 15.7662\n",
      "Epoch [109/150], Loss: 20.1443\n",
      "Epoch [110/150], Loss: 29.2370\n",
      "Epoch [111/150], Loss: 17.1625\n",
      "Epoch [112/150], Loss: 17.6707\n",
      "Epoch [113/150], Loss: 17.9153\n",
      "Epoch [114/150], Loss: 20.5674\n",
      "Epoch [115/150], Loss: 21.3889\n",
      "Epoch [116/150], Loss: 20.4728\n",
      "Epoch [117/150], Loss: 23.8761\n",
      "Epoch [118/150], Loss: 24.1889\n",
      "Epoch [119/150], Loss: 17.0779\n",
      "Epoch [120/150], Loss: 14.4270\n",
      "Epoch [121/150], Loss: 16.1989\n",
      "Epoch [122/150], Loss: 15.7833\n",
      "Epoch [123/150], Loss: 15.2150\n",
      "Epoch [124/150], Loss: 14.7442\n",
      "Epoch [125/150], Loss: 18.2860\n",
      "Epoch [126/150], Loss: 24.0193\n",
      "Epoch [127/150], Loss: 21.3804\n",
      "Epoch [128/150], Loss: 16.8874\n",
      "Epoch [129/150], Loss: 26.8396\n",
      "Epoch [130/150], Loss: 26.7462\n",
      "Epoch [131/150], Loss: 22.6721\n",
      "Epoch [132/150], Loss: 14.2150\n",
      "Epoch [133/150], Loss: 16.4108\n",
      "Epoch [134/150], Loss: 14.5351\n",
      "Epoch [135/150], Loss: 14.7311\n",
      "Epoch [136/150], Loss: 14.2989\n",
      "Epoch [137/150], Loss: 15.9428\n",
      "Epoch [138/150], Loss: 16.7853\n",
      "Epoch [139/150], Loss: 14.3280\n",
      "Epoch [140/150], Loss: 15.8164\n",
      "Epoch [141/150], Loss: 14.2522\n",
      "Epoch [142/150], Loss: 16.2488\n",
      "Epoch [143/150], Loss: 13.7032\n",
      "Epoch [144/150], Loss: 16.2758\n",
      "Epoch [145/150], Loss: 16.3901\n",
      "Epoch [146/150], Loss: 21.3517\n",
      "Epoch [147/150], Loss: 13.4472\n",
      "Epoch [148/150], Loss: 13.9734\n",
      "Epoch [149/150], Loss: 17.9675\n",
      "Epoch [150/150], Loss: 18.2647\n",
      "Validation RMSE: 16.451117515563965\n",
      "Iteration 39: Training NN with optimizer=sgd, hidden_size=128, lr=0.001, batch_size=16, n_epochs=150\n",
      "Epoch [1/150], Loss: 371.6043\n",
      "Epoch [2/150], Loss: 112.8404\n",
      "Epoch [3/150], Loss: 88.9924\n",
      "Epoch [4/150], Loss: 72.5618\n",
      "Epoch [5/150], Loss: 63.9852\n",
      "Epoch [6/150], Loss: 57.7165\n",
      "Epoch [7/150], Loss: 57.2009\n",
      "Epoch [8/150], Loss: 50.4412\n",
      "Epoch [9/150], Loss: 47.9289\n",
      "Epoch [10/150], Loss: 46.5208\n",
      "Epoch [11/150], Loss: 45.1730\n",
      "Epoch [12/150], Loss: 42.2284\n",
      "Epoch [13/150], Loss: 45.5033\n",
      "Epoch [14/150], Loss: 40.6356\n",
      "Epoch [15/150], Loss: 38.8273\n",
      "Epoch [16/150], Loss: 37.3533\n",
      "Epoch [17/150], Loss: 35.9325\n",
      "Epoch [18/150], Loss: 34.8642\n",
      "Epoch [19/150], Loss: 34.8538\n",
      "Epoch [20/150], Loss: 33.5763\n",
      "Epoch [21/150], Loss: 32.1388\n",
      "Epoch [22/150], Loss: 31.6573\n",
      "Epoch [23/150], Loss: 30.6329\n",
      "Epoch [24/150], Loss: 30.4659\n",
      "Epoch [25/150], Loss: 29.5057\n",
      "Epoch [26/150], Loss: 30.1910\n",
      "Epoch [27/150], Loss: 28.3114\n",
      "Epoch [28/150], Loss: 29.7473\n",
      "Epoch [29/150], Loss: 27.1352\n",
      "Epoch [30/150], Loss: 27.7488\n",
      "Epoch [31/150], Loss: 26.7916\n",
      "Epoch [32/150], Loss: 26.6079\n",
      "Epoch [33/150], Loss: 25.8837\n",
      "Epoch [34/150], Loss: 25.3358\n",
      "Epoch [35/150], Loss: 25.2533\n",
      "Epoch [36/150], Loss: 24.8160\n",
      "Epoch [37/150], Loss: 24.4718\n",
      "Epoch [38/150], Loss: 25.6233\n",
      "Epoch [39/150], Loss: 25.1767\n",
      "Epoch [40/150], Loss: 24.0485\n",
      "Epoch [41/150], Loss: 23.5434\n",
      "Epoch [42/150], Loss: 24.3285\n",
      "Epoch [43/150], Loss: 23.2930\n",
      "Epoch [44/150], Loss: 25.5261\n",
      "Epoch [45/150], Loss: 23.5964\n",
      "Epoch [46/150], Loss: 23.2699\n",
      "Epoch [47/150], Loss: 23.4395\n",
      "Epoch [48/150], Loss: 22.5814\n",
      "Epoch [49/150], Loss: 22.5294\n",
      "Epoch [50/150], Loss: 28.2121\n",
      "Epoch [51/150], Loss: 23.5461\n",
      "Epoch [52/150], Loss: 22.0530\n",
      "Epoch [53/150], Loss: 22.2199\n",
      "Epoch [54/150], Loss: 21.7191\n",
      "Epoch [55/150], Loss: 22.2175\n",
      "Epoch [56/150], Loss: 21.6274\n",
      "Epoch [57/150], Loss: 21.5670\n",
      "Epoch [58/150], Loss: 22.1017\n",
      "Epoch [59/150], Loss: 21.7581\n",
      "Epoch [60/150], Loss: 21.3286\n",
      "Epoch [61/150], Loss: 21.5036\n",
      "Epoch [62/150], Loss: 21.6025\n",
      "Epoch [63/150], Loss: 21.1942\n",
      "Epoch [64/150], Loss: 21.0462\n",
      "Epoch [65/150], Loss: 21.2690\n",
      "Epoch [66/150], Loss: 20.8845\n",
      "Epoch [67/150], Loss: 22.2041\n",
      "Epoch [68/150], Loss: 21.0286\n",
      "Epoch [69/150], Loss: 21.4554\n",
      "Epoch [70/150], Loss: 20.8090\n",
      "Epoch [71/150], Loss: 21.0736\n",
      "Epoch [72/150], Loss: 20.4475\n",
      "Epoch [73/150], Loss: 21.7789\n",
      "Epoch [74/150], Loss: 20.3242\n",
      "Epoch [75/150], Loss: 20.6731\n",
      "Epoch [76/150], Loss: 20.0583\n",
      "Epoch [77/150], Loss: 20.9294\n",
      "Epoch [78/150], Loss: 20.0500\n",
      "Epoch [79/150], Loss: 20.0408\n",
      "Epoch [80/150], Loss: 20.0213\n",
      "Epoch [81/150], Loss: 22.4629\n",
      "Epoch [82/150], Loss: 19.9121\n",
      "Epoch [83/150], Loss: 19.9474\n",
      "Epoch [84/150], Loss: 20.1547\n",
      "Epoch [85/150], Loss: 19.8352\n",
      "Epoch [86/150], Loss: 20.4005\n",
      "Epoch [87/150], Loss: 19.8479\n",
      "Epoch [88/150], Loss: 19.3866\n",
      "Epoch [89/150], Loss: 20.0725\n",
      "Epoch [90/150], Loss: 20.1939\n",
      "Epoch [91/150], Loss: 19.8278\n",
      "Epoch [92/150], Loss: 19.7611\n",
      "Epoch [93/150], Loss: 19.9260\n",
      "Epoch [94/150], Loss: 19.2115\n",
      "Epoch [95/150], Loss: 19.2637\n",
      "Epoch [96/150], Loss: 18.7281\n",
      "Epoch [97/150], Loss: 19.3315\n",
      "Epoch [98/150], Loss: 19.2951\n",
      "Epoch [99/150], Loss: 18.8773\n",
      "Epoch [100/150], Loss: 18.9445\n",
      "Epoch [101/150], Loss: 19.2153\n",
      "Epoch [102/150], Loss: 19.1795\n",
      "Epoch [103/150], Loss: 18.5174\n",
      "Epoch [104/150], Loss: 18.8704\n",
      "Epoch [105/150], Loss: 18.8525\n",
      "Epoch [106/150], Loss: 18.8315\n",
      "Epoch [107/150], Loss: 19.2408\n",
      "Epoch [108/150], Loss: 18.5168\n",
      "Epoch [109/150], Loss: 18.6716\n",
      "Epoch [110/150], Loss: 18.8901\n",
      "Epoch [111/150], Loss: 18.3755\n",
      "Epoch [112/150], Loss: 18.3387\n",
      "Epoch [113/150], Loss: 18.4713\n",
      "Epoch [114/150], Loss: 18.1039\n",
      "Epoch [115/150], Loss: 18.8966\n",
      "Epoch [116/150], Loss: 18.3914\n",
      "Epoch [117/150], Loss: 18.1647\n",
      "Epoch [118/150], Loss: 18.6506\n",
      "Epoch [119/150], Loss: 18.1405\n",
      "Epoch [120/150], Loss: 18.3108\n",
      "Epoch [121/150], Loss: 18.5480\n",
      "Epoch [122/150], Loss: 17.8074\n",
      "Epoch [123/150], Loss: 19.9742\n",
      "Epoch [124/150], Loss: 18.3885\n",
      "Epoch [125/150], Loss: 17.7026\n",
      "Epoch [126/150], Loss: 17.7121\n",
      "Epoch [127/150], Loss: 17.5747\n",
      "Epoch [128/150], Loss: 18.0418\n",
      "Epoch [129/150], Loss: 18.0471\n",
      "Epoch [130/150], Loss: 17.6393\n",
      "Epoch [131/150], Loss: 17.4104\n",
      "Epoch [132/150], Loss: 18.3754\n",
      "Epoch [133/150], Loss: 17.4619\n",
      "Epoch [134/150], Loss: 17.4517\n",
      "Epoch [135/150], Loss: 17.5346\n",
      "Epoch [136/150], Loss: 17.5096\n",
      "Epoch [137/150], Loss: 17.2385\n",
      "Epoch [138/150], Loss: 18.2170\n",
      "Epoch [139/150], Loss: 18.5252\n",
      "Epoch [140/150], Loss: 17.2137\n",
      "Epoch [141/150], Loss: 18.4954\n",
      "Epoch [142/150], Loss: 17.4902\n",
      "Epoch [143/150], Loss: 17.0650\n",
      "Epoch [144/150], Loss: 17.0015\n",
      "Epoch [145/150], Loss: 16.6789\n",
      "Epoch [146/150], Loss: 17.2294\n",
      "Epoch [147/150], Loss: 16.8385\n",
      "Epoch [148/150], Loss: 17.6618\n",
      "Epoch [149/150], Loss: 17.0307\n",
      "Epoch [150/150], Loss: 17.0215\n",
      "Validation RMSE: 9.34587390082223\n",
      "Iteration 40: Training NN with optimizer=sgd, hidden_size=128, lr=0.01, batch_size=16, n_epochs=50\n",
      "Epoch [1/50], Loss: 130.5056\n",
      "Epoch [2/50], Loss: 58.7802\n",
      "Epoch [3/50], Loss: 35.4104\n",
      "Epoch [4/50], Loss: 30.4290\n",
      "Epoch [5/50], Loss: 38.4837\n",
      "Epoch [6/50], Loss: 43.0486\n",
      "Epoch [7/50], Loss: 34.7433\n",
      "Epoch [8/50], Loss: 34.0911\n",
      "Epoch [9/50], Loss: 36.6289\n",
      "Epoch [10/50], Loss: 31.1721\n",
      "Epoch [11/50], Loss: 31.0141\n",
      "Epoch [12/50], Loss: 27.7790\n",
      "Epoch [13/50], Loss: 34.0545\n",
      "Epoch [14/50], Loss: 30.1009\n",
      "Epoch [15/50], Loss: 30.6284\n",
      "Epoch [16/50], Loss: 27.4638\n",
      "Epoch [17/50], Loss: 25.8504\n",
      "Epoch [18/50], Loss: 24.2158\n",
      "Epoch [19/50], Loss: 26.4635\n",
      "Epoch [20/50], Loss: 21.1274\n",
      "Epoch [21/50], Loss: 25.7441\n",
      "Epoch [22/50], Loss: 24.1696\n",
      "Epoch [23/50], Loss: 19.2563\n",
      "Epoch [24/50], Loss: 18.9963\n",
      "Epoch [25/50], Loss: 23.9832\n",
      "Epoch [26/50], Loss: 33.3828\n",
      "Epoch [27/50], Loss: 27.2341\n",
      "Epoch [28/50], Loss: 21.0756\n",
      "Epoch [29/50], Loss: 19.0880\n",
      "Epoch [30/50], Loss: 24.5860\n",
      "Epoch [31/50], Loss: 30.2274\n",
      "Epoch [32/50], Loss: 19.3377\n",
      "Epoch [33/50], Loss: 21.0463\n",
      "Epoch [34/50], Loss: 20.3471\n",
      "Epoch [35/50], Loss: 18.6478\n",
      "Epoch [36/50], Loss: 16.4682\n",
      "Epoch [37/50], Loss: 22.1202\n",
      "Epoch [38/50], Loss: 19.2235\n",
      "Epoch [39/50], Loss: 19.1612\n",
      "Epoch [40/50], Loss: 48.8936\n",
      "Epoch [41/50], Loss: 19.2545\n",
      "Epoch [42/50], Loss: 18.8812\n",
      "Epoch [43/50], Loss: 16.5898\n",
      "Epoch [44/50], Loss: 19.4974\n",
      "Epoch [45/50], Loss: 17.2633\n",
      "Epoch [46/50], Loss: 18.0250\n",
      "Epoch [47/50], Loss: 15.7868\n",
      "Epoch [48/50], Loss: 17.4037\n",
      "Epoch [49/50], Loss: 17.3821\n",
      "Epoch [50/50], Loss: 20.9254\n",
      "Validation RMSE: 25.0001836504255\n",
      "Iteration 41: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=64, n_epochs=150\n",
      "Epoch [1/150], Loss: 571.4077\n",
      "Epoch [2/150], Loss: 499.4735\n",
      "Epoch [3/150], Loss: 404.9935\n",
      "Epoch [4/150], Loss: 270.7217\n",
      "Epoch [5/150], Loss: 151.8476\n",
      "Epoch [6/150], Loss: 107.0077\n",
      "Epoch [7/150], Loss: 100.8277\n",
      "Epoch [8/150], Loss: 94.9263\n",
      "Epoch [9/150], Loss: 97.4022\n",
      "Epoch [10/150], Loss: 87.0727\n",
      "Epoch [11/150], Loss: 83.2786\n",
      "Epoch [12/150], Loss: 79.1108\n",
      "Epoch [13/150], Loss: 80.2596\n",
      "Epoch [14/150], Loss: 70.6949\n",
      "Epoch [15/150], Loss: 70.7161\n",
      "Epoch [16/150], Loss: 71.2840\n",
      "Epoch [17/150], Loss: 63.7298\n",
      "Epoch [18/150], Loss: 62.7873\n",
      "Epoch [19/150], Loss: 66.4381\n",
      "Epoch [20/150], Loss: 64.7721\n",
      "Epoch [21/150], Loss: 59.3542\n",
      "Epoch [22/150], Loss: 54.7452\n",
      "Epoch [23/150], Loss: 55.2072\n",
      "Epoch [24/150], Loss: 56.7051\n",
      "Epoch [25/150], Loss: 49.9076\n",
      "Epoch [26/150], Loss: 49.8945\n",
      "Epoch [27/150], Loss: 52.0071\n",
      "Epoch [28/150], Loss: 49.9111\n",
      "Epoch [29/150], Loss: 50.0022\n",
      "Epoch [30/150], Loss: 46.7460\n",
      "Epoch [31/150], Loss: 48.6501\n",
      "Epoch [32/150], Loss: 52.3889\n",
      "Epoch [33/150], Loss: 50.6900\n",
      "Epoch [34/150], Loss: 45.6301\n",
      "Epoch [35/150], Loss: 48.0981\n",
      "Epoch [36/150], Loss: 43.8369\n",
      "Epoch [37/150], Loss: 54.3716\n",
      "Epoch [38/150], Loss: 43.1679\n",
      "Epoch [39/150], Loss: 45.0896\n",
      "Epoch [40/150], Loss: 45.3706\n",
      "Epoch [41/150], Loss: 47.6850\n",
      "Epoch [42/150], Loss: 42.6689\n",
      "Epoch [43/150], Loss: 44.7686\n",
      "Epoch [44/150], Loss: 40.5881\n",
      "Epoch [45/150], Loss: 41.9294\n",
      "Epoch [46/150], Loss: 43.2486\n",
      "Epoch [47/150], Loss: 39.8209\n",
      "Epoch [48/150], Loss: 43.2684\n",
      "Epoch [49/150], Loss: 47.9555\n",
      "Epoch [50/150], Loss: 40.5041\n",
      "Epoch [51/150], Loss: 38.4202\n",
      "Epoch [52/150], Loss: 38.0407\n",
      "Epoch [53/150], Loss: 38.2631\n",
      "Epoch [54/150], Loss: 38.1284\n",
      "Epoch [55/150], Loss: 37.2812\n",
      "Epoch [56/150], Loss: 40.9197\n",
      "Epoch [57/150], Loss: 37.8519\n",
      "Epoch [58/150], Loss: 37.8941\n",
      "Epoch [59/150], Loss: 39.2499\n",
      "Epoch [60/150], Loss: 35.6887\n",
      "Epoch [61/150], Loss: 39.6007\n",
      "Epoch [62/150], Loss: 40.2064\n",
      "Epoch [63/150], Loss: 35.9385\n",
      "Epoch [64/150], Loss: 35.5465\n",
      "Epoch [65/150], Loss: 36.8692\n",
      "Epoch [66/150], Loss: 38.6731\n",
      "Epoch [67/150], Loss: 35.0052\n",
      "Epoch [68/150], Loss: 34.2049\n",
      "Epoch [69/150], Loss: 35.9145\n",
      "Epoch [70/150], Loss: 35.1969\n",
      "Epoch [71/150], Loss: 34.3411\n",
      "Epoch [72/150], Loss: 34.9386\n",
      "Epoch [73/150], Loss: 31.9657\n",
      "Epoch [74/150], Loss: 32.3944\n",
      "Epoch [75/150], Loss: 33.4287\n",
      "Epoch [76/150], Loss: 31.3591\n",
      "Epoch [77/150], Loss: 31.7163\n",
      "Epoch [78/150], Loss: 33.4560\n",
      "Epoch [79/150], Loss: 36.0800\n",
      "Epoch [80/150], Loss: 30.2410\n",
      "Epoch [81/150], Loss: 31.1675\n",
      "Epoch [82/150], Loss: 32.2817\n",
      "Epoch [83/150], Loss: 29.9181\n",
      "Epoch [84/150], Loss: 29.6286\n",
      "Epoch [85/150], Loss: 32.4079\n",
      "Epoch [86/150], Loss: 29.7722\n",
      "Epoch [87/150], Loss: 30.3212\n",
      "Epoch [88/150], Loss: 30.0853\n",
      "Epoch [89/150], Loss: 28.6003\n",
      "Epoch [90/150], Loss: 30.3573\n",
      "Epoch [91/150], Loss: 28.6508\n",
      "Epoch [92/150], Loss: 30.5589\n",
      "Epoch [93/150], Loss: 31.2011\n",
      "Epoch [94/150], Loss: 35.4380\n",
      "Epoch [95/150], Loss: 33.3300\n",
      "Epoch [96/150], Loss: 30.7369\n",
      "Epoch [97/150], Loss: 33.7790\n",
      "Epoch [98/150], Loss: 28.9694\n",
      "Epoch [99/150], Loss: 29.7575\n",
      "Epoch [100/150], Loss: 31.7646\n",
      "Epoch [101/150], Loss: 26.8021\n",
      "Epoch [102/150], Loss: 27.7602\n",
      "Epoch [103/150], Loss: 30.0893\n",
      "Epoch [104/150], Loss: 27.2547\n",
      "Epoch [105/150], Loss: 27.9198\n",
      "Epoch [106/150], Loss: 31.4259\n",
      "Epoch [107/150], Loss: 27.3365\n",
      "Epoch [108/150], Loss: 28.4040\n",
      "Epoch [109/150], Loss: 26.0882\n",
      "Epoch [110/150], Loss: 26.7738\n",
      "Epoch [111/150], Loss: 30.2028\n",
      "Epoch [112/150], Loss: 26.5811\n",
      "Epoch [113/150], Loss: 26.0927\n",
      "Epoch [114/150], Loss: 26.3622\n",
      "Epoch [115/150], Loss: 26.5037\n",
      "Epoch [116/150], Loss: 25.2452\n",
      "Epoch [117/150], Loss: 29.8461\n",
      "Epoch [118/150], Loss: 25.7685\n",
      "Epoch [119/150], Loss: 26.7432\n",
      "Epoch [120/150], Loss: 25.4255\n",
      "Epoch [121/150], Loss: 26.0711\n",
      "Epoch [122/150], Loss: 25.0739\n",
      "Epoch [123/150], Loss: 28.1880\n",
      "Epoch [124/150], Loss: 24.5713\n",
      "Epoch [125/150], Loss: 24.2998\n",
      "Epoch [126/150], Loss: 26.3637\n",
      "Epoch [127/150], Loss: 25.3234\n",
      "Epoch [128/150], Loss: 25.4785\n",
      "Epoch [129/150], Loss: 24.9054\n",
      "Epoch [130/150], Loss: 26.6514\n",
      "Epoch [131/150], Loss: 28.6008\n",
      "Epoch [132/150], Loss: 27.4729\n",
      "Epoch [133/150], Loss: 25.0051\n",
      "Epoch [134/150], Loss: 28.3001\n",
      "Epoch [135/150], Loss: 26.0943\n",
      "Epoch [136/150], Loss: 23.7433\n",
      "Epoch [137/150], Loss: 26.3528\n",
      "Epoch [138/150], Loss: 26.0450\n",
      "Epoch [139/150], Loss: 25.2882\n",
      "Epoch [140/150], Loss: 23.5099\n",
      "Epoch [141/150], Loss: 23.5861\n",
      "Epoch [142/150], Loss: 25.4445\n",
      "Epoch [143/150], Loss: 23.1701\n",
      "Epoch [144/150], Loss: 25.5380\n",
      "Epoch [145/150], Loss: 24.9450\n",
      "Epoch [146/150], Loss: 24.1791\n",
      "Epoch [147/150], Loss: 26.4392\n",
      "Epoch [148/150], Loss: 25.0479\n",
      "Epoch [149/150], Loss: 24.9472\n",
      "Epoch [150/150], Loss: 23.8216\n",
      "Validation RMSE: 13.683177471160889\n",
      "Iteration 42: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=16, n_epochs=150\n",
      "Epoch [1/150], Loss: 429.8878\n",
      "Epoch [2/150], Loss: 112.2141\n",
      "Epoch [3/150], Loss: 86.6685\n",
      "Epoch [4/150], Loss: 74.5528\n",
      "Epoch [5/150], Loss: 65.8357\n",
      "Epoch [6/150], Loss: 65.8571\n",
      "Epoch [7/150], Loss: 55.3193\n",
      "Epoch [8/150], Loss: 51.9606\n",
      "Epoch [9/150], Loss: 53.3739\n",
      "Epoch [10/150], Loss: 47.9860\n",
      "Epoch [11/150], Loss: 46.9679\n",
      "Epoch [12/150], Loss: 47.4539\n",
      "Epoch [13/150], Loss: 43.6342\n",
      "Epoch [14/150], Loss: 42.4549\n",
      "Epoch [15/150], Loss: 40.9099\n",
      "Epoch [16/150], Loss: 40.9655\n",
      "Epoch [17/150], Loss: 39.0810\n",
      "Epoch [18/150], Loss: 37.6640\n",
      "Epoch [19/150], Loss: 38.3640\n",
      "Epoch [20/150], Loss: 35.5437\n",
      "Epoch [21/150], Loss: 36.5821\n",
      "Epoch [22/150], Loss: 33.3911\n",
      "Epoch [23/150], Loss: 31.9229\n",
      "Epoch [24/150], Loss: 32.2135\n",
      "Epoch [25/150], Loss: 31.4836\n",
      "Epoch [26/150], Loss: 30.4744\n",
      "Epoch [27/150], Loss: 29.7049\n",
      "Epoch [28/150], Loss: 28.8910\n",
      "Epoch [29/150], Loss: 36.8981\n",
      "Epoch [30/150], Loss: 28.3326\n",
      "Epoch [31/150], Loss: 27.7910\n",
      "Epoch [32/150], Loss: 27.6225\n",
      "Epoch [33/150], Loss: 27.5266\n",
      "Epoch [34/150], Loss: 26.4741\n",
      "Epoch [35/150], Loss: 25.9815\n",
      "Epoch [36/150], Loss: 26.2516\n",
      "Epoch [37/150], Loss: 25.7872\n",
      "Epoch [38/150], Loss: 25.6805\n",
      "Epoch [39/150], Loss: 25.1279\n",
      "Epoch [40/150], Loss: 25.9012\n",
      "Epoch [41/150], Loss: 24.8081\n",
      "Epoch [42/150], Loss: 25.0888\n",
      "Epoch [43/150], Loss: 24.5174\n",
      "Epoch [44/150], Loss: 24.2579\n",
      "Epoch [45/150], Loss: 25.4011\n",
      "Epoch [46/150], Loss: 23.5632\n",
      "Epoch [47/150], Loss: 23.9514\n",
      "Epoch [48/150], Loss: 23.3384\n",
      "Epoch [49/150], Loss: 23.1484\n",
      "Epoch [50/150], Loss: 22.5677\n",
      "Epoch [51/150], Loss: 23.6244\n",
      "Epoch [52/150], Loss: 23.6471\n",
      "Epoch [53/150], Loss: 22.5897\n",
      "Epoch [54/150], Loss: 22.6940\n",
      "Epoch [55/150], Loss: 23.4790\n",
      "Epoch [56/150], Loss: 23.9979\n",
      "Epoch [57/150], Loss: 22.4903\n",
      "Epoch [58/150], Loss: 22.2814\n",
      "Epoch [59/150], Loss: 22.0803\n",
      "Epoch [60/150], Loss: 21.8808\n",
      "Epoch [61/150], Loss: 21.6406\n",
      "Epoch [62/150], Loss: 21.9443\n",
      "Epoch [63/150], Loss: 22.4407\n",
      "Epoch [64/150], Loss: 21.5569\n",
      "Epoch [65/150], Loss: 21.3935\n",
      "Epoch [66/150], Loss: 21.2696\n",
      "Epoch [67/150], Loss: 20.9933\n",
      "Epoch [68/150], Loss: 21.1239\n",
      "Epoch [69/150], Loss: 20.8455\n",
      "Epoch [70/150], Loss: 20.7786\n",
      "Epoch [71/150], Loss: 21.3364\n",
      "Epoch [72/150], Loss: 21.1321\n",
      "Epoch [73/150], Loss: 26.7774\n",
      "Epoch [74/150], Loss: 21.4456\n",
      "Epoch [75/150], Loss: 20.4030\n",
      "Epoch [76/150], Loss: 20.4091\n",
      "Epoch [77/150], Loss: 20.4691\n",
      "Epoch [78/150], Loss: 22.1197\n",
      "Epoch [79/150], Loss: 20.3900\n",
      "Epoch [80/150], Loss: 20.2675\n",
      "Epoch [81/150], Loss: 19.8760\n",
      "Epoch [82/150], Loss: 20.2609\n",
      "Epoch [83/150], Loss: 24.0192\n",
      "Epoch [84/150], Loss: 20.6645\n",
      "Epoch [85/150], Loss: 20.1435\n",
      "Epoch [86/150], Loss: 19.7523\n",
      "Epoch [87/150], Loss: 19.5786\n",
      "Epoch [88/150], Loss: 19.8271\n",
      "Epoch [89/150], Loss: 19.4206\n",
      "Epoch [90/150], Loss: 19.3197\n",
      "Epoch [91/150], Loss: 23.9936\n",
      "Epoch [92/150], Loss: 19.9292\n",
      "Epoch [93/150], Loss: 19.5519\n",
      "Epoch [94/150], Loss: 19.6284\n",
      "Epoch [95/150], Loss: 19.3225\n",
      "Epoch [96/150], Loss: 19.4477\n",
      "Epoch [97/150], Loss: 20.0790\n",
      "Epoch [98/150], Loss: 19.1490\n",
      "Epoch [99/150], Loss: 19.4138\n",
      "Epoch [100/150], Loss: 19.6395\n",
      "Epoch [101/150], Loss: 18.7720\n",
      "Epoch [102/150], Loss: 19.8767\n",
      "Epoch [103/150], Loss: 22.2750\n",
      "Epoch [104/150], Loss: 19.1465\n",
      "Epoch [105/150], Loss: 18.9914\n",
      "Epoch [106/150], Loss: 19.8472\n",
      "Epoch [107/150], Loss: 18.8393\n",
      "Epoch [108/150], Loss: 18.6411\n",
      "Epoch [109/150], Loss: 18.7193\n",
      "Epoch [110/150], Loss: 18.9222\n",
      "Epoch [111/150], Loss: 18.2699\n",
      "Epoch [112/150], Loss: 18.8385\n",
      "Epoch [113/150], Loss: 22.6711\n",
      "Epoch [114/150], Loss: 19.0931\n",
      "Epoch [115/150], Loss: 18.6576\n",
      "Epoch [116/150], Loss: 18.3199\n",
      "Epoch [117/150], Loss: 18.2757\n",
      "Epoch [118/150], Loss: 18.0448\n",
      "Epoch [119/150], Loss: 17.8632\n",
      "Epoch [120/150], Loss: 18.4099\n",
      "Epoch [121/150], Loss: 17.6187\n",
      "Epoch [122/150], Loss: 18.2062\n",
      "Epoch [123/150], Loss: 17.9613\n",
      "Epoch [124/150], Loss: 18.1996\n",
      "Epoch [125/150], Loss: 18.9411\n",
      "Epoch [126/150], Loss: 17.8041\n",
      "Epoch [127/150], Loss: 18.3287\n",
      "Epoch [128/150], Loss: 17.9030\n",
      "Epoch [129/150], Loss: 18.3622\n",
      "Epoch [130/150], Loss: 17.7729\n",
      "Epoch [131/150], Loss: 18.1182\n",
      "Epoch [132/150], Loss: 17.7677\n",
      "Epoch [133/150], Loss: 17.5173\n",
      "Epoch [134/150], Loss: 18.4207\n",
      "Epoch [135/150], Loss: 17.6937\n",
      "Epoch [136/150], Loss: 17.4380\n",
      "Epoch [137/150], Loss: 17.5251\n",
      "Epoch [138/150], Loss: 17.6201\n",
      "Epoch [139/150], Loss: 17.3523\n",
      "Epoch [140/150], Loss: 17.9675\n",
      "Epoch [141/150], Loss: 17.4929\n",
      "Epoch [142/150], Loss: 17.2835\n",
      "Epoch [143/150], Loss: 17.3894\n",
      "Epoch [144/150], Loss: 17.3346\n",
      "Epoch [145/150], Loss: 17.1385\n",
      "Epoch [146/150], Loss: 17.6698\n",
      "Epoch [147/150], Loss: 17.2694\n",
      "Epoch [148/150], Loss: 17.3857\n",
      "Epoch [149/150], Loss: 17.9653\n",
      "Epoch [150/150], Loss: 17.4211\n",
      "Validation RMSE: 9.558111463274274\n",
      "Iteration 43: Training NN with optimizer=sgd, hidden_size=64, lr=0.01, batch_size=64, n_epochs=150\n",
      "Epoch [1/150], Loss: 290.0626\n",
      "Epoch [2/150], Loss: 82.3870\n",
      "Epoch [3/150], Loss: 75.8621\n",
      "Epoch [4/150], Loss: 51.6953\n",
      "Epoch [5/150], Loss: 48.6633\n",
      "Epoch [6/150], Loss: 42.1791\n",
      "Epoch [7/150], Loss: 41.1553\n",
      "Epoch [8/150], Loss: 61.2728\n",
      "Epoch [9/150], Loss: 52.7718\n",
      "Epoch [10/150], Loss: 33.3357\n",
      "Epoch [11/150], Loss: 41.9633\n",
      "Epoch [12/150], Loss: 38.5701\n",
      "Epoch [13/150], Loss: 35.4855\n",
      "Epoch [14/150], Loss: 47.4078\n",
      "Epoch [15/150], Loss: 30.8357\n",
      "Epoch [16/150], Loss: 25.7015\n",
      "Epoch [17/150], Loss: 30.0771\n",
      "Epoch [18/150], Loss: 29.7568\n",
      "Epoch [19/150], Loss: 48.8563\n",
      "Epoch [20/150], Loss: 47.5762\n",
      "Epoch [21/150], Loss: 34.7083\n",
      "Epoch [22/150], Loss: 30.2358\n",
      "Epoch [23/150], Loss: 33.0276\n",
      "Epoch [24/150], Loss: 24.1990\n",
      "Epoch [25/150], Loss: 30.0605\n",
      "Epoch [26/150], Loss: 39.3385\n",
      "Epoch [27/150], Loss: 29.7657\n",
      "Epoch [28/150], Loss: 32.7292\n",
      "Epoch [29/150], Loss: 35.3305\n",
      "Epoch [30/150], Loss: 29.0619\n",
      "Epoch [31/150], Loss: 24.8733\n",
      "Epoch [32/150], Loss: 26.2352\n",
      "Epoch [33/150], Loss: 24.8363\n",
      "Epoch [34/150], Loss: 21.3155\n",
      "Epoch [35/150], Loss: 30.1288\n",
      "Epoch [36/150], Loss: 28.5867\n",
      "Epoch [37/150], Loss: 21.0370\n",
      "Epoch [38/150], Loss: 27.4298\n",
      "Epoch [39/150], Loss: 34.3575\n",
      "Epoch [40/150], Loss: 25.1307\n",
      "Epoch [41/150], Loss: 25.9997\n",
      "Epoch [42/150], Loss: 21.0439\n",
      "Epoch [43/150], Loss: 24.9638\n",
      "Epoch [44/150], Loss: 23.6363\n",
      "Epoch [45/150], Loss: 27.0187\n",
      "Epoch [46/150], Loss: 22.7291\n",
      "Epoch [47/150], Loss: 20.0133\n",
      "Epoch [48/150], Loss: 24.0204\n",
      "Epoch [49/150], Loss: 18.9787\n",
      "Epoch [50/150], Loss: 19.8361\n",
      "Epoch [51/150], Loss: 25.1766\n",
      "Epoch [52/150], Loss: 23.9378\n",
      "Epoch [53/150], Loss: 25.8549\n",
      "Epoch [54/150], Loss: 20.9383\n",
      "Epoch [55/150], Loss: 23.6831\n",
      "Epoch [56/150], Loss: 22.4598\n",
      "Epoch [57/150], Loss: 26.2733\n",
      "Epoch [58/150], Loss: 22.2019\n",
      "Epoch [59/150], Loss: 18.8702\n",
      "Epoch [60/150], Loss: 22.2952\n",
      "Epoch [61/150], Loss: 20.7602\n",
      "Epoch [62/150], Loss: 23.8731\n",
      "Epoch [63/150], Loss: 20.0257\n",
      "Epoch [64/150], Loss: 20.6647\n",
      "Epoch [65/150], Loss: 17.2229\n",
      "Epoch [66/150], Loss: 21.0199\n",
      "Epoch [67/150], Loss: 19.6964\n",
      "Epoch [68/150], Loss: 26.0406\n",
      "Epoch [69/150], Loss: 26.4486\n",
      "Epoch [70/150], Loss: 23.3562\n",
      "Epoch [71/150], Loss: 23.5327\n",
      "Epoch [72/150], Loss: 17.9265\n",
      "Epoch [73/150], Loss: 20.4922\n",
      "Epoch [74/150], Loss: 20.5751\n",
      "Epoch [75/150], Loss: 18.0200\n",
      "Epoch [76/150], Loss: 21.0281\n",
      "Epoch [77/150], Loss: 18.7074\n",
      "Epoch [78/150], Loss: 16.4197\n",
      "Epoch [79/150], Loss: 19.5637\n",
      "Epoch [80/150], Loss: 17.5494\n",
      "Epoch [81/150], Loss: 19.8412\n",
      "Epoch [82/150], Loss: 25.8172\n",
      "Epoch [83/150], Loss: 15.9501\n",
      "Epoch [84/150], Loss: 20.9460\n",
      "Epoch [85/150], Loss: 18.3512\n",
      "Epoch [86/150], Loss: 19.5639\n",
      "Epoch [87/150], Loss: 20.9970\n",
      "Epoch [88/150], Loss: 19.4013\n",
      "Epoch [89/150], Loss: 17.3089\n",
      "Epoch [90/150], Loss: 17.3519\n",
      "Epoch [91/150], Loss: 15.9085\n",
      "Epoch [92/150], Loss: 25.7498\n",
      "Epoch [93/150], Loss: 17.3254\n",
      "Epoch [94/150], Loss: 15.3725\n",
      "Epoch [95/150], Loss: 16.7292\n",
      "Epoch [96/150], Loss: 20.5168\n",
      "Epoch [97/150], Loss: 18.2938\n",
      "Epoch [98/150], Loss: 17.9058\n",
      "Epoch [99/150], Loss: 17.5949\n",
      "Epoch [100/150], Loss: 17.6752\n",
      "Epoch [101/150], Loss: 15.3554\n",
      "Epoch [102/150], Loss: 16.7916\n",
      "Epoch [103/150], Loss: 16.5011\n",
      "Epoch [104/150], Loss: 18.2961\n",
      "Epoch [105/150], Loss: 19.7615\n",
      "Epoch [106/150], Loss: 22.4024\n",
      "Epoch [107/150], Loss: 15.8050\n",
      "Epoch [108/150], Loss: 19.1059\n",
      "Epoch [109/150], Loss: 17.2664\n",
      "Epoch [110/150], Loss: 20.1259\n",
      "Epoch [111/150], Loss: 15.8643\n",
      "Epoch [112/150], Loss: 14.6127\n",
      "Epoch [113/150], Loss: 15.9640\n",
      "Epoch [114/150], Loss: 14.3838\n",
      "Epoch [115/150], Loss: 16.2231\n",
      "Epoch [116/150], Loss: 15.7735\n",
      "Epoch [117/150], Loss: 17.5187\n",
      "Epoch [118/150], Loss: 14.3511\n",
      "Epoch [119/150], Loss: 18.8164\n",
      "Epoch [120/150], Loss: 13.0808\n",
      "Epoch [121/150], Loss: 13.4954\n",
      "Epoch [122/150], Loss: 20.1465\n",
      "Epoch [123/150], Loss: 17.5185\n",
      "Epoch [124/150], Loss: 24.3832\n",
      "Epoch [125/150], Loss: 18.8079\n",
      "Epoch [126/150], Loss: 14.6564\n",
      "Epoch [127/150], Loss: 14.4331\n",
      "Epoch [128/150], Loss: 23.2469\n",
      "Epoch [129/150], Loss: 30.1818\n",
      "Epoch [130/150], Loss: 13.5588\n",
      "Epoch [131/150], Loss: 12.8563\n",
      "Epoch [132/150], Loss: 13.0034\n",
      "Epoch [133/150], Loss: 13.2771\n",
      "Epoch [134/150], Loss: 12.0392\n",
      "Epoch [135/150], Loss: 12.9578\n",
      "Epoch [136/150], Loss: 13.5978\n",
      "Epoch [137/150], Loss: 13.3310\n",
      "Epoch [138/150], Loss: 14.4463\n",
      "Epoch [139/150], Loss: 14.4067\n",
      "Epoch [140/150], Loss: 12.3056\n",
      "Epoch [141/150], Loss: 12.6297\n",
      "Epoch [142/150], Loss: 14.1470\n",
      "Epoch [143/150], Loss: 14.4480\n",
      "Epoch [144/150], Loss: 16.1596\n",
      "Epoch [145/150], Loss: 12.5891\n",
      "Epoch [146/150], Loss: 19.8283\n",
      "Epoch [147/150], Loss: 16.4904\n",
      "Epoch [148/150], Loss: 16.5026\n",
      "Epoch [149/150], Loss: 11.8639\n",
      "Epoch [150/150], Loss: 11.5846\n",
      "Validation RMSE: 7.801339626312256\n",
      "Iteration 44: Training NN with optimizer=sgd, hidden_size=128, lr=0.001, batch_size=64, n_epochs=50\n",
      "Epoch [1/50], Loss: 552.0014\n",
      "Epoch [2/50], Loss: 455.4966\n",
      "Epoch [3/50], Loss: 299.0444\n",
      "Epoch [4/50], Loss: 170.0645\n",
      "Epoch [5/50], Loss: 122.5221\n",
      "Epoch [6/50], Loss: 106.5711\n",
      "Epoch [7/50], Loss: 92.9264\n",
      "Epoch [8/50], Loss: 90.3791\n",
      "Epoch [9/50], Loss: 83.2280\n",
      "Epoch [10/50], Loss: 77.7044\n",
      "Epoch [11/50], Loss: 78.2956\n",
      "Epoch [12/50], Loss: 77.7050\n",
      "Epoch [13/50], Loss: 68.4967\n",
      "Epoch [14/50], Loss: 63.7532\n",
      "Epoch [15/50], Loss: 65.4789\n",
      "Epoch [16/50], Loss: 67.0235\n",
      "Epoch [17/50], Loss: 61.2857\n",
      "Epoch [18/50], Loss: 59.5932\n",
      "Epoch [19/50], Loss: 59.3860\n",
      "Epoch [20/50], Loss: 56.2569\n",
      "Epoch [21/50], Loss: 55.2597\n",
      "Epoch [22/50], Loss: 57.4628\n",
      "Epoch [23/50], Loss: 52.6538\n",
      "Epoch [24/50], Loss: 50.9434\n",
      "Epoch [25/50], Loss: 51.4437\n",
      "Epoch [26/50], Loss: 50.4827\n",
      "Epoch [27/50], Loss: 50.6410\n",
      "Epoch [28/50], Loss: 48.3385\n",
      "Epoch [29/50], Loss: 49.1364\n",
      "Epoch [30/50], Loss: 51.6702\n",
      "Epoch [31/50], Loss: 45.8558\n",
      "Epoch [32/50], Loss: 46.2584\n",
      "Epoch [33/50], Loss: 51.8623\n",
      "Epoch [34/50], Loss: 44.1095\n",
      "Epoch [35/50], Loss: 46.2082\n",
      "Epoch [36/50], Loss: 44.5758\n",
      "Epoch [37/50], Loss: 44.7168\n",
      "Epoch [38/50], Loss: 43.0212\n",
      "Epoch [39/50], Loss: 43.1501\n",
      "Epoch [40/50], Loss: 41.1847\n",
      "Epoch [41/50], Loss: 48.4707\n",
      "Epoch [42/50], Loss: 42.1953\n",
      "Epoch [43/50], Loss: 40.9959\n",
      "Epoch [44/50], Loss: 42.0296\n",
      "Epoch [45/50], Loss: 39.7782\n",
      "Epoch [46/50], Loss: 40.4584\n",
      "Epoch [47/50], Loss: 41.8172\n",
      "Epoch [48/50], Loss: 39.7831\n",
      "Epoch [49/50], Loss: 39.7136\n",
      "Epoch [50/50], Loss: 41.2721\n",
      "Validation RMSE: 27.499608039855957\n",
      "Iteration 45: Training NN with optimizer=sgd, hidden_size=128, lr=0.0001, batch_size=64, n_epochs=100\n",
      "Epoch [1/100], Loss: 576.0583\n",
      "Epoch [2/100], Loss: 582.5330\n",
      "Epoch [3/100], Loss: 570.2006\n",
      "Epoch [4/100], Loss: 544.7092\n",
      "Epoch [5/100], Loss: 556.7309\n",
      "Epoch [6/100], Loss: 522.8877\n",
      "Epoch [7/100], Loss: 520.4728\n",
      "Epoch [8/100], Loss: 498.5101\n",
      "Epoch [9/100], Loss: 493.7058\n",
      "Epoch [10/100], Loss: 480.3703\n",
      "Epoch [11/100], Loss: 477.4162\n",
      "Epoch [12/100], Loss: 462.2110\n",
      "Epoch [13/100], Loss: 438.7181\n",
      "Epoch [14/100], Loss: 414.0399\n",
      "Epoch [15/100], Loss: 436.1283\n",
      "Epoch [16/100], Loss: 391.6824\n",
      "Epoch [17/100], Loss: 396.0712\n",
      "Epoch [18/100], Loss: 370.9842\n",
      "Epoch [19/100], Loss: 355.3035\n",
      "Epoch [20/100], Loss: 349.5891\n",
      "Epoch [21/100], Loss: 321.7853\n",
      "Epoch [22/100], Loss: 310.0955\n",
      "Epoch [23/100], Loss: 284.4047\n",
      "Epoch [24/100], Loss: 273.1230\n",
      "Epoch [25/100], Loss: 262.5913\n",
      "Epoch [26/100], Loss: 241.0940\n",
      "Epoch [27/100], Loss: 229.5151\n",
      "Epoch [28/100], Loss: 199.3715\n",
      "Epoch [29/100], Loss: 189.8122\n",
      "Epoch [30/100], Loss: 198.6490\n",
      "Epoch [31/100], Loss: 178.6813\n",
      "Epoch [32/100], Loss: 167.0786\n",
      "Epoch [33/100], Loss: 159.7162\n",
      "Epoch [34/100], Loss: 149.2071\n",
      "Epoch [35/100], Loss: 139.2122\n",
      "Epoch [36/100], Loss: 139.9792\n",
      "Epoch [37/100], Loss: 132.8601\n",
      "Epoch [38/100], Loss: 143.2531\n",
      "Epoch [39/100], Loss: 131.4719\n",
      "Epoch [40/100], Loss: 126.2017\n",
      "Epoch [41/100], Loss: 123.8658\n",
      "Epoch [42/100], Loss: 120.9827\n",
      "Epoch [43/100], Loss: 122.8774\n",
      "Epoch [44/100], Loss: 110.9073\n",
      "Epoch [45/100], Loss: 117.4046\n",
      "Epoch [46/100], Loss: 111.9258\n",
      "Epoch [47/100], Loss: 110.7526\n",
      "Epoch [48/100], Loss: 114.4741\n",
      "Epoch [49/100], Loss: 105.2172\n",
      "Epoch [50/100], Loss: 105.2517\n",
      "Epoch [51/100], Loss: 100.3791\n",
      "Epoch [52/100], Loss: 103.7526\n",
      "Epoch [53/100], Loss: 106.8196\n",
      "Epoch [54/100], Loss: 101.7875\n",
      "Epoch [55/100], Loss: 102.9215\n",
      "Epoch [56/100], Loss: 99.0443\n",
      "Epoch [57/100], Loss: 98.2510\n",
      "Epoch [58/100], Loss: 110.0667\n",
      "Epoch [59/100], Loss: 92.4809\n",
      "Epoch [60/100], Loss: 94.5465\n",
      "Epoch [61/100], Loss: 102.6516\n",
      "Epoch [62/100], Loss: 97.8682\n",
      "Epoch [63/100], Loss: 93.3709\n",
      "Epoch [64/100], Loss: 92.2091\n",
      "Epoch [65/100], Loss: 106.7333\n",
      "Epoch [66/100], Loss: 95.4156\n",
      "Epoch [67/100], Loss: 98.2398\n",
      "Epoch [68/100], Loss: 89.7358\n",
      "Epoch [69/100], Loss: 90.2392\n",
      "Epoch [70/100], Loss: 97.0415\n",
      "Epoch [71/100], Loss: 97.8135\n",
      "Epoch [72/100], Loss: 95.5936\n",
      "Epoch [73/100], Loss: 95.5752\n",
      "Epoch [74/100], Loss: 88.5037\n",
      "Epoch [75/100], Loss: 89.4929\n",
      "Epoch [76/100], Loss: 93.0702\n",
      "Epoch [77/100], Loss: 96.2817\n",
      "Epoch [78/100], Loss: 96.3972\n",
      "Epoch [79/100], Loss: 94.4023\n",
      "Epoch [80/100], Loss: 85.8708\n",
      "Epoch [81/100], Loss: 86.1638\n",
      "Epoch [82/100], Loss: 90.4683\n",
      "Epoch [83/100], Loss: 86.6044\n",
      "Epoch [84/100], Loss: 88.0719\n",
      "Epoch [85/100], Loss: 85.7958\n",
      "Epoch [86/100], Loss: 88.5986\n",
      "Epoch [87/100], Loss: 86.6732\n",
      "Epoch [88/100], Loss: 86.8792\n",
      "Epoch [89/100], Loss: 85.9562\n",
      "Epoch [90/100], Loss: 80.5252\n",
      "Epoch [91/100], Loss: 81.1588\n",
      "Epoch [92/100], Loss: 82.3199\n",
      "Epoch [93/100], Loss: 80.4191\n",
      "Epoch [94/100], Loss: 87.3023\n",
      "Epoch [95/100], Loss: 82.0759\n",
      "Epoch [96/100], Loss: 80.7424\n",
      "Epoch [97/100], Loss: 82.7297\n",
      "Epoch [98/100], Loss: 79.8228\n",
      "Epoch [99/100], Loss: 84.4517\n",
      "Epoch [100/100], Loss: 87.9129\n",
      "Validation RMSE: 67.58391380310059\n",
      "Iteration 46: Training NN with optimizer=sgd, hidden_size=128, lr=0.0001, batch_size=32, n_epochs=150\n",
      "Epoch [1/150], Loss: 587.0906\n",
      "Epoch [2/150], Loss: 573.0955\n",
      "Epoch [3/150], Loss: 547.3144\n",
      "Epoch [4/150], Loss: 524.5204\n",
      "Epoch [5/150], Loss: 506.0604\n",
      "Epoch [6/150], Loss: 483.5873\n",
      "Epoch [7/150], Loss: 457.5442\n",
      "Epoch [8/150], Loss: 427.6618\n",
      "Epoch [9/150], Loss: 405.3883\n",
      "Epoch [10/150], Loss: 373.2796\n",
      "Epoch [11/150], Loss: 343.6813\n",
      "Epoch [12/150], Loss: 313.3846\n",
      "Epoch [13/150], Loss: 281.6081\n",
      "Epoch [14/150], Loss: 254.3322\n",
      "Epoch [15/150], Loss: 230.2857\n",
      "Epoch [16/150], Loss: 205.9557\n",
      "Epoch [17/150], Loss: 184.4631\n",
      "Epoch [18/150], Loss: 167.0679\n",
      "Epoch [19/150], Loss: 152.7660\n",
      "Epoch [20/150], Loss: 144.0739\n",
      "Epoch [21/150], Loss: 132.6580\n",
      "Epoch [22/150], Loss: 126.6774\n",
      "Epoch [23/150], Loss: 123.2451\n",
      "Epoch [24/150], Loss: 117.9539\n",
      "Epoch [25/150], Loss: 115.3331\n",
      "Epoch [26/150], Loss: 111.0305\n",
      "Epoch [27/150], Loss: 112.4479\n",
      "Epoch [28/150], Loss: 107.0888\n",
      "Epoch [29/150], Loss: 107.1712\n",
      "Epoch [30/150], Loss: 104.6595\n",
      "Epoch [31/150], Loss: 104.7660\n",
      "Epoch [32/150], Loss: 102.0309\n",
      "Epoch [33/150], Loss: 101.0938\n",
      "Epoch [34/150], Loss: 102.8033\n",
      "Epoch [35/150], Loss: 100.3435\n",
      "Epoch [36/150], Loss: 98.7642\n",
      "Epoch [37/150], Loss: 97.6650\n",
      "Epoch [38/150], Loss: 98.4589\n",
      "Epoch [39/150], Loss: 96.9735\n",
      "Epoch [40/150], Loss: 97.2563\n",
      "Epoch [41/150], Loss: 94.5518\n",
      "Epoch [42/150], Loss: 93.3912\n",
      "Epoch [43/150], Loss: 91.2965\n",
      "Epoch [44/150], Loss: 93.3809\n",
      "Epoch [45/150], Loss: 90.7130\n",
      "Epoch [46/150], Loss: 90.4649\n",
      "Epoch [47/150], Loss: 90.0603\n",
      "Epoch [48/150], Loss: 89.2230\n",
      "Epoch [49/150], Loss: 87.0201\n",
      "Epoch [50/150], Loss: 87.6839\n",
      "Epoch [51/150], Loss: 85.7693\n",
      "Epoch [52/150], Loss: 85.9734\n",
      "Epoch [53/150], Loss: 84.3304\n",
      "Epoch [54/150], Loss: 84.2785\n",
      "Epoch [55/150], Loss: 83.2539\n",
      "Epoch [56/150], Loss: 82.4602\n",
      "Epoch [57/150], Loss: 80.7335\n",
      "Epoch [58/150], Loss: 81.9707\n",
      "Epoch [59/150], Loss: 79.5729\n",
      "Epoch [60/150], Loss: 80.3335\n",
      "Epoch [61/150], Loss: 79.6715\n",
      "Epoch [62/150], Loss: 79.1337\n",
      "Epoch [63/150], Loss: 78.7711\n",
      "Epoch [64/150], Loss: 77.6905\n",
      "Epoch [65/150], Loss: 77.5940\n",
      "Epoch [66/150], Loss: 77.6740\n",
      "Epoch [67/150], Loss: 75.1175\n",
      "Epoch [68/150], Loss: 75.6212\n",
      "Epoch [69/150], Loss: 77.6329\n",
      "Epoch [70/150], Loss: 73.5347\n",
      "Epoch [71/150], Loss: 75.0353\n",
      "Epoch [72/150], Loss: 72.0502\n",
      "Epoch [73/150], Loss: 72.3622\n",
      "Epoch [74/150], Loss: 72.7806\n",
      "Epoch [75/150], Loss: 70.7268\n",
      "Epoch [76/150], Loss: 70.7872\n",
      "Epoch [77/150], Loss: 70.3567\n",
      "Epoch [78/150], Loss: 69.9354\n",
      "Epoch [79/150], Loss: 69.2101\n",
      "Epoch [80/150], Loss: 69.2942\n",
      "Epoch [81/150], Loss: 67.7690\n",
      "Epoch [82/150], Loss: 68.5094\n",
      "Epoch [83/150], Loss: 69.4336\n",
      "Epoch [84/150], Loss: 67.5756\n",
      "Epoch [85/150], Loss: 67.4419\n",
      "Epoch [86/150], Loss: 67.4720\n",
      "Epoch [87/150], Loss: 65.6195\n",
      "Epoch [88/150], Loss: 65.8407\n",
      "Epoch [89/150], Loss: 67.6338\n",
      "Epoch [90/150], Loss: 64.8777\n",
      "Epoch [91/150], Loss: 64.8618\n",
      "Epoch [92/150], Loss: 65.5020\n",
      "Epoch [93/150], Loss: 63.7760\n",
      "Epoch [94/150], Loss: 64.2043\n",
      "Epoch [95/150], Loss: 62.5957\n",
      "Epoch [96/150], Loss: 63.8842\n",
      "Epoch [97/150], Loss: 62.7265\n",
      "Epoch [98/150], Loss: 64.0293\n",
      "Epoch [99/150], Loss: 62.5183\n",
      "Epoch [100/150], Loss: 63.6006\n",
      "Epoch [101/150], Loss: 61.7611\n",
      "Epoch [102/150], Loss: 62.1381\n",
      "Epoch [103/150], Loss: 60.0510\n",
      "Epoch [104/150], Loss: 61.8606\n",
      "Epoch [105/150], Loss: 61.2978\n",
      "Epoch [106/150], Loss: 60.3938\n",
      "Epoch [107/150], Loss: 60.7475\n",
      "Epoch [108/150], Loss: 60.5602\n",
      "Epoch [109/150], Loss: 59.6896\n",
      "Epoch [110/150], Loss: 60.0468\n",
      "Epoch [111/150], Loss: 59.7790\n",
      "Epoch [112/150], Loss: 59.3828\n",
      "Epoch [113/150], Loss: 58.5392\n",
      "Epoch [114/150], Loss: 57.4335\n",
      "Epoch [115/150], Loss: 58.2748\n",
      "Epoch [116/150], Loss: 57.3536\n",
      "Epoch [117/150], Loss: 56.6871\n",
      "Epoch [118/150], Loss: 57.9110\n",
      "Epoch [119/150], Loss: 57.0172\n",
      "Epoch [120/150], Loss: 56.1923\n",
      "Epoch [121/150], Loss: 56.6733\n",
      "Epoch [122/150], Loss: 55.3491\n",
      "Epoch [123/150], Loss: 56.6202\n",
      "Epoch [124/150], Loss: 55.5497\n",
      "Epoch [125/150], Loss: 56.7249\n",
      "Epoch [126/150], Loss: 56.3798\n",
      "Epoch [127/150], Loss: 55.3104\n",
      "Epoch [128/150], Loss: 54.6676\n",
      "Epoch [129/150], Loss: 55.3945\n",
      "Epoch [130/150], Loss: 55.0653\n",
      "Epoch [131/150], Loss: 54.7513\n",
      "Epoch [132/150], Loss: 54.3404\n",
      "Epoch [133/150], Loss: 54.0851\n",
      "Epoch [134/150], Loss: 53.8818\n",
      "Epoch [135/150], Loss: 53.5950\n",
      "Epoch [136/150], Loss: 53.4348\n",
      "Epoch [137/150], Loss: 54.0229\n",
      "Epoch [138/150], Loss: 54.0823\n",
      "Epoch [139/150], Loss: 53.8646\n",
      "Epoch [140/150], Loss: 54.0664\n",
      "Epoch [141/150], Loss: 53.2940\n",
      "Epoch [142/150], Loss: 52.2363\n",
      "Epoch [143/150], Loss: 52.8345\n",
      "Epoch [144/150], Loss: 51.9951\n",
      "Epoch [145/150], Loss: 51.8056\n",
      "Epoch [146/150], Loss: 52.3733\n",
      "Epoch [147/150], Loss: 53.2158\n",
      "Epoch [148/150], Loss: 52.4983\n",
      "Epoch [149/150], Loss: 51.3862\n",
      "Epoch [150/150], Loss: 52.3210\n",
      "Validation RMSE: 33.89818286895752\n",
      "Iteration 47: Training NN with optimizer=sgd, hidden_size=32, lr=0.001, batch_size=32, n_epochs=100\n",
      "Epoch [1/100], Loss: 544.4002\n",
      "Epoch [2/100], Loss: 339.2299\n",
      "Epoch [3/100], Loss: 137.6593\n",
      "Epoch [4/100], Loss: 101.7560\n",
      "Epoch [5/100], Loss: 90.0365\n",
      "Epoch [6/100], Loss: 83.7404\n",
      "Epoch [7/100], Loss: 75.3364\n",
      "Epoch [8/100], Loss: 73.3431\n",
      "Epoch [9/100], Loss: 67.4929\n",
      "Epoch [10/100], Loss: 60.8535\n",
      "Epoch [11/100], Loss: 59.1917\n",
      "Epoch [12/100], Loss: 55.8583\n",
      "Epoch [13/100], Loss: 53.9952\n",
      "Epoch [14/100], Loss: 52.3895\n",
      "Epoch [15/100], Loss: 51.1352\n",
      "Epoch [16/100], Loss: 50.1713\n",
      "Epoch [17/100], Loss: 49.8802\n",
      "Epoch [18/100], Loss: 47.5417\n",
      "Epoch [19/100], Loss: 46.3526\n",
      "Epoch [20/100], Loss: 45.4241\n",
      "Epoch [21/100], Loss: 45.6700\n",
      "Epoch [22/100], Loss: 43.7704\n",
      "Epoch [23/100], Loss: 43.2118\n",
      "Epoch [24/100], Loss: 42.5314\n",
      "Epoch [25/100], Loss: 42.1169\n",
      "Epoch [26/100], Loss: 40.6367\n",
      "Epoch [27/100], Loss: 39.7587\n",
      "Epoch [28/100], Loss: 39.3143\n",
      "Epoch [29/100], Loss: 39.1340\n",
      "Epoch [30/100], Loss: 38.4479\n",
      "Epoch [31/100], Loss: 37.4024\n",
      "Epoch [32/100], Loss: 37.8652\n",
      "Epoch [33/100], Loss: 36.6491\n",
      "Epoch [34/100], Loss: 36.0081\n",
      "Epoch [35/100], Loss: 35.9764\n",
      "Epoch [36/100], Loss: 35.1753\n",
      "Epoch [37/100], Loss: 34.7204\n",
      "Epoch [38/100], Loss: 34.0909\n",
      "Epoch [39/100], Loss: 33.7840\n",
      "Epoch [40/100], Loss: 33.5278\n",
      "Epoch [41/100], Loss: 34.4249\n",
      "Epoch [42/100], Loss: 32.6010\n",
      "Epoch [43/100], Loss: 32.2305\n",
      "Epoch [44/100], Loss: 33.0855\n",
      "Epoch [45/100], Loss: 31.7679\n",
      "Epoch [46/100], Loss: 31.5349\n",
      "Epoch [47/100], Loss: 30.8212\n",
      "Epoch [48/100], Loss: 30.3492\n",
      "Epoch [49/100], Loss: 30.9980\n",
      "Epoch [50/100], Loss: 29.8141\n",
      "Epoch [51/100], Loss: 29.9577\n",
      "Epoch [52/100], Loss: 29.4500\n",
      "Epoch [53/100], Loss: 29.8898\n",
      "Epoch [54/100], Loss: 28.1491\n",
      "Epoch [55/100], Loss: 27.8985\n",
      "Epoch [56/100], Loss: 27.9644\n",
      "Epoch [57/100], Loss: 27.5216\n",
      "Epoch [58/100], Loss: 28.0540\n",
      "Epoch [59/100], Loss: 26.9451\n",
      "Epoch [60/100], Loss: 26.9566\n",
      "Epoch [61/100], Loss: 26.9485\n",
      "Epoch [62/100], Loss: 26.7569\n",
      "Epoch [63/100], Loss: 26.4534\n",
      "Epoch [64/100], Loss: 26.1248\n",
      "Epoch [65/100], Loss: 26.2390\n",
      "Epoch [66/100], Loss: 26.1828\n",
      "Epoch [67/100], Loss: 26.1443\n",
      "Epoch [68/100], Loss: 26.9508\n",
      "Epoch [69/100], Loss: 26.0738\n",
      "Epoch [70/100], Loss: 26.8787\n",
      "Epoch [71/100], Loss: 25.3998\n",
      "Epoch [72/100], Loss: 25.0412\n",
      "Epoch [73/100], Loss: 24.8389\n",
      "Epoch [74/100], Loss: 25.1053\n",
      "Epoch [75/100], Loss: 25.0184\n",
      "Epoch [76/100], Loss: 24.7033\n",
      "Epoch [77/100], Loss: 24.6807\n",
      "Epoch [78/100], Loss: 24.6544\n",
      "Epoch [79/100], Loss: 24.5262\n",
      "Epoch [80/100], Loss: 25.2983\n",
      "Epoch [81/100], Loss: 24.4503\n",
      "Epoch [82/100], Loss: 25.5657\n",
      "Epoch [83/100], Loss: 24.0450\n",
      "Epoch [84/100], Loss: 23.9700\n",
      "Epoch [85/100], Loss: 23.8532\n",
      "Epoch [86/100], Loss: 23.8050\n",
      "Epoch [87/100], Loss: 24.0519\n",
      "Epoch [88/100], Loss: 23.2505\n",
      "Epoch [89/100], Loss: 24.7589\n",
      "Epoch [90/100], Loss: 23.3119\n",
      "Epoch [91/100], Loss: 23.4212\n",
      "Epoch [92/100], Loss: 24.0967\n",
      "Epoch [93/100], Loss: 23.0171\n",
      "Epoch [94/100], Loss: 23.6787\n",
      "Epoch [95/100], Loss: 23.1241\n",
      "Epoch [96/100], Loss: 23.3900\n",
      "Epoch [97/100], Loss: 22.9462\n",
      "Epoch [98/100], Loss: 23.2984\n",
      "Epoch [99/100], Loss: 23.2139\n",
      "Epoch [100/100], Loss: 23.3388\n",
      "Validation RMSE: 13.143331289291382\n",
      "Iteration 48: Training NN with optimizer=sgd, hidden_size=128, lr=0.001, batch_size=16, n_epochs=150\n",
      "Epoch [1/150], Loss: 381.6396\n",
      "Epoch [2/150], Loss: 105.3064\n",
      "Epoch [3/150], Loss: 84.0866\n",
      "Epoch [4/150], Loss: 70.9921\n",
      "Epoch [5/150], Loss: 63.9288\n",
      "Epoch [6/150], Loss: 61.6901\n",
      "Epoch [7/150], Loss: 54.1175\n",
      "Epoch [8/150], Loss: 51.6512\n",
      "Epoch [9/150], Loss: 48.3315\n",
      "Epoch [10/150], Loss: 47.8531\n",
      "Epoch [11/150], Loss: 45.1997\n",
      "Epoch [12/150], Loss: 44.4429\n",
      "Epoch [13/150], Loss: 42.2407\n",
      "Epoch [14/150], Loss: 40.3739\n",
      "Epoch [15/150], Loss: 41.1534\n",
      "Epoch [16/150], Loss: 37.9382\n",
      "Epoch [17/150], Loss: 37.4239\n",
      "Epoch [18/150], Loss: 36.0826\n",
      "Epoch [19/150], Loss: 35.1593\n",
      "Epoch [20/150], Loss: 35.3444\n",
      "Epoch [21/150], Loss: 33.8358\n",
      "Epoch [22/150], Loss: 34.5521\n",
      "Epoch [23/150], Loss: 31.3769\n",
      "Epoch [24/150], Loss: 31.0537\n",
      "Epoch [25/150], Loss: 30.5671\n",
      "Epoch [26/150], Loss: 30.5906\n",
      "Epoch [27/150], Loss: 28.7846\n",
      "Epoch [28/150], Loss: 28.3870\n",
      "Epoch [29/150], Loss: 28.0592\n",
      "Epoch [30/150], Loss: 27.6291\n",
      "Epoch [31/150], Loss: 27.4629\n",
      "Epoch [32/150], Loss: 27.1905\n",
      "Epoch [33/150], Loss: 26.9049\n",
      "Epoch [34/150], Loss: 26.5019\n",
      "Epoch [35/150], Loss: 25.2815\n",
      "Epoch [36/150], Loss: 25.5888\n",
      "Epoch [37/150], Loss: 24.8531\n",
      "Epoch [38/150], Loss: 24.1345\n",
      "Epoch [39/150], Loss: 23.8771\n",
      "Epoch [40/150], Loss: 23.9012\n",
      "Epoch [41/150], Loss: 23.8830\n",
      "Epoch [42/150], Loss: 24.4755\n",
      "Epoch [43/150], Loss: 23.1864\n",
      "Epoch [44/150], Loss: 23.5675\n",
      "Epoch [45/150], Loss: 23.2520\n",
      "Epoch [46/150], Loss: 22.8066\n",
      "Epoch [47/150], Loss: 23.4008\n",
      "Epoch [48/150], Loss: 25.1049\n",
      "Epoch [49/150], Loss: 22.3379\n",
      "Epoch [50/150], Loss: 22.5377\n",
      "Epoch [51/150], Loss: 28.0046\n",
      "Epoch [52/150], Loss: 21.6664\n",
      "Epoch [53/150], Loss: 27.2854\n",
      "Epoch [54/150], Loss: 21.8730\n",
      "Epoch [55/150], Loss: 21.7142\n",
      "Epoch [56/150], Loss: 21.8753\n",
      "Epoch [57/150], Loss: 21.7834\n",
      "Epoch [58/150], Loss: 25.0406\n",
      "Epoch [59/150], Loss: 22.1794\n",
      "Epoch [60/150], Loss: 20.8228\n",
      "Epoch [61/150], Loss: 21.2169\n",
      "Epoch [62/150], Loss: 20.6443\n",
      "Epoch [63/150], Loss: 21.8827\n",
      "Epoch [64/150], Loss: 20.5078\n",
      "Epoch [65/150], Loss: 20.5257\n",
      "Epoch [66/150], Loss: 22.0643\n",
      "Epoch [67/150], Loss: 21.2476\n",
      "Epoch [68/150], Loss: 20.2683\n",
      "Epoch [69/150], Loss: 20.5173\n",
      "Epoch [70/150], Loss: 20.1053\n",
      "Epoch [71/150], Loss: 21.2095\n",
      "Epoch [72/150], Loss: 20.1492\n",
      "Epoch [73/150], Loss: 19.8425\n",
      "Epoch [74/150], Loss: 20.4973\n",
      "Epoch [75/150], Loss: 20.0205\n",
      "Epoch [76/150], Loss: 19.9843\n",
      "Epoch [77/150], Loss: 19.4682\n",
      "Epoch [78/150], Loss: 20.6510\n",
      "Epoch [79/150], Loss: 20.6218\n",
      "Epoch [80/150], Loss: 19.5050\n",
      "Epoch [81/150], Loss: 20.1949\n",
      "Epoch [82/150], Loss: 19.8535\n",
      "Epoch [83/150], Loss: 19.3262\n",
      "Epoch [84/150], Loss: 19.4677\n",
      "Epoch [85/150], Loss: 20.1898\n",
      "Epoch [86/150], Loss: 19.1304\n",
      "Epoch [87/150], Loss: 23.7439\n",
      "Epoch [88/150], Loss: 19.7301\n",
      "Epoch [89/150], Loss: 19.3537\n",
      "Epoch [90/150], Loss: 20.6574\n",
      "Epoch [91/150], Loss: 19.2537\n",
      "Epoch [92/150], Loss: 19.7105\n",
      "Epoch [93/150], Loss: 18.8262\n",
      "Epoch [94/150], Loss: 19.1874\n",
      "Epoch [95/150], Loss: 18.7741\n",
      "Epoch [96/150], Loss: 18.6576\n",
      "Epoch [97/150], Loss: 18.6682\n",
      "Epoch [98/150], Loss: 18.5816\n",
      "Epoch [99/150], Loss: 18.6481\n",
      "Epoch [100/150], Loss: 18.4067\n",
      "Epoch [101/150], Loss: 19.3442\n",
      "Epoch [102/150], Loss: 19.0740\n",
      "Epoch [103/150], Loss: 18.4946\n",
      "Epoch [104/150], Loss: 19.3246\n",
      "Epoch [105/150], Loss: 18.5047\n",
      "Epoch [106/150], Loss: 18.3502\n",
      "Epoch [107/150], Loss: 18.0969\n",
      "Epoch [108/150], Loss: 18.1786\n",
      "Epoch [109/150], Loss: 18.3856\n",
      "Epoch [110/150], Loss: 18.0981\n",
      "Epoch [111/150], Loss: 17.8144\n",
      "Epoch [112/150], Loss: 18.5685\n",
      "Epoch [113/150], Loss: 18.1961\n",
      "Epoch [114/150], Loss: 17.7146\n",
      "Epoch [115/150], Loss: 18.2773\n",
      "Epoch [116/150], Loss: 17.4885\n",
      "Epoch [117/150], Loss: 17.5706\n",
      "Epoch [118/150], Loss: 17.4466\n",
      "Epoch [119/150], Loss: 17.6852\n",
      "Epoch [120/150], Loss: 17.8260\n",
      "Epoch [121/150], Loss: 17.6557\n",
      "Epoch [122/150], Loss: 17.5570\n",
      "Epoch [123/150], Loss: 17.7304\n",
      "Epoch [124/150], Loss: 17.4715\n",
      "Epoch [125/150], Loss: 17.2206\n",
      "Epoch [126/150], Loss: 17.2446\n",
      "Epoch [127/150], Loss: 18.0680\n",
      "Epoch [128/150], Loss: 17.3747\n",
      "Epoch [129/150], Loss: 17.1079\n",
      "Epoch [130/150], Loss: 17.4200\n",
      "Epoch [131/150], Loss: 17.0958\n",
      "Epoch [132/150], Loss: 17.5316\n",
      "Epoch [133/150], Loss: 17.3169\n",
      "Epoch [134/150], Loss: 16.8807\n",
      "Epoch [135/150], Loss: 17.2977\n",
      "Epoch [136/150], Loss: 17.9121\n",
      "Epoch [137/150], Loss: 17.0123\n",
      "Epoch [138/150], Loss: 17.1235\n",
      "Epoch [139/150], Loss: 17.4969\n",
      "Epoch [140/150], Loss: 17.9753\n",
      "Epoch [141/150], Loss: 17.3543\n",
      "Epoch [142/150], Loss: 16.7224\n",
      "Epoch [143/150], Loss: 16.5163\n",
      "Epoch [144/150], Loss: 17.1106\n",
      "Epoch [145/150], Loss: 16.8947\n",
      "Epoch [146/150], Loss: 16.4619\n",
      "Epoch [147/150], Loss: 16.8349\n",
      "Epoch [148/150], Loss: 16.1965\n",
      "Epoch [149/150], Loss: 16.5606\n",
      "Epoch [150/150], Loss: 16.1536\n",
      "Validation RMSE: 9.474393027169365\n",
      "Iteration 49: Training NN with optimizer=sgd, hidden_size=64, lr=0.001, batch_size=16, n_epochs=100\n",
      "Epoch [1/100], Loss: 382.8954\n",
      "Epoch [2/100], Loss: 109.5780\n",
      "Epoch [3/100], Loss: 91.7006\n",
      "Epoch [4/100], Loss: 73.2423\n",
      "Epoch [5/100], Loss: 69.8377\n",
      "Epoch [6/100], Loss: 65.1849\n",
      "Epoch [7/100], Loss: 55.7628\n",
      "Epoch [8/100], Loss: 59.0029\n",
      "Epoch [9/100], Loss: 50.5188\n",
      "Epoch [10/100], Loss: 49.7879\n",
      "Epoch [11/100], Loss: 45.8939\n",
      "Epoch [12/100], Loss: 47.2649\n",
      "Epoch [13/100], Loss: 43.5292\n",
      "Epoch [14/100], Loss: 39.9938\n",
      "Epoch [15/100], Loss: 40.9569\n",
      "Epoch [16/100], Loss: 38.5678\n",
      "Epoch [17/100], Loss: 37.2164\n",
      "Epoch [18/100], Loss: 37.6175\n",
      "Epoch [19/100], Loss: 36.1446\n",
      "Epoch [20/100], Loss: 34.3367\n",
      "Epoch [21/100], Loss: 33.3786\n",
      "Epoch [22/100], Loss: 32.7660\n",
      "Epoch [23/100], Loss: 33.4529\n",
      "Epoch [24/100], Loss: 32.2514\n",
      "Epoch [25/100], Loss: 33.1239\n",
      "Epoch [26/100], Loss: 30.0409\n",
      "Epoch [27/100], Loss: 29.8384\n",
      "Epoch [28/100], Loss: 28.5623\n",
      "Epoch [29/100], Loss: 28.2296\n",
      "Epoch [30/100], Loss: 27.7551\n",
      "Epoch [31/100], Loss: 27.8469\n",
      "Epoch [32/100], Loss: 27.2319\n",
      "Epoch [33/100], Loss: 28.0082\n",
      "Epoch [34/100], Loss: 26.2020\n",
      "Epoch [35/100], Loss: 26.6419\n",
      "Epoch [36/100], Loss: 27.3896\n",
      "Epoch [37/100], Loss: 25.4042\n",
      "Epoch [38/100], Loss: 24.6942\n",
      "Epoch [39/100], Loss: 25.4949\n",
      "Epoch [40/100], Loss: 27.0870\n",
      "Epoch [41/100], Loss: 25.0504\n",
      "Epoch [42/100], Loss: 23.6741\n",
      "Epoch [43/100], Loss: 24.1997\n",
      "Epoch [44/100], Loss: 26.7748\n",
      "Epoch [45/100], Loss: 23.5029\n",
      "Epoch [46/100], Loss: 27.9903\n",
      "Epoch [47/100], Loss: 22.7472\n",
      "Epoch [48/100], Loss: 22.9553\n",
      "Epoch [49/100], Loss: 22.7952\n",
      "Epoch [50/100], Loss: 22.2674\n",
      "Epoch [51/100], Loss: 22.8372\n",
      "Epoch [52/100], Loss: 22.3990\n",
      "Epoch [53/100], Loss: 22.3038\n",
      "Epoch [54/100], Loss: 21.7932\n",
      "Epoch [55/100], Loss: 22.0981\n",
      "Epoch [56/100], Loss: 21.6179\n",
      "Epoch [57/100], Loss: 21.5742\n",
      "Epoch [58/100], Loss: 22.2764\n",
      "Epoch [59/100], Loss: 22.7375\n",
      "Epoch [60/100], Loss: 26.5804\n",
      "Epoch [61/100], Loss: 21.5820\n",
      "Epoch [62/100], Loss: 21.0137\n",
      "Epoch [63/100], Loss: 21.5942\n",
      "Epoch [64/100], Loss: 21.1907\n",
      "Epoch [65/100], Loss: 21.2879\n",
      "Epoch [66/100], Loss: 21.7516\n",
      "Epoch [67/100], Loss: 21.2074\n",
      "Epoch [68/100], Loss: 22.5799\n",
      "Epoch [69/100], Loss: 21.1064\n",
      "Epoch [70/100], Loss: 22.8497\n",
      "Epoch [71/100], Loss: 25.5160\n",
      "Epoch [72/100], Loss: 20.8663\n",
      "Epoch [73/100], Loss: 21.2328\n",
      "Epoch [74/100], Loss: 21.0750\n",
      "Epoch [75/100], Loss: 20.4174\n",
      "Epoch [76/100], Loss: 20.3815\n",
      "Epoch [77/100], Loss: 20.1853\n",
      "Epoch [78/100], Loss: 20.6565\n",
      "Epoch [79/100], Loss: 20.0476\n",
      "Epoch [80/100], Loss: 20.1225\n",
      "Epoch [81/100], Loss: 22.1375\n",
      "Epoch [82/100], Loss: 20.7160\n",
      "Epoch [83/100], Loss: 21.0423\n",
      "Epoch [84/100], Loss: 20.2713\n",
      "Epoch [85/100], Loss: 20.0131\n",
      "Epoch [86/100], Loss: 19.7577\n",
      "Epoch [87/100], Loss: 19.7728\n",
      "Epoch [88/100], Loss: 21.3708\n",
      "Epoch [89/100], Loss: 19.5508\n",
      "Epoch [90/100], Loss: 19.9190\n",
      "Epoch [91/100], Loss: 20.6416\n",
      "Epoch [92/100], Loss: 19.6390\n",
      "Epoch [93/100], Loss: 19.4693\n",
      "Epoch [94/100], Loss: 19.2673\n",
      "Epoch [95/100], Loss: 19.3139\n",
      "Epoch [96/100], Loss: 20.0530\n",
      "Epoch [97/100], Loss: 20.0916\n",
      "Epoch [98/100], Loss: 20.4738\n",
      "Epoch [99/100], Loss: 19.6830\n",
      "Epoch [100/100], Loss: 19.1251\n",
      "Validation RMSE: 10.214064870561872\n",
      "Iteration 50: Training NN with optimizer=sgd, hidden_size=128, lr=0.01, batch_size=32, n_epochs=100\n",
      "Epoch [1/100], Loss: 186.2526\n",
      "Epoch [2/100], Loss: 71.3979\n",
      "Epoch [3/100], Loss: 44.7060\n",
      "Epoch [4/100], Loss: 49.8968\n",
      "Epoch [5/100], Loss: 51.6253\n",
      "Epoch [6/100], Loss: 44.7710\n",
      "Epoch [7/100], Loss: 34.9611\n",
      "Epoch [8/100], Loss: 38.7971\n",
      "Epoch [9/100], Loss: 47.6061\n",
      "Epoch [10/100], Loss: 37.4468\n",
      "Epoch [11/100], Loss: 30.3048\n",
      "Epoch [12/100], Loss: 27.6011\n",
      "Epoch [13/100], Loss: 42.7492\n",
      "Epoch [14/100], Loss: 34.2566\n",
      "Epoch [15/100], Loss: 39.0087\n",
      "Epoch [16/100], Loss: 32.7149\n",
      "Epoch [17/100], Loss: 39.8089\n",
      "Epoch [18/100], Loss: 28.6172\n",
      "Epoch [19/100], Loss: 22.7351\n",
      "Epoch [20/100], Loss: 32.3516\n",
      "Epoch [21/100], Loss: 23.7875\n",
      "Epoch [22/100], Loss: 30.4507\n",
      "Epoch [23/100], Loss: 20.8241\n",
      "Epoch [24/100], Loss: 26.4894\n",
      "Epoch [25/100], Loss: 24.4900\n",
      "Epoch [26/100], Loss: 23.4285\n",
      "Epoch [27/100], Loss: 20.7276\n",
      "Epoch [28/100], Loss: 21.3384\n",
      "Epoch [29/100], Loss: 21.1030\n",
      "Epoch [30/100], Loss: 22.6349\n",
      "Epoch [31/100], Loss: 20.5837\n",
      "Epoch [32/100], Loss: 29.0031\n",
      "Epoch [33/100], Loss: 23.1163\n",
      "Epoch [34/100], Loss: 19.3318\n",
      "Epoch [35/100], Loss: 20.8210\n",
      "Epoch [36/100], Loss: 20.2587\n",
      "Epoch [37/100], Loss: 18.4733\n",
      "Epoch [38/100], Loss: 20.1657\n",
      "Epoch [39/100], Loss: 19.9912\n",
      "Epoch [40/100], Loss: 21.5076\n",
      "Epoch [41/100], Loss: 18.9774\n",
      "Epoch [42/100], Loss: 18.9310\n",
      "Epoch [43/100], Loss: 17.1388\n",
      "Epoch [44/100], Loss: 16.6240\n",
      "Epoch [45/100], Loss: 28.6599\n",
      "Epoch [46/100], Loss: 22.4957\n",
      "Epoch [47/100], Loss: 20.4897\n",
      "Epoch [48/100], Loss: 20.7217\n",
      "Epoch [49/100], Loss: 17.2362\n",
      "Epoch [50/100], Loss: 18.2122\n",
      "Epoch [51/100], Loss: 16.4845\n",
      "Epoch [52/100], Loss: 18.4813\n",
      "Epoch [53/100], Loss: 25.6085\n",
      "Epoch [54/100], Loss: 16.4555\n",
      "Epoch [55/100], Loss: 25.7835\n",
      "Epoch [56/100], Loss: 15.5106\n",
      "Epoch [57/100], Loss: 18.8285\n",
      "Epoch [58/100], Loss: 16.9512\n",
      "Epoch [59/100], Loss: 20.4298\n",
      "Epoch [60/100], Loss: 23.4143\n",
      "Epoch [61/100], Loss: 18.6307\n",
      "Epoch [62/100], Loss: 15.8725\n",
      "Epoch [63/100], Loss: 18.2175\n",
      "Epoch [64/100], Loss: 16.9279\n",
      "Epoch [65/100], Loss: 16.8382\n",
      "Epoch [66/100], Loss: 16.3220\n",
      "Epoch [67/100], Loss: 18.0854\n",
      "Epoch [68/100], Loss: 22.1398\n",
      "Epoch [69/100], Loss: 14.0460\n",
      "Epoch [70/100], Loss: 28.3891\n",
      "Epoch [71/100], Loss: 15.5691\n",
      "Epoch [72/100], Loss: 17.2688\n",
      "Epoch [73/100], Loss: 14.3919\n",
      "Epoch [74/100], Loss: 17.8634\n",
      "Epoch [75/100], Loss: 13.3902\n",
      "Epoch [76/100], Loss: 14.4128\n",
      "Epoch [77/100], Loss: 14.0847\n",
      "Epoch [78/100], Loss: 13.7349\n",
      "Epoch [79/100], Loss: 12.8951\n",
      "Epoch [80/100], Loss: 15.8872\n",
      "Epoch [81/100], Loss: 14.9183\n",
      "Epoch [82/100], Loss: 13.8634\n",
      "Epoch [83/100], Loss: 18.6364\n",
      "Epoch [84/100], Loss: 15.3699\n",
      "Epoch [85/100], Loss: 14.7524\n",
      "Epoch [86/100], Loss: 13.3128\n",
      "Epoch [87/100], Loss: 12.2171\n",
      "Epoch [88/100], Loss: 18.5331\n",
      "Epoch [89/100], Loss: 13.2520\n",
      "Epoch [90/100], Loss: 17.7863\n",
      "Epoch [91/100], Loss: 12.3349\n",
      "Epoch [92/100], Loss: 15.1466\n",
      "Epoch [93/100], Loss: 14.0692\n",
      "Epoch [94/100], Loss: 15.4614\n",
      "Epoch [95/100], Loss: 13.4043\n",
      "Epoch [96/100], Loss: 15.6277\n",
      "Epoch [97/100], Loss: 12.0609\n",
      "Epoch [98/100], Loss: 14.1553\n",
      "Epoch [99/100], Loss: 11.4215\n",
      "Epoch [100/100], Loss: 17.4102\n",
      "Validation RMSE: 5.963234215974808\n",
      "Best hyperparameters: {'optimizer_type': 'sgd', 'hidden_size': 32, 'lr': 0.01, 'batch_size': 32, 'n_epochs': 150, 'betas': (0.9, 0.999)}, Best RMSE: 5.7026718854904175\n"
     ]
    }
   ],
   "source": [
    "model = 'nn'\n",
    "\n",
    "param_grid = grids[model]\n",
    "\n",
    "optimizer = 'sgd'\n",
    "search_iterations = 50\n",
    "\n",
    "best_params = random_search(model, X_train, y_train, X_test, y_test, param_grid, search_iterations, optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
